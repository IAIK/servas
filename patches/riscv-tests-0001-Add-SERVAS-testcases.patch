From 4b24cf428c450c6ac96bfa77066ed339ec7d6d0d Mon Sep 17 00:00:00 2001
From: Stefan Steinegger <51908919+Steinegger@users.noreply.github.com>
Date: Fri, 20 Aug 2021 16:33:34 +0200
Subject: [PATCH] Add SERVAS testcases

---
 isa/Makefile                                  |   7 +-
 isa/macros/scalar/z_routines.h                | 508 ++++++++++++
 isa/rv64mz/Makefrag                           |  11 +
 isa/rv64mz/basic_csr_access.S                 | 769 ++++++++++++++++++
 isa/rv64mz/machine_store_tweak.S              | 198 +++++
 isa/rv64uz/Makefrag                           |  26 +
 isa/rv64uz/basic_csr_access.S                 | 479 +++++++++++
 isa/rv64uz/double_mapping.S                   | 312 +++++++
 isa/rv64uz/elrange_access_performance.S       | 510 ++++++++++++
 isa/rv64uz/elrange_access_precise_bounds.S    | 412 ++++++++++
 .../enclave_execute_encrypted_memory_simple.S | 210 +++++
 isa/rv64uz/enclave_load_encrypted_memory.S    | 211 +++++
 isa/rv64uz/enclave_load_store_byte_sizes.S    | 447 ++++++++++
 isa/rv64uz/fetch_authentication_exception.S   | 266 ++++++
 isa/rv64uz/load_authentication_exception.S    | 217 +++++
 isa/rv64uz/load_store_operation_for_hw.S      | 385 +++++++++
 isa/rv64uz/machine_store_tweak_user_load.S    | 204 +++++
 isa/rv64uz/modify_pte.S                       | 137 ++++
 isa/rv64uz/page_type_data_access.S            | 455 +++++++++++
 isa/rv64uz/store_authentication_exception.S   | 213 +++++
 isa/rv64uz/tweaked_vaddr_paddr_access.S       | 197 +++++
 isa/rv64uz/vectored_exceptions_base.S         | 169 ++++
 isa/rv64uz/vectored_exceptions_extended.S     | 193 +++++
 23 files changed, 6535 insertions(+), 1 deletion(-)
 create mode 100644 isa/macros/scalar/z_routines.h
 create mode 100644 isa/rv64mz/Makefrag
 create mode 100644 isa/rv64mz/basic_csr_access.S
 create mode 100644 isa/rv64mz/machine_store_tweak.S
 create mode 100644 isa/rv64uz/Makefrag
 create mode 100644 isa/rv64uz/basic_csr_access.S
 create mode 100644 isa/rv64uz/double_mapping.S
 create mode 100644 isa/rv64uz/elrange_access_performance.S
 create mode 100644 isa/rv64uz/elrange_access_precise_bounds.S
 create mode 100644 isa/rv64uz/enclave_execute_encrypted_memory_simple.S
 create mode 100644 isa/rv64uz/enclave_load_encrypted_memory.S
 create mode 100644 isa/rv64uz/enclave_load_store_byte_sizes.S
 create mode 100644 isa/rv64uz/fetch_authentication_exception.S
 create mode 100644 isa/rv64uz/load_authentication_exception.S
 create mode 100644 isa/rv64uz/load_store_operation_for_hw.S
 create mode 100644 isa/rv64uz/machine_store_tweak_user_load.S
 create mode 100644 isa/rv64uz/modify_pte.S
 create mode 100644 isa/rv64uz/page_type_data_access.S
 create mode 100644 isa/rv64uz/store_authentication_exception.S
 create mode 100644 isa/rv64uz/tweaked_vaddr_paddr_access.S
 create mode 100644 isa/rv64uz/vectored_exceptions_base.S
 create mode 100644 isa/rv64uz/vectored_exceptions_extended.S

diff --git a/isa/Makefile b/isa/Makefile
index 4e1ba20..48b8e6a 100644
--- a/isa/Makefile
+++ b/isa/Makefile
@@ -15,6 +15,8 @@ include $(src_dir)/rv64uf/Makefrag
 include $(src_dir)/rv64ud/Makefrag
 include $(src_dir)/rv64si/Makefrag
 include $(src_dir)/rv64mi/Makefrag
+include $(src_dir)/rv64mz/Makefrag
+include $(src_dir)/rv64uz/Makefrag
 endif
 include $(src_dir)/rv32ui/Makefrag
 include $(src_dir)/rv32uc/Makefrag
@@ -24,7 +26,6 @@ include $(src_dir)/rv32uf/Makefrag
 include $(src_dir)/rv32ud/Makefrag
 include $(src_dir)/rv32si/Makefrag
 include $(src_dir)/rv32mi/Makefrag
-
 default: all
 
 #--------------------------------------------------------------------
@@ -79,6 +80,8 @@ $(eval $(call compile_template,rv32uf,-march=rv32g -mabi=ilp32))
 $(eval $(call compile_template,rv32ud,-march=rv32g -mabi=ilp32))
 $(eval $(call compile_template,rv32si,-march=rv32g -mabi=ilp32))
 $(eval $(call compile_template,rv32mi,-march=rv32g -mabi=ilp32))
+$(eval $(call compile_template,rv32mz,-march=rv32g -mabi=ilp32))
+$(eval $(call compile_template,rv32uz,-march=rv32g -mabi=ilp32))
 ifeq ($(XLEN),64)
 $(eval $(call compile_template,rv64ui,-march=rv64g -mabi=lp64))
 $(eval $(call compile_template,rv64uc,-march=rv64g -mabi=lp64))
@@ -88,6 +91,8 @@ $(eval $(call compile_template,rv64uf,-march=rv64g -mabi=lp64))
 $(eval $(call compile_template,rv64ud,-march=rv64g -mabi=lp64))
 $(eval $(call compile_template,rv64si,-march=rv64g -mabi=lp64))
 $(eval $(call compile_template,rv64mi,-march=rv64g -mabi=lp64))
+$(eval $(call compile_template,rv64mz,-march=rv64g -mabi=lp64))
+$(eval $(call compile_template,rv64uz,-march=rv64g -mabi=lp64))
 endif
 
 tests_dump = $(addsuffix .dump, $(tests))
diff --git a/isa/macros/scalar/z_routines.h b/isa/macros/scalar/z_routines.h
new file mode 100644
index 0000000..85c2c47
--- /dev/null
+++ b/isa/macros/scalar/z_routines.h
@@ -0,0 +1,508 @@
+#ifndef __TEST_Z_ROUTINES_H
+#define __TEST_Z_ROUTINES_H
+
+#define INTERMEDIATE_PTES 0x0000000080003000
+#define LEAF_PTES         0x0000000080004000
+
+#define RISCV_PGSHIFT 12
+
+#define PAGE_ZERO       0x0000000080005000
+#define PAGE_ONE        0x0000000080006000
+#define PAGE_TWO        0x0000000080007000
+#define PAGE_THREE      0x0000000080008000
+#define PAGE_FOUR       0x0000000080009000
+#define PAGE_FIVE       0x000000008000A000
+#define PAGE_SIX        0x000000008000B000
+#define PAGE_SEVEN      0x000000008000C000
+#define PAGE_EIGHT      0x000000008000D000
+#define PAGE_NINE       0x000000008000E000
+#define PAGE_TEN        0x000000008000F000
+#define PAGE_ELEVEN     0x0000000080010000
+#define PAGE_TWELVE     0x0000000080011000
+#define PAGE_THIRTEEN   0x0000000080012000
+#define PAGE_FOURTEEN   0x0000000080013000
+#define PAGE_FIFTEEN    0x0000000080014000
+#define PAGE_SIXTEEN    0x0000000080015000
+#define PAGE_SEVENTEEN  0x0000000080016000
+#define PAGE_EIGHTEEN   0x0000000080017000
+#define PAGE_NINETEEN   0x0000000080018000
+#define PAGE_TWENTY     0x0000000080019000
+#define PAGE_TWENTYONE  0x000000008001A000
+#define PAGE_TWENTYTWO  0x000000008001B000
+
+
+#define PERSISTANT_REG_FOR_RET ra
+
+// #### operates on whatever the current mode is
+#define read_value(treg_0, treg_1, label, expected_value) \
+  la treg_1, label ; \
+  lwu treg_0, 0(treg_1) ; \
+  li treg_1, expected_value ; \
+
+#define read_value_test(treg_0, treg_1, label, expected_value, error_label) \
+  read_value(treg_0, treg_1, label, expected_value) \
+  bne treg_0, treg_1, error_label ;
+
+#define read_value_test_expect_fail(treg_0, treg_1, label, expected_value, error_label) \
+  read_value(treg_0, treg_1, label, expected_value) \
+  beq treg_0, treg_1, error_label ;
+
+// ####
+
+#define write_and_readback_value(treg_0, treg_1, label, expected_value) \
+  li treg_0, expected_value ; \
+  la treg_1, label ; \
+  sw treg_0, 0(treg_1) ; \
+  nop ; \
+  lwu treg_1, 0(treg_1) ; \
+
+#define write_and_readback_value_test(treg_0, treg_1, label, expected_value, error_label) \
+  write_and_readback_value(treg_0, treg_1, label, expected_value) \
+  bne treg_0, treg_1, error_label ;
+
+#define write_and_readback_value_test_expect_fail(treg_0, treg_1, label, expected_value, error_label) \
+  write_and_readback_value(treg_0, treg_1, label, expected_value) \
+  beq treg_0, treg_1, error_label ;
+
+// ####
+
+#define jump_and_check_return(treg, persistant_return_reg, jump_target_label) \
+  jal persistant_return_reg, jump_target_label;  \
+  la treg, jump_target_label; \
+  addi treg, treg, 4;
+
+#define jump_and_check_return_test(treg_0, treg_1, persistant_return_reg, jump_target_label, error_label) \
+  jump_and_check_return(treg_0, persistant_return_reg, jump_target_label) \
+  bne treg_0, persistant_return_reg, error_label;
+
+#define jump_and_check_return_test_expect_fail(treg_0, treg_1, persistant_return_reg, jump_target_label, error_label); \
+  jump_and_check_return(treg_0, persistant_return_reg, jump_target_label) \
+  beq treg_0, persistant_return_reg, error_label;
+
+#define jump_and_check_return_dst(persistant_return_reg) \
+  jalr PERSISTANT_REG_FOR_RET, 0(PERSISTANT_REG_FOR_RET);
+
+// ####
+
+#define quick_enable_m_vm_access(treg_0) \
+  li treg_0, MSTATUS_MPRV ; \
+  csrs mstatus, treg_0 ;
+
+#define quick_disable_m_vm_access(treg_0) \
+  li treg_0, MSTATUS_MPRV ; \
+  csrc mstatus, treg_0 ;
+
+#define setup_m_to_u_vm(treg_0) \
+  /* Set up MPRV with MPP=U */ \
+  quick_enable_m_vm_access(treg_0) \
+  li treg_0, MSTATUS_SUM ; \
+  csrs mstatus, treg_0 ; \
+  /* clear the MPP for usermode */ \
+  li treg_0, MSTATUS_MPP ; \
+  csrc mstatus, treg_0 ;
+
+#define set_bit_in_csr(mask, csr_name, treg_0) \
+  li treg_0, mask; \
+  csrs csr_name, treg_0; \
+
+#define clear_bit_in_csr(mask, csr_name, treg) \
+  li treg, mask; \
+  csrc csr_name, treg;
+
+#define write_value_to_csr(value, csr_name, treg) \
+  li treg, value; \
+  csrw csr_name, treg;
+
+#define disable_ltweak(treg) \
+  write_value_to_csr(0, CSR_E_LTWEAK, treg);
+
+#define disable_stweak(treg) \
+  write_value_to_csr(0, CSR_E_STWEAK, treg);
+
+#define disable_lstweaks(treg) \
+  disable_ltweak(treg); \
+  disable_stweak(treg);
+
+#define set_and_enable_load_tweak(treg, tweak) \
+  li treg, tweak ; \
+  csrw CSR_E_LTWEAK, treg ;
+
+#define set_and_enable_store_tweak(treg, tweak) \
+  li treg, tweak ; \
+  csrw CSR_E_STWEAK, treg ;
+
+//######
+// CSR routines
+#define write_csr(SOME_CSR, treg_0, treg_1, value) \
+        li    treg_0 ,  value ; \
+        li    treg_1 ,  0     ; \
+        not   treg_1 ,  treg_0 ; \
+        csrw  SOME_CSR ,  treg_0 ; \
+
+#define check_csr(SOME_CSR, treg_0, treg_1, value, fail_label) \
+        li    treg_0 ,  value ; \
+        not   treg_1 ,  treg_0 ; \
+        csrr  treg_1 ,  SOME_CSR ; \
+        bne   treg_0 ,  treg_1 , fail_label;
+
+#define test_and_check_csr(SOME_CSR, treg_0, treg_1, value, fail_label) \
+        write_csr(SOME_CSR, treg_0, treg_1, value) \
+        nop; \
+        nop; \
+        csrr  treg_1 ,  SOME_CSR ; \
+        bne   treg_0 ,  treg_1 , fail_label;
+
+#define test_and_check_csr_expect_diff(SOME_CSR, treg_0, treg_1, value, expected, fail_label) \
+        write_csr(SOME_CSR, treg_0, treg_1, value) \
+        nop; \
+        nop; \
+        check_csr(SOME_CSR, treg_0, treg_1, expected, fail_label)
+        
+
+//######
+
+#define page_to_leaf_address(PAGE, PAGE_ZERO) \
+  (LEAF_PTES + ((PAGE-PAGE_ZERO) >> RISCV_PGSHIFT) << 3)
+
+#define save_PTE_disable_VM(pte_reg, pte_address) \
+  quick_disable_m_vm_access(pte_reg) \
+  li pte_reg, pte_address; \
+  ld pte_reg, 0(pte_reg);
+
+#define make_pte_in_reg_rwx(pte_reg, result_reg) \
+  li result_reg, (PTE_R | PTE_W | PTE_X); \
+  or result_reg, result_reg, pte_reg;
+
+#define restore_PTE_disable_VM(pte_reg, treg_0, pte_address) \
+  quick_disable_m_vm_access(treg_0) \
+  li treg_0, pte_address; \
+  sd pte_reg, 0(treg_0); \
+  sfence.vma;
+
+//######  
+
+// TODO fix this routine needs one more temp register to backup and restore the ltweak
+#define write_with_stweak(stweak, treg_0, treg_2, treg_3, memdst_label, reg_with_page_zero, value, fail_label) \
+  quick_enable_m_vm_access(treg_3) ; \
+  /* set tweak */ \
+  set_and_enable_store_tweak(treg_0, stweak) ; \
+  /* determine address */ \
+  la treg_0, memdst_label ; \
+  sub treg_0, treg_0, reg_with_page_zero ; \
+  li treg_2, value ; \
+  /* write to address */ \
+  sw treg_2, 0(treg_0) ; \
+  /* read value back */ \
+  lwu treg_0, 0(treg_0) ; \
+  disable_stweak(treg_3) ; \
+  quick_disable_m_vm_access(treg_3) ; \
+  bne treg_0, treg_2, fail_label ;
+
+#define write_with_stweak_no_verify(stweak, treg_0, treg_2, treg_3, memdst_label, reg_with_page_zero, value, fail_label) \
+  quick_enable_m_vm_access(treg_3) ; \
+  /* set tweak */ \
+  set_and_enable_store_tweak(treg_0, stweak) ; \
+  /* determine address */ \
+  la treg_0, memdst_label ; \
+  sub treg_0, treg_0, reg_with_page_zero ; \
+  li treg_2, value ; \
+  /* write to address */ \
+  sw treg_2, 0(treg_0) ; \
+  disable_stweak(treg_3) ; \
+  quick_disable_m_vm_access(treg_3) ;
+
+#define check_physical_memory(memsrc_label, treg_0, treg_2, exp_value, fail_label) \
+  la treg_0, memsrc_label ; \
+  lwu treg_0, 0(treg_0) ; \
+  li treg_2, exp_value ; \
+  bne treg_0, treg_2, fail_label ;
+
+#define check_physical_memory_change(memsrc_label, treg_0, treg_2, exp_value, fail_label) \
+  la treg_0, memsrc_label ; \
+  lwu treg_0, 0(treg_0) ; \
+  li treg_2, exp_value ; \
+  beq treg_0, treg_2, fail_label ;
+
+#define inv_32bit(value) \
+  (~value) & 0xFFFFFFFF
+
+#define set_user_pc_to_userlabel(treg_0, treg_1, label, PAGE_ZERO) \
+la treg_0, label; \
+li treg_1, PAGE_ZERO; \
+sub treg_0, treg_0, treg_1; \
+csrw mepc, treg_0;
+
+#define set_user_pc_to_return_reg(return_reg) \
+csrw mepc, return_reg;
+
+
+//################################################################
+// Machine Mode Exception Handler blocks
+
+#define save_and_deploy_mtvec(treg_0, persistant_reg_to_restore, mtvec_label) \
+  /* read and save the current mtvec */ \
+  csrr persistant_reg_to_restore, CSR_MTVEC; \
+  /* set own mtvec to leave userspace after we're done */ \
+  la treg_0, mtvec_label; \
+  /* make it vectored if possible */ \
+  ori treg_0, treg_0, 1; \
+  csrw CSR_MTVEC, treg_0;
+
+#define m_exception_handler_jump_table(synchronous_exception_handler_label, ecall_exception_handler_label, descryption_exception_handler_label) \
+my_mtvec_handler: \
+  j synchronous_exception_handler_label   ;     /* Instruction address misaligned */ \
+  j fail_skip_i                           ;     /* Instruction access fault */ \
+  j fail_skip_i                           ;     /* Illegal instruction */ \
+  j fail_skip_i                           ;     /* Breakpoint */ \
+  j fail_skip_i                           ;     /* Load address misaligned */ \
+  j fail_skip_i                           ;     /* Load access fault */ \
+  j fail_skip_i                           ;     /* Store/AMO address misaligned */ \
+  j fail_skip_i                           ;     /* Store/AMO access fault */ \
+  j ecall_exception_handler_label         ;     /* Environment call from U-mode */ \
+  j fail_skip_i                           ;     /* Environment call from S-mode */ \
+  j fail_skip_i                           ;     /* reserved */ \
+  j fail_skip_i                           ;     /* Environment call from M-mode */ \
+  j fail_skip_i                           ;     /* Instruction page fault */ \
+  j fail_skip_i                           ;     /* Load page fault */ \
+  j fail_skip_i                           ;     /* Donky Exception */ \
+  j fail_skip_i                           ;     /* Store/AMO page fault */ \
+  j descryption_exception_handler_label   ;     /* Decryption/ Authentication exception */
+
+
+#define exception_handler_skip_instruction(treg_0, treg_1) \
+fail_skip_i: \
+  li treg_0, 0xDEADBEEF; \
+  csrr treg_1, mepc; \
+  ADDI treg_1,treg_1,4; \
+  csrw mepc, treg_1; \
+  li treg_1, 0xAAAAAAAA; \
+  mret;
+
+#define m_exception_handler_end_and_die(treg_0, treg_1, saved_mtvec_reg) \
+end: \
+  csrw CSR_MTVEC, saved_mtvec_reg; \
+  li treg_0, 0xCAFEBABE; \
+  li treg_1, 0xAAAAAAAA; \
+  nop; \
+  nop; \
+RVTEST_PASS; \
+ \
+die: \
+  csrw CSR_MTVEC, saved_mtvec_reg; \
+  li a7, 3; \
+  li treg_1, 0xDEADDEAD; \
+  li treg_0, 0xDEADDEAD; \
+  RVTEST_FAIL
+
+// pass 2 registers that can be overwritten, add error register (usually a7) to decide if the userspace application succeeded or not
+#define usermode_exit_handler(treg_0, treg_1, saved_mtvec_reg, error_reg) \
+m_exception_handler_jump_table(my_synchronous_exception, my_synchronous_exception, my_synchronous_exception); \
+\
+exception_handler_skip_instruction(treg_0, treg_1) \
+\
+my_synchronous_exception: \
+  li treg_0, 0xDEADDEAD; \
+no_ecall: \
+  csrw CSR_MTVEC, saved_mtvec_reg; \
+  li treg_1, 0xAAAAAAAA; \
+  bnez error_reg, die; \
+  j end; \
+\
+m_exception_handler_end_and_die(treg_0, treg_1, saved_mtvec_reg);
+
+// pass 2 registers that can be overwritten, add error register (usually a7) to decide if the userspace application succeeded or not
+#define usermode_exit_handler_success_on_decryption_error(treg_0, treg_1, saved_mtvec_reg, error_reg, expected_exeptions, ecall_exception_handler_label, descryption_exception_handler_label) \
+m_exception_handler_jump_table(my_synchronous_exception, ecall_exception_handler_label, descryption_exception_handler_label); \
+\
+exception_save_slot: .word 0x0; \
+exception_handler_skip_instruction(treg_0, treg_1) \
+\
+my_synchronous_exception: \
+  csrr treg_0, CSR_MCAUSE; \
+  slli treg_0, treg_0, 2; \
+  la treg_1, my_mtvec_handler; \
+  add treg_1, treg_0, treg_1; \
+  jalr treg_1, 0(treg_1); \
+\
+final_ecall: \
+  la treg_0, exception_save_slot; \
+  lwu treg_1, 0(treg_0); \
+  li treg_0, expected_exeptions; \
+  beq treg_1, treg_0, end; \
+  j die; \
+\
+decryption_exception: \
+  la treg_0, exception_save_slot; \
+  lwu treg_1, 0(treg_0); \
+  addi treg_1, treg_1, 1; \
+  sw treg_1, 0(treg_0); \
+  li treg_0, expected_exeptions; \
+  bgt treg_1, treg_0, die; \
+  j fail_skip_i; \
+  \
+no_ecall: \
+  bnez treg_1, die; \
+  \
+  li treg_1, 0xAAAAAAAA; \
+  bnez error_reg, die; \
+  j end; \
+\
+m_exception_handler_end_and_die(treg_0, treg_1, saved_mtvec_reg);
+
+
+#define usermode_exit_handler_success_on_fetch_decryption_error(treg_0, treg_1, saved_mtvec_reg, error_reg, expected_exeptions, ecall_exception_handler_label, descryption_exception_handler_label) \
+m_exception_handler_jump_table(my_synchronous_exception, ecall_exception_handler_label, descryption_exception_handler_label); \
+\
+exception_save_slot: .word 0x0; \
+exception_handler_skip_instruction(treg_0, treg_1) \
+\
+my_synchronous_exception: \
+  csrr treg_0, CSR_MCAUSE; \
+  slli treg_0, treg_0, 2; \
+  la treg_1, my_mtvec_handler; \
+  add treg_1, treg_0, treg_1; \
+  jalr treg_1, 0(treg_1); \
+\
+final_ecall: \
+  la treg_0, exception_save_slot; \
+  lwu treg_1, 0(treg_0); \
+  li treg_0, expected_exeptions; \
+  beq treg_1, treg_0, end; \
+  j die; \
+\
+decryption_exception: \
+  la treg_0, exception_save_slot; \
+  lwu treg_1, 0(treg_0); \
+  addi treg_1, treg_1, 1; \
+  sw treg_1, 0(treg_0); \
+  li treg_0, expected_exeptions; \
+  bgt treg_1, treg_0, die; \
+  j return_to_user_jump_table_init; \
+  \
+no_ecall: \
+  bnez treg_1, die; \
+  \
+  li treg_1, 0xAAAAAAAA; \
+  bnez error_reg, die; \
+  j end; \
+\
+m_exception_handler_end_and_die(treg_0, treg_1, saved_mtvec_reg);
+
+//################################################################
+// ending a userspace test with an ecall
+#define userspace_end_labels(success_label, error_label, error_reg) \
+    success_label: \
+        li error_reg, 0; \
+        ecall; \
+    \
+    error_label: \
+        mv error_reg, gp; \
+        ecall;
+
+#define vma_from_physical(pyhsical, vm_offset, vma_retreg, treg_1) \
+  li vma_retreg, pyhsical; \
+  li treg_1, vm_offset; \
+  sub vma_retreg, vma_retreg, treg_1;
+
+#define copy_vma_to_enclave_vma(start_phys, end_phys, dst_phys, vm_offset, treg_0, treg_1, treg_2, treg_3) \
+  vma_from_physical(start_phys, vm_offset, treg_0, treg_1) \
+  vma_from_physical(end_phys, vm_offset, treg_1, treg_2) \
+  vma_from_physical(dst_phys, vm_offset, treg_2, treg_3) \
+  1: \
+  lwu treg_3, 0(treg_0); \
+  sw treg_3, 0(treg_2); \
+  addi treg_0, treg_0, 4; \
+  addi treg_2, treg_2, 4; \
+  bltu treg_0, treg_1, 1b;
+
+// saves and preserves the pte of the first page
+// todo make this a loop to save each pte
+#define copy_vma_to_enclave_vma_preserve_pte(start_phys, end_phys, dst_phys, vm_offset, treg_0, treg_1, treg_2, treg_3, treg_4) \
+  save_PTE_disable_VM(treg_4, page_to_leaf_address(start_phys, vm_offset)) \
+  make_pte_in_reg_rwx(treg_4, treg_0) \
+  restore_PTE_disable_VM(treg_0, treg_1, page_to_leaf_address(start_phys, vm_offset)) \
+  quick_enable_m_vm_access(treg_0) \
+  copy_vma_to_enclave_vma(start_phys, end_phys, dst_phys, vm_offset, treg_0, treg_1, treg_2, treg_3) \
+  restore_PTE_disable_VM(treg_4, treg_1, page_to_leaf_address(start_phys, vm_offset)) \
+  quick_enable_m_vm_access(treg_0) \
+
+#define set_physical_memory_range_to_value(start_phys, end_phys, value, treg_0, treg_1, treg_2) \
+  li treg_1, start_phys; \
+  li treg_0, end_phys; \
+  li treg_2, value; \
+  1: \
+  sw treg_2, 0(treg_1); \
+  addi treg_1, treg_1, 4; \
+  bltu treg_1, treg_0, 1b;
+
+// ###############################
+
+#define load_store_execute_cycle_rwx(temp0, temp1, persistant_return_reg, load_label, store_label, jump_label, expected_load_value, store_value, error_label) \
+  addi TESTNUM, TESTNUM, 1; \
+  read_value_test(temp0, temp1, load_label, expected_load_value, error_label); \
+  read_value_test(temp0, temp1, load_label, expected_load_value, error_label); \
+  addi TESTNUM, TESTNUM, 1; \
+  write_and_readback_value_test(temp0, temp1, store_label, store_value, error_label); \
+  write_and_readback_value_test(temp0, temp1, store_label, store_value, error_label); \
+  addi TESTNUM, TESTNUM, 1; \
+  jump_and_check_return_test(temp0, temp1, persistant_return_reg, jump_label, error_label); \
+  jump_and_check_return_test(temp0, temp1, persistant_return_reg, jump_label, error_label);
+
+#define load_store_execute_cycle_rox(temp0, temp1, persistant_return_reg, load_label, store_label, jump_label, expected_load_value, store_value, error_label) \
+  addi TESTNUM, TESTNUM, 1; \
+  read_value_test(temp0, temp1, load_label, expected_load_value, error_label); \
+  read_value_test(temp0, temp1, load_label, expected_load_value, error_label); \
+  addi TESTNUM, TESTNUM, 1; \
+  write_and_readback_value_test_expect_fail(temp0, temp1, store_label, store_value, error_label); \
+  write_and_readback_value_test_expect_fail(temp0, temp1, store_label, store_value, error_label); \
+  addi TESTNUM, TESTNUM, 1; \
+  jump_and_check_return_test(temp0, temp1, persistant_return_reg, jump_label, error_label); \
+  jump_and_check_return_test(temp0, temp1, persistant_return_reg, jump_label, error_label);
+  
+#define load_store_execute_cycle_rwo(temp0, temp1, persistant_return_reg, load_label, store_label, jump_label, expected_load_value, store_value, error_label) \
+  addi TESTNUM, TESTNUM, 1; \
+  read_value_test(temp0, temp1, load_label, expected_load_value, error_label); \
+  read_value_test(temp0, temp1, load_label, expected_load_value, error_label); \
+  addi TESTNUM, TESTNUM, 1; \
+  write_and_readback_value_test(temp0, temp1, store_label, store_value, error_label); \
+  write_and_readback_value_test(temp0, temp1, store_label, store_value, error_label); \
+  addi TESTNUM, TESTNUM, 1; \
+  jump_and_check_return_test_expect_fail(temp0, temp1, persistant_return_reg, jump_label, error_label); \
+  jump_and_check_return_test_expect_fail(temp0, temp1, persistant_return_reg, jump_label, error_label);
+
+#define load_store_execute_cycle_roo(temp0, temp1, persistant_return_reg, load_label, store_label, jump_label, expected_load_value, store_value, error_label) \
+  addi TESTNUM, TESTNUM, 1; \
+  read_value_test(temp0, temp1, load_label, expected_load_value, error_label); \
+  read_value_test(temp0, temp1, load_label, expected_load_value, error_label); \
+  addi TESTNUM, TESTNUM, 1; \
+  write_and_readback_value_test_expect_fail(temp0, temp1, store_label, store_value, error_label); \
+  write_and_readback_value_test_expect_fail(temp0, temp1, store_label, store_value, error_label); \
+  addi TESTNUM, TESTNUM, 1; \
+  jump_and_check_return_test_expect_fail(temp0, temp1, persistant_return_reg, jump_label, error_label); \
+  jump_and_check_return_test_expect_fail(temp0, temp1, persistant_return_reg, jump_label, error_label);
+
+#define load_store_execute_cycle_ooo(temp0, temp1, persistant_return_reg, load_label, store_label, jump_label, expected_load_value, store_value, error_label) \
+  addi TESTNUM, TESTNUM, 1; \
+  read_value_test_expect_fail(temp0, temp1, load_label, expected_load_value, error_label); \
+  read_value_test_expect_fail(temp0, temp1, load_label, expected_load_value, error_label); \
+  addi TESTNUM, TESTNUM, 1; \
+  write_and_readback_value_test_expect_fail(temp0, temp1, store_label, store_value, error_label); \
+  write_and_readback_value_test_expect_fail(temp0, temp1, store_label, store_value, error_label); \
+  addi TESTNUM, TESTNUM, 1; \
+  jump_and_check_return_test_expect_fail(temp0, temp1, persistant_return_reg, jump_label, error_label); \
+  jump_and_check_return_test_expect_fail(temp0, temp1, persistant_return_reg, jump_label, error_label);
+
+#define load_store_execute_cycle_oox(temp0, temp1, persistant_return_reg, load_label, store_label, jump_label, expected_load_value, store_value, error_label) \
+  addi TESTNUM, TESTNUM, 1; \
+  read_value_test_expect_fail(temp0, temp1, load_label, expected_load_value, error_label); \
+  read_value_test_expect_fail(temp0, temp1, load_label, expected_load_value, error_label); \
+  addi TESTNUM, TESTNUM, 1; \
+  write_and_readback_value_test_expect_fail(temp0, temp1, store_label, store_value, error_label); \
+  write_and_readback_value_test_expect_fail(temp0, temp1, store_label, store_value, error_label); \
+  addi TESTNUM, TESTNUM, 1; \
+  jump_and_check_return_test(temp0, temp1, persistant_return_reg, jump_label, error_label); \
+  jump_and_check_return_test(temp0, temp1, persistant_return_reg, jump_label, error_label);
+
+
+
+#endif
\ No newline at end of file
diff --git a/isa/rv64mz/Makefrag b/isa/rv64mz/Makefrag
new file mode 100644
index 0000000..b58391b
--- /dev/null
+++ b/isa/rv64mz/Makefrag
@@ -0,0 +1,11 @@
+#=======================================================================
+# Makefrag for rv64mz tests
+#-----------------------------------------------------------------------
+
+rv64mz_sc_tests = \
+	basic_csr_access \
+	machine_store_tweak \
+
+rv64mz_p_tests = $(addprefix rv64mz-p-, $(rv64mz_sc_tests))
+
+spike64_tests += $(rv64mz_p_tests)
diff --git a/isa/rv64mz/basic_csr_access.S b/isa/rv64mz/basic_csr_access.S
new file mode 100644
index 0000000..0b6ad2d
--- /dev/null
+++ b/isa/rv64mz/basic_csr_access.S
@@ -0,0 +1,769 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# basic_csr_access.S
+#-----------------------------------------------------------------------------
+#
+# This is the most basic self checking test. If your simulator does not
+# pass thiss then there is little chance that it will pass any of the
+# more complicated self checking tests.
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+
+RVTEST_RV64M
+
+RVTEST_CODE_BEGIN
+
+li a0, 0x80000000
+li a0, 0x12345678
+nop
+nop
+li TESTNUM, 0
+test_and_check_csr(CSR_E_MRANGE_VBASE          , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_MRANGE_VSIZE          , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_MSID_0                , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_MSID_1                , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_SRANGE_VBASE          , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_SRANGE_VSIZE          , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_SSID_0                , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_SSID_1                , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_SECS                  , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_TCS                   , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_LTWEAK                , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_LTWEAK_XRANGE_MAP     , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_LTWEAK_XRANGE_MAP_MASK, t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_LTWEAK_PRV_LVL        , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_LTWEAK_PRV_LVL_MASK   , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_LTWEAK_TWEAK_EN       , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_LTWEAK_PTE_MSB        , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_LTWEAK_PTE_MSB_MASK   , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_LTWEAK_PTE_LSB        , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_LTWEAK_PTE_LSB_MASK   , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_STWEAK                , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_STWEAK_XRANGE_MAP     , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_STWEAK_XRANGE_MAP_MASK, t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_STWEAK_PRV_LVL        , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_STWEAK_PRV_LVL_MASK   , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_STWEAK_TWEAK_EN       , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_STWEAK_PTE_MSB        , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_STWEAK_PTE_MSB_MASK   , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_STWEAK_PTE_LSB        , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_STWEAK_PTE_LSB_MASK   , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_STATUS                , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_URANGE_VBASE          , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_URANGE_VSIZE          , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_USID_0                , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_USID_1                , t0, a0, 0x0, fail)
+
+nop
+nop
+
+li TESTNUM, 100
+test_and_check_csr(CSR_E_MRANGE_VBASE                       , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_MRANGE_VSIZE                       , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_MSID_0                             , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_MSID_1                             , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_SRANGE_VBASE                       , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_SRANGE_VSIZE                       , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_SSID_0                             , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_SSID_1                             , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_SECS                               , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_TCS                                , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK                 , t0, a0, 0xFFFFFFFFFFFFFFFF, 0x7FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_XRANGE_MAP      , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_XRANGE_MAP_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_XRANGE_MAP_MASK , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_XRANGE_MAP_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_PRV_LVL         , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_PRV_LVL_MASK    , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_TWEAK_EN        , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_TWEAK_ENABLE_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_PTE_MSB         , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_PTE_MSB_MASK    , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_PTE_LSB         , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_PTE_LSB_MASK    , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK                 , t0, a0, 0xFFFFFFFFFFFFFFFF, 0x7FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_XRANGE_MAP      , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_XRANGE_MAP_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_XRANGE_MAP_MASK , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_XRANGE_MAP_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_PRV_LVL         , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_PRV_LVL_MASK    , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_TWEAK_EN        , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_TWEAK_ENABLE_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_PTE_MSB         , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_PTE_MSB_MASK    , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_PTE_LSB         , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_PTE_LSB_MASK    , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_STATUS                             , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_URANGE_VBASE                       , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_URANGE_VSIZE                       , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_USID_0                             , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_USID_1                             , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+
+nop
+nop
+
+li TESTNUM, 200
+test_and_check_csr(CSR_E_MRANGE_VBASE                       , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_MRANGE_VSIZE                       , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_MSID_0                             , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_MSID_1                             , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_SRANGE_VBASE                       , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_SRANGE_VSIZE                       , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_SSID_0                             , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_SSID_1                             , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_SECS                               , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_TCS                                , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK                 , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & 0x7FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_XRANGE_MAP      , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_XRANGE_MAP_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_XRANGE_MAP_MASK , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_XRANGE_MAP_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_PRV_LVL         , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_PRV_LVL_MASK    , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_TWEAK_EN        , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_TWEAK_ENABLE_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_PTE_MSB         , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_PTE_MSB_MASK    , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_PTE_LSB         , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_LTWEAK_PTE_LSB_MASK    , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK                 , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & 0x7FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_XRANGE_MAP      , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_XRANGE_MAP_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_XRANGE_MAP_MASK , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_XRANGE_MAP_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_PRV_LVL         , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_PRV_LVL_MASK    , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_TWEAK_EN        , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_TWEAK_ENABLE_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_PTE_MSB         , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_PTE_MSB_MASK    , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_PTE_LSB         , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_STWEAK_PTE_LSB_MASK    , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_STATUS                             , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_URANGE_VBASE                       , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_URANGE_VSIZE                       , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_USID_0                             , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_USID_1                             , t0, a0, 0xFEDCBA012345789F, fail)
+nop
+nop
+nop
+nop
+nop
+
+
+// Set the partial registers one by one
+li TESTNUM, 300
+write_csr(CSR_E_LTWEAK, t0, a0, 0x0000000000000000)
+check_csr(CSR_E_LTWEAK, t0, a0, 0, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_PTE_LSB, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x00000000000003FF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_PTE_LSB_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x00000000000FFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_PTE_MSB, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x000000003FFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_PTE_MSB_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x000000FFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_TWEAK_EN, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x000001FFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_PRV_LVL, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x000007FFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_PRV_LVL_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x00001FFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_XRANGE_MAP, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0000FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_XRANGE_MAP_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK, t0, a0, 0x0000000000000000)
+check_csr(CSR_E_STWEAK, t0, a0, 0, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK, t0, a0, 0x0000000000000000)
+check_csr(CSR_E_STWEAK, t0, a0, 0, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_PTE_LSB, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x00000000000003FF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_PTE_LSB_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x00000000000FFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_PTE_MSB, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x000000003FFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_PTE_MSB_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x000000FFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_TWEAK_EN, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x000001FFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_PRV_LVL, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x000007FFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_PRV_LVL_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x00001FFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_XRANGE_MAP, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0000FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_XRANGE_MAP_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+
+// Clear the partial registers one by one
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_PTE_LSB, t0, a0, 0x0)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007FFFFFFFFFC00, fail)
+write_csr(CSR_E_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_PTE_LSB_MASK, t0, a0, 0x0)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007FFFFFFF003FF, fail)
+write_csr(CSR_E_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_PTE_MSB, t0, a0, 0x0)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007FFFFC00FFFFF, fail)
+write_csr(CSR_E_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_PTE_MSB_MASK, t0, a0, 0x0)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007FF003FFFFFFF, fail)
+write_csr(CSR_E_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_TWEAK_EN, t0, a0, 0x0)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007FEFFFFFFFFFF, fail)
+write_csr(CSR_E_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_PRV_LVL, t0, a0, 0x0)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007F9FFFFFFFFFF, fail)
+write_csr(CSR_E_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_PRV_LVL_MASK, t0, a0, 0x0)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007E7FFFFFFFFFF, fail)
+write_csr(CSR_E_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_XRANGE_MAP, t0, a0, 0x0)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x00071FFFFFFFFFFF, fail)
+write_csr(CSR_E_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_LTWEAK_XRANGE_MAP_MASK, t0, a0, 0x0)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0000FFFFFFFFFFFF, fail)
+write_csr(CSR_E_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_PTE_LSB, t0, a0, 0x0)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007FFFFFFFFFC00, fail)
+write_csr(CSR_E_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_PTE_LSB_MASK, t0, a0, 0x0)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007FFFFFFF003FF, fail)
+write_csr(CSR_E_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_PTE_MSB, t0, a0, 0x0)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007FFFFC00FFFFF, fail)
+write_csr(CSR_E_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_PTE_MSB_MASK, t0, a0, 0x0)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007FF003FFFFFFF, fail)
+write_csr(CSR_E_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_TWEAK_EN, t0, a0, 0x0)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007FEFFFFFFFFFF, fail)
+write_csr(CSR_E_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_PRV_LVL, t0, a0, 0x0)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007F9FFFFFFFFFF, fail)
+write_csr(CSR_E_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_PRV_LVL_MASK, t0, a0, 0x0)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007E7FFFFFFFFFF, fail)
+write_csr(CSR_E_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_XRANGE_MAP, t0, a0, 0x0)
+check_csr(CSR_E_STWEAK, t0, a0, 0x00071FFFFFFFFFFF, fail)
+write_csr(CSR_E_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_STWEAK_XRANGE_MAP_MASK, t0, a0, 0x0)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0000FFFFFFFFFFFF, fail)
+write_csr(CSR_E_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+
+// #########################################
+// write the userspace debug views of these registers
+
+li TESTNUM, 400
+test_and_check_csr(CSR_E_DBG_MRANGE_VBASE          , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MRANGE_VSIZE          , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MSID_0                , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MSID_1                , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SRANGE_VBASE          , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SRANGE_VSIZE          , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SSID_0                , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SSID_1                , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SECS                  , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_TCS                   , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK                , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_XRANGE_MAP     , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_XRANGE_MAP_MASK, t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_PRV_LVL        , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_PRV_LVL_MASK   , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_TWEAK_EN       , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_PTE_MSB        , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_PTE_MSB_MASK   , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_PTE_LSB        , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_PTE_LSB_MASK   , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK                , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK_PRV_LVL        , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK_PRV_LVL_MASK   , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK_TWEAK_EN       , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK_PTE_MSB        , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK_PTE_MSB_MASK   , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK_PTE_LSB        , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK_PTE_LSB_MASK   , t0, a0, 0x0, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STATUS                , t0, a0, 0x0, fail)
+
+nop
+nop
+
+li TESTNUM, 500
+test_and_check_csr(CSR_E_DBG_MRANGE_VBASE                      , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MRANGE_VSIZE                      , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MSID_0                            , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MSID_1                            , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SRANGE_VBASE                      , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SRANGE_VSIZE                      , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SSID_0                            , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SSID_1                            , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SECS                              , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_TCS                               , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK                , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & 0x7FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_XRANGE_MAP     , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_XRANGE_MAP_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_XRANGE_MAP_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_XRANGE_MAP_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PRV_LVL        , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PRV_LVL_MASK   , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_TWEAK_EN       , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_TWEAK_ENABLE_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_MSB        , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_MSB_MASK   , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_LSB        , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_LSB_MASK   , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK                , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & 0x7FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PRV_LVL        , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PRV_LVL_MASK   , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_TWEAK_EN       , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_TWEAK_ENABLE_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_MSB        , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_MSB_MASK   , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_LSB        , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_LSB_MASK   , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STATUS                            , t0, a0, 0xFFFFFFFFFFFFFFFF, fail)
+
+nop
+nop
+
+li TESTNUM, 600
+test_and_check_csr(CSR_E_DBG_MRANGE_VBASE                      , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MRANGE_VSIZE                      , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MSID_0                            , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MSID_1                            , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SRANGE_VBASE                      , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SRANGE_VSIZE                      , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SSID_0                            , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SSID_1                            , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SECS                              , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_TCS                               , t0, a0, 0xFEDCBA012345789F, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK                , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & 0x7FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_XRANGE_MAP     , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_XRANGE_MAP_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_XRANGE_MAP_MASK, t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_XRANGE_MAP_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PRV_LVL        , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PRV_LVL_MASK   , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_TWEAK_EN       , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_TWEAK_ENABLE_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_MSB        , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_MSB_MASK   , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_LSB        , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_LSB_MASK   , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK                , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & 0x7FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PRV_LVL        , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PRV_LVL_MASK   , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_PRV_LVL_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_TWEAK_EN       , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_TWEAK_ENABLE_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_MSB        , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_MSB_MASK   , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_MSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_LSB        , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_LSB_MASK   , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_LSB_MASK, fail)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STATUS                            , t0, a0, 0xFEDCBA012345789F, fail)
+nop
+nop
+nop
+nop
+nop
+
+
+li TESTNUM, 700
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0000000000000000)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_LSB, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x00000000000003FF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_LSB_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x00000000000FFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_MSB, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x000000003FFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_MSB_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x000000FFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_TWEAK_EN, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x000001FFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PRV_LVL, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x000007FFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PRV_LVL_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x00001FFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_XRANGE_MAP, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0000FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_XRANGE_MAP_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0000000000000000)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_LSB, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x00000000000003FF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_LSB_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x00000000000FFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_MSB, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x000000003FFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_MSB_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x000000FFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_TWEAK_EN, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x000001FFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PRV_LVL, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x000007FFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PRV_LVL_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x00001FFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_XRANGE_MAP, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0000FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_XRANGE_MAP_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_LSB, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFC00, fail)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_LSB_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFF003FF, fail)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_MSB, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFC00FFFFF, fail)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_MSB_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FF003FFFFFFF, fail)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_TWEAK_EN, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FEFFFFFFFFFF, fail)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PRV_LVL, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007F9FFFFFFFFFF, fail)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PRV_LVL_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007E7FFFFFFFFFF, fail)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_XRANGE_MAP, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x00071FFFFFFFFFFF, fail)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_XRANGE_MAP_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0000FFFFFFFFFFFF, fail)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_LSB, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFC00, fail)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_LSB_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFF003FF, fail)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_MSB, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFC00FFFFF, fail)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_MSB_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FF003FFFFFFF, fail)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_TWEAK_EN, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FEFFFFFFFFFF, fail)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PRV_LVL, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007F9FFFFFFFFFF, fail)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PRV_LVL_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007E7FFFFFFFFFF, fail)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_XRANGE_MAP, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x00071FFFFFFFFFFF, fail)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_XRANGE_MAP_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0000FFFFFFFFFFFF, fail)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, fail)
+
+
+
+RVTEST_PASS
+
+fail:
+  RVTEST_FAIL
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+  TEST_DATA
+
+RVTEST_DATA_END
diff --git a/isa/rv64mz/machine_store_tweak.S b/isa/rv64mz/machine_store_tweak.S
new file mode 100644
index 0000000..6195104
--- /dev/null
+++ b/isa/rv64mz/machine_store_tweak.S
@@ -0,0 +1,198 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# machine_store_tweak.S
+#-----------------------------------------------------------------------------
+#
+# This testcase writes to the userspace using the stweak register.
+# First the current content of the physical memory is checked. Values are written
+# to and read back from the virtual memory. Lastly the content is checked over a
+# physical memory access and must be different from the original value
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+
+csrr t3, mstatus
+
+setup_m_to_u_vm(a1)
+
+// Set SID for write and configure mrange
+write_value_to_csr(         0x8000000000000001, CSR_E_MSID_0 , a1)
+write_value_to_csr((PAGE_TWO  - PAGE_ZERO  ) , CSR_E_MRANGE_VBASE, a1)
+write_value_to_csr((PAGE_FOUR - PAGE_TWO - 1), CSR_E_MRANGE_VSIZE, a1)
+
+// set the unaligned elrangetest
+set_bit_in_csr(Z_DBG_VSIZE_XRANGE_IS_SIZE_NOT_MASK_NO_ALIGNMENT, CSR_E_DBG_CONTROL, a1)
+
+nop;
+nop;
+nop;
+nop;
+
+quick_disable_m_vm_access(a0)
+li TESTNUM, 0x1
+check_physical_memory(dummy_001, a0, a1, 0xBEEFBEEF, die)
+li TESTNUM, 0x2
+check_physical_memory(dummy_102, a0, a1, 0xBEEFBEEF, die)
+li TESTNUM, 0x3
+check_physical_memory(dummy_203, a0, a1, 0xBEEFBEEF, die)
+li TESTNUM, 0x4
+check_physical_memory(dummy_300, a0, a1, 0xBEEFBEEF, die)
+nop;
+nop;
+nop;
+
+li a1, PAGE_ZERO
+li TESTNUM, 0x10
+set_and_enable_load_tweak(a0, E_TWEAK_UNPROTECTED_U)
+write_with_stweak_no_verify(E_TWEAK_UNPROTECTED_U, a0, t0, t1, dummy_001, a1, 0xBADEAFFE, die)
+li TESTNUM, 0x11
+write_with_stweak_no_verify(E_TWEAK_UNPROTECTED_U, a0, t0, t1, dummy_102, a1, 0xBADEAFFE, die)
+li TESTNUM, 0x12
+set_and_enable_load_tweak(a0, E_TWEAK_UNPROTECTED_U)
+write_with_stweak_no_verify(E_TWEAK_ENCLAVE, a0, t0, t1, dummy_203, a1, 0xBADEAFFE, die)
+li TESTNUM, 0x13
+write_with_stweak_no_verify(E_TWEAK_ENCLAVE, a0, t0, t1, dummy_300, a1, 0xBADEAFFE, die)
+disable_lstweaks(a0)
+
+nop;
+nop;
+nop;
+
+quick_disable_m_vm_access(a0)
+// unencrypted page
+li TESTNUM, 0x20
+check_physical_memory(dummy_001, a0, a1, 0xBADEAFFE, die)
+li TESTNUM, 0x21
+check_physical_memory(dummy_102, a0, a1, 0xBADEAFFE, die)
+li TESTNUM, 0x23
+check_physical_memory_change(dummy_203, a0, a1, 0xBEEFBEEF, die)
+li TESTNUM, 0x25
+check_physical_memory_change(dummy_300, a0, a1, 0xBEEFBEEF, die)
+
+nop;
+nop;
+// ONLY for spike and the NOT "encryption", as we know the outcome of the encryption
+# li TESTNUM, 0x31
+# check_physical_memory(dummy_203, a0, a1, inv_32bit(0xBADEAFFE), die)
+# li TESTNUM, 0x32
+# check_physical_memory(dummy_300, a0, a1, inv_32bit(0xBADEAFFE), die)
+
+
+j end
+
+
+my_synchronous_exception:
+m_ecall_end:
+  li a0, 0xDEADDEAD
+no_ecall:
+  li t0, 0xAAAAAAAA
+
+  bnez a7, die
+  j end
+
+
+end:
+  li a0, 0xCAFEBABE
+  li t0, 0xAAAAAAAA
+  nop
+  nop
+
+RVTEST_PASS
+
+die:
+  li t0, 0xDEADDEAD
+  li a0, 0xDEADDEAD
+  RVTEST_FAIL
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ZERO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_2: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ONE   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_3: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_TWO   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_4: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_THREE / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_5: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_FOUR  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... befor an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0xDEADBEEF
+dummy_001: .word 0xBEEFBEEF
+dummy_002: .word 0xDEADBEEF
+dummy_003: .word 0xDEADBEEF
+dummy_004: .word 0xDEADBEEF
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+.align 12
+dummy_100: .word 0xDEADBEEF
+dummy_101: .word 0xDEADBEEF
+dummy_102: .word 0xBEEFBEEF
+dummy_103: .word 0xDEADBEEF
+dummy_104: .word 0xDEADBEEF
+
+# PAGE_TWO
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0xDEADBEEF
+dummy_201: .word 0xDEADBEEF
+dummy_202: .word 0xDEADBEEF
+dummy_203: .word 0xBEEFBEEF
+dummy_204: .word 0xDEADBEEF
+
+# PAGE_THREE
+# dummy data on separate page to test everything
+.align 12
+dummy_300: .word 0xBEEFBEEF
+dummy_301: .word 0xDEADBEEF
+dummy_302: .word 0xDEADBEEF
+dummy_303: .word 0xDEADBEEF
+dummy_304: .word 0xDEADBEEF
+
+# PAGE_FOUR
+# dummy data on separate page to test everything
+.align 12
+dummy_400: .word 0xDEADBEEF
+dummy_401: .word 0xDEADBEEF
+dummy_402: .word 0xDEADBEEF
+dummy_403: .word 0xDEADBEEF
+dummy_404: .word 0xDEADBEEF
+
+RVTEST_DATA_END
diff --git a/isa/rv64uz/Makefrag b/isa/rv64uz/Makefrag
new file mode 100644
index 0000000..a78f884
--- /dev/null
+++ b/isa/rv64uz/Makefrag
@@ -0,0 +1,26 @@
+#=======================================================================
+# Makefrag for rv64uz tests
+#-----------------------------------------------------------------------
+
+rv64uz_sc_tests = \
+	enclave_load_encrypted_memory \
+	machine_store_tweak_user_load \
+	enclave_execute_encrypted_memory_simple \
+	load_authentication_exception \
+	store_authentication_exception \
+	fetch_authentication_exception \
+	page_type_data_access \
+	modify_pte \
+	basic_csr_access \
+	tweaked_vaddr_paddr_access \
+	double_mapping \
+	vectored_exceptions_base \
+	vectored_exceptions_extended \
+	enclave_load_store_byte_sizes \
+	elrange_access_performance \
+	elrange_access_precise_bounds \
+	load_store_operation_for_hw
+
+rv64uz_p_tests = $(addprefix rv64uz-p-, $(rv64uz_sc_tests))
+
+spike64_tests += $(rv64uz_p_tests)
diff --git a/isa/rv64uz/basic_csr_access.S b/isa/rv64uz/basic_csr_access.S
new file mode 100644
index 0000000..8516a6f
--- /dev/null
+++ b/isa/rv64uz/basic_csr_access.S
@@ -0,0 +1,479 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# basic_csr_access.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+#define EXPECTED_EXCEPTIONS 6
+
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+
+
+# prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+li TESTNUM, 5
+mret
+
+.align 8
+// Last parameter must hold the number of expected exceptions!
+usermode_exit_handler(a0, t0, t3, a7)
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_UNPROTECTED | (( PAGE_ZERO / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_2: .dword E_PT_UNPROTECTED | (( PAGE_ONE  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0x00123456
+dummy_001: .word 0xCAFEBABE
+dummy_002: .word 0xFFFFFFFF
+dummy_003: .word 0xABCDEF12
+dummy_004: .word 0xDEADBEEF
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x1
+
+set_bit_in_csr(Z_DBG_ALLOW_UNSAFE_CODE_FETCHES, CSR_E_DBG_CONTROL, t0)
+
+// #########################################
+// write the userspace debug views of these registers
+li TESTNUM, 100
+test_and_check_csr(CSR_E_DBG_MRANGE_VBASE          , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MRANGE_VSIZE          , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MSID_0                , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MSID_1                , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SRANGE_VBASE          , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SRANGE_VSIZE          , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SSID_0                , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SSID_1                , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SECS                  , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_TCS                   , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK                , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_XRANGE_MAP     , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_XRANGE_MAP_MASK, t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_PRV_LVL        , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_PRV_LVL_MASK   , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_TWEAK_EN       , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_PTE_MSB        , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_PTE_MSB_MASK   , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_PTE_LSB        , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_LTWEAK_PTE_LSB_MASK   , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK                , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK_PRV_LVL        , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK_PRV_LVL_MASK   , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK_TWEAK_EN       , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK_PTE_MSB        , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK_PTE_MSB_MASK   , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK_PTE_LSB        , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STWEAK_PTE_LSB_MASK   , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STATUS                , t0, a0, 0x0, die_u)
+
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_URANGE_VBASE              , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_URANGE_VSIZE              , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_USID_0                    , t0, a0, 0x0, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_USID_1                    , t0, a0, 0x0, die_u)
+
+nop
+nop
+
+li TESTNUM, 200
+test_and_check_csr(CSR_E_DBG_MRANGE_VBASE                      , t0, a0, 0xFFFFFFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MRANGE_VSIZE                      , t0, a0, 0xFFFFFFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MSID_0                            , t0, a0, 0xFFFFFFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MSID_1                            , t0, a0, 0xFFFFFFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SRANGE_VBASE                      , t0, a0, 0xFFFFFFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SRANGE_VSIZE                      , t0, a0, 0xFFFFFFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SSID_0                            , t0, a0, 0xFFFFFFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SSID_1                            , t0, a0, 0xFFFFFFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SECS                              , t0, a0, 0xFFFFFFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_TCS                               , t0, a0, 0xFFFFFFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK                , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & 0x7FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_XRANGE_MAP     , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_XRANGE_MAP_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_XRANGE_MAP_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_XRANGE_MAP_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PRV_LVL        , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_PRV_LVL_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PRV_LVL_MASK   , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_PRV_LVL_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_TWEAK_EN       , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_TWEAK_ENABLE_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_MSB        , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_MSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_MSB_MASK   , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_MSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_LSB        , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_LSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_LSB_MASK   , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_LSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK                , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & 0x7FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PRV_LVL        , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_PRV_LVL_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PRV_LVL_MASK   , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_PRV_LVL_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_TWEAK_EN       , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_TWEAK_ENABLE_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_MSB        , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_MSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_MSB_MASK   , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_MSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_LSB        , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_LSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_LSB_MASK   , t0, a0, 0xFFFFFFFFFFFFFFFF, 0xFFFFFFFFFFFFFFFF & E_LSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STATUS                            , t0, a0, 0xFFFFFFFFFFFFFFFF, die_u)
+
+
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_URANGE_VBASE                          , t0, a0, 0xFFFFFFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_URANGE_VSIZE                          , t0, a0, 0xFFFFFFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_USID_0                                , t0, a0, 0xFFFFFFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_USID_1                                , t0, a0, 0xFFFFFFFFFFFFFFFF, die_u)
+
+nop
+nop
+
+li TESTNUM, 300
+test_and_check_csr(CSR_E_DBG_MRANGE_VBASE                      , t0, a0, 0xFEDCBA012345789F, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MRANGE_VSIZE                      , t0, a0, 0xFEDCBA012345789F, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MSID_0                            , t0, a0, 0xFEDCBA012345789F, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_MSID_1                            , t0, a0, 0xFEDCBA012345789F, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SRANGE_VBASE                      , t0, a0, 0xFEDCBA012345789F, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SRANGE_VSIZE                      , t0, a0, 0xFEDCBA012345789F, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SSID_0                            , t0, a0, 0xFEDCBA012345789F, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SSID_1                            , t0, a0, 0xFEDCBA012345789F, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_SECS                              , t0, a0, 0xFEDCBA012345789F, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_TCS                               , t0, a0, 0xFEDCBA012345789F, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK                , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & 0x7FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_XRANGE_MAP     , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_XRANGE_MAP_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_XRANGE_MAP_MASK, t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_XRANGE_MAP_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PRV_LVL        , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_PRV_LVL_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PRV_LVL_MASK   , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_PRV_LVL_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_TWEAK_EN       , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_TWEAK_ENABLE_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_MSB        , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_MSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_MSB_MASK   , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_MSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_LSB        , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_LSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_LTWEAK_PTE_LSB_MASK   , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_LSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK                , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & 0x7FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PRV_LVL        , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_PRV_LVL_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PRV_LVL_MASK   , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_PRV_LVL_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_TWEAK_EN       , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_TWEAK_ENABLE_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_MSB        , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_MSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_MSB_MASK   , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_MSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_LSB        , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_LSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr_expect_diff(CSR_E_DBG_STWEAK_PTE_LSB_MASK   , t0, a0, 0xFEDCBA012345789F, 0xFEDCBA012345789F & E_LSB_MASK, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_DBG_STATUS                            , t0, a0, 0xFEDCBA012345789F, die_u)
+
+
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_URANGE_VBASE                          , t0, a0, 0xFEDCBA012345789F, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_URANGE_VSIZE                          , t0, a0, 0xFEDCBA012345789F, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_USID_0                                , t0, a0, 0xFEDCBA012345789F, die_u)
+addi TESTNUM, TESTNUM, 1
+test_and_check_csr(CSR_E_USID_1                                , t0, a0, 0xFEDCBA012345789F, die_u)
+nop
+nop
+nop
+nop
+nop
+
+
+li TESTNUM, 400
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0000000000000000)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_LSB, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x00000000000003FF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_LSB_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x00000000000FFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_MSB, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x000000003FFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_MSB_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x000000FFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_TWEAK_EN, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x000001FFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PRV_LVL, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x000007FFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PRV_LVL_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x00001FFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_XRANGE_MAP, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0000FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_XRANGE_MAP_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0000000000000000)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_LSB, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x00000000000003FF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_LSB_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x00000000000FFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_MSB, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x000000003FFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_MSB_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x000000FFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_TWEAK_EN, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x000001FFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PRV_LVL, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x000007FFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PRV_LVL_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x00001FFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_XRANGE_MAP, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0000FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_XRANGE_MAP_MASK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_LSB, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFC00, die_u)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_LSB_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFF003FF, die_u)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_MSB, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFC00FFFFF, die_u)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PTE_MSB_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FF003FFFFFFF, die_u)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_TWEAK_EN, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FEFFFFFFFFFF, die_u)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PRV_LVL, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007F9FFFFFFFFFF, die_u)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_PRV_LVL_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007E7FFFFFFFFFF, die_u)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_XRANGE_MAP, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x00071FFFFFFFFFFF, die_u)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_LTWEAK_XRANGE_MAP_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0000FFFFFFFFFFFF, die_u)
+write_csr(CSR_E_DBG_LTWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_LTWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_LSB, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFC00, die_u)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_LSB_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFF003FF, die_u)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_MSB, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFC00FFFFF, die_u)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PTE_MSB_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FF003FFFFFFF, die_u)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_TWEAK_EN, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FEFFFFFFFFFF, die_u)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PRV_LVL, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007F9FFFFFFFFFF, die_u)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_PRV_LVL_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007E7FFFFFFFFFF, die_u)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_XRANGE_MAP, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x00071FFFFFFFFFFF, die_u)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+addi TESTNUM, TESTNUM, 1
+write_csr(CSR_E_DBG_STWEAK_XRANGE_MAP_MASK, t0, a0, 0x0)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0000FFFFFFFFFFFF, die_u)
+write_csr(CSR_E_DBG_STWEAK, t0, a0, 0xFFFFFFFFFFFFFFFF)
+check_csr(CSR_E_DBG_STWEAK, t0, a0, 0x0007FFFFFFFFFFFF, die_u)
+nop
+nop
+nop
+nop
+nop
+li TESTNUM, 800
+li a7, 0
+ecall
+  
+RVTEST_CODE_END
+
+
+die_u:
+  li a7, 0x1
+  ecall
+  
+
+RVTEST_CODE_END
+
+RVTEST_DATA_END
diff --git a/isa/rv64uz/double_mapping.S b/isa/rv64uz/double_mapping.S
new file mode 100644
index 0000000..19d5e9f
--- /dev/null
+++ b/isa/rv64uz/double_mapping.S
@@ -0,0 +1,312 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# double_mapping.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+#define EXPECTED_EXCEPTIONS 16
+#define USER_TEST_START 11
+
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+setup_m_to_u_vm(a1)
+
+// Set SID for write set mrange, disable aligned ranges
+write_value_to_csr(         0x8000000000000001, CSR_E_MSID_0 , a1)
+write_value_to_csr((PAGE_TWO  - PAGE_ZERO)     , CSR_E_MRANGE_VBASE, a1)
+write_value_to_csr((PAGE_SIX - PAGE_TWO - 1) , CSR_E_MRANGE_VSIZE, a1)
+set_bit_in_csr(Z_DBG_VSIZE_XRANGE_IS_SIZE_NOT_MASK_NO_ALIGNMENT, CSR_E_DBG_CONTROL, a1)
+
+li TESTNUM, 1
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+li TESTNUM, 2
+set_and_enable_store_tweak(a1, E_TWEAK_ENCLAVE)
+li TESTNUM, 3
+// Encrypt Page Two
+copy_vma_to_enclave_vma(PAGE_TWO, PAGE_THREE, PAGE_TWO, PAGE_ZERO, a1, a2, a3, a4)
+
+li TESTNUM, 4
+// PAGE_THREE is also mapped as page five. Encrypt page two as if it were page four
+copy_vma_to_enclave_vma(PAGE_FIVE, PAGE_SIX, PAGE_FIVE, PAGE_ZERO, a1, a2, a3, a4)
+
+disable_lstweaks(a1)
+quick_disable_m_vm_access(a1)
+
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+# prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+li TESTNUM, 5
+mret
+
+adapted_ecall:
+  la a0, exception_save_slot;
+  lwu t0, 0(a0);
+  bnez a7, die;
+  li a0, EXPECTED_EXCEPTIONS;
+  beq t0, a0, end;
+  j die;
+
+return_to_user_jump_table_init:
+  addi a0, TESTNUM, (-(USER_TEST_START)); // align to counter start in usermode
+  slli a0, a0, 2;
+  la t0, return_to_user_jump_table;
+  add t0, a0, t0;
+  jalr x0, 0(t0);
+
+return_to_user_jump_table:
+  j die;                   // access on PAGE_ZERO
+  j die;                   // access on PAGE_ZERO
+  j die;                   // access on PAGE_ZERO
+  j die;                   // access on PAGE_ONE
+  j die;                   // access on PAGE_ONE
+  j die;                   // access on PAGE_ONE
+  j die;                   // access on PAGE_ONE via PAGE_FIVE
+  j die;                   // access on PAGE_ONE via PAGE_FIVE
+  j die;                   // access on PAGE_ONE via PAGE_FIVE
+  j die;                   // access on PAGE_TWO (weak)
+  j die;                   // access on PAGE_TWO (weak)
+  j die;                   // access on PAGE_TWO (weak)
+  j die;                   // access on PAGE_TWO via PAGE_SIX (weak)
+  j die;                   // access on PAGE_TWO via PAGE_SIX (weak)
+  j die;                   // access on PAGE_TWO via PAGE_SIX (weak)
+  j die;                   // access on PAGE_THREE (weak)
+  j die;                   // access on PAGE_THREE (weak)
+  j die;                   // access on PAGE_THREE (weak)
+  j die;                   // access on PAGE_THREE via PAGE_SEVEN (weak)
+  j die;                   // access on PAGE_THREE via PAGE_SEVEN (weak)
+  j die;                   // access on PAGE_THREE via PAGE_SEVEN (weak)
+  j die;                   // access on PAGE_TWO (strict)
+  j die;                   // access on PAGE_TWO (strict)
+  j die;                   // access on PAGE_TWO (strict)
+  j skip_instruction;      // access on PAGE_TWO via PAGE_SIX (strict)
+  j skip_instruction;      // access on PAGE_TWO via PAGE_SIX (strict)
+  j return_to_caller;      // access on PAGE_TWO via PAGE_SIX (strict)
+  j skip_instruction;      // access on PAGE_THREE (strict)
+  j skip_instruction;      // access on PAGE_THREE (strict)
+  j return_to_caller;      // access on PAGE_THREE (strict)
+  j die;                   // access on PAGE_THREE via PAGE_SEVEN (strict)
+  j die;                   // access on PAGE_THREE via PAGE_SEVEN (strict)
+  j die;                   // access on PAGE_THREE via PAGE_SEVEN (strict)
+  nop
+  nop
+  nop
+  j die
+
+skip_instruction:
+  csrr t0, mepc;
+  ADDI t0, t0, 4;
+  csrw mepc, t0;
+  j return_to_userland
+
+return_to_caller:
+  set_user_pc_to_return_reg(PERSISTANT_REG_FOR_RET)
+  j return_to_userland
+
+# set_user_pc_to_userlabel(a0, t0, fetch_test_08, PAGE_ZERO)
+return_to_userland:
+  mret
+
+.align 8
+// Last parameter must hold the number of expected exceptions!
+usermode_exit_handler_success_on_fetch_decryption_error(a0, t0, t3, a7, EXPECTED_EXCEPTIONS, adapted_ecall, decryption_exception)
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ZERO / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+// first mapping of pages
+page_table_leaf_entry_2: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ONE  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_3: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_TWO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_4: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_THREE/ RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+// regular fourth page
+page_table_leaf_entry_7: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_TWO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_8: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_THREE/ RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_5: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_FOUR / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+// second mapping of pages PAGE_FIVE, PAGE_SIX and PAGE_SEVEN map on PAGE_ONE PAGE_TWO PAGE_THREE
+page_table_leaf_entry_6: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ONE  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0x00123456
+dummy_001: .word 0xCAFEBABE
+dummy_002: .word 0xFFFFFFFF
+dummy_003: .word 0xABCDEF12
+dummy_004: jump_and_check_return_dst(persistant_return_reg)
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x0
+
+  # this must pass fine!
+  li TESTNUM, 10
+  // 11 - 13
+  // access on same unprotected page
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_000, dummy_002, dummy_004, 0x00123456, 0xAFFED00F, die_u)
+  nop
+  nop
+  nop
+  // 14 - 16
+  // access on PAGE_ONE
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_100, dummy_104, dummy_109, 0x11111111, 0xAFFED00F, die_u)
+  nop
+  // 17 - 19
+  // access on PAGE_ONE via PAGE_FIVE
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, (dummy_101 + (PAGE_SEVEN-PAGE_ONE)), (dummy_105 + (PAGE_SEVEN-PAGE_ONE)), (dummy_109 + (PAGE_SEVEN-PAGE_ONE)), 0x10101010, 0xAFFED00F, die_u)
+  
+  nop
+  nop
+  nop
+
+  ////////////////////////////////////////////////////////
+
+  // weak authentication -> all accesses should work
+  set_bit_in_csr(Z_DBG_AUTH_WEAK, CSR_E_DBG_CONTROL, t0)
+  // 20 - 22
+  // access on PAGE_TWO
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_200, dummy_204, dummy_209, 0x22222222, 0xAFFED00F, die_u)
+  nop
+  // 23 - 25
+  // access on PAGE_TWO via PAGE_SIX
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, (dummy_201 + (PAGE_FOUR-PAGE_TWO)), (dummy_205 + (PAGE_FOUR-PAGE_TWO)), (dummy_209 + (PAGE_FOUR-PAGE_TWO)), 0x20202020, 0xAFFED00F, die_u)
+  nop
+  nop
+  // 26 - 28
+  // access on PAGE_THREE
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_300, dummy_304, dummy_309, 0x33333333, 0xAFFED00F, die_u)
+  nop
+  // 29 - 31
+  // access on PAGE_THREE via PAGE_SEVEN
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, (dummy_301 + (PAGE_FIVE-PAGE_THREE)), (dummy_305 + (PAGE_FIVE-PAGE_THREE)), (dummy_309 + (PAGE_FIVE-PAGE_THREE)), 0x30303030, 0xAFFED00F, die_u)
+  
+  nop
+  nop
+  nop
+
+  ////////////////////////////////////////////////////////
+
+  // strict authentication -> accesses on double mappings should fail
+  clear_bit_in_csr(Z_DBG_AUTH_WEAK, CSR_E_DBG_CONTROL, t0)
+  // 32 - 34
+  // access on PAGE_TWO should work (initial mapping)
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_202, dummy_206, dummy_209, 0x02020202, 0xAFFED00F, die_u)
+  nop
+  // 35 - 37
+  // access on PAGE_TWO via PAGE_SIX double mapping should fail
+  load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, (dummy_203 + (PAGE_FOUR-PAGE_TWO)), (dummy_207 + (PAGE_FOUR-PAGE_TWO)), (dummy_208 + (PAGE_FOUR-PAGE_TWO)), 0x22220000, 0xAFFED00F, die_u)
+  nop
+  nop
+  // 38 - 40
+  // access on PAGE_THREE double mapping should fail
+  load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_302, dummy_306, dummy_308, 0x03030303, 0xAFFED00F, die_u)
+  nop
+  // 41 - 43
+  // access on PAGE_THREE via PAGE_SEVEN (initial mapping)
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, (dummy_303 + (PAGE_FIVE-PAGE_THREE)), (dummy_307 + (PAGE_FIVE-PAGE_THREE)), (dummy_309 + (PAGE_FIVE-PAGE_THREE)), 0x33330000, 0xAFFED00F, die_u)
+
+  nop
+  nop
+  nop
+
+  li TESTNUM, 100
+  ecall
+  
+  RVTEST_CODE_END
+
+die_u:
+  li a7, 0x1
+  ecall
+  
+
+RVTEST_CODE_END
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+.align 12
+dummy_100: .word 0x11111111
+dummy_101: .word 0x10101010
+dummy_102: .word 0x01010101
+dummy_103: .word 0x11110000
+dummy_104: .word 0xDEADBEEF
+dummy_105: .word 0xDEADBEEF
+dummy_106: .word 0xDEADBEEF
+dummy_107: .word 0xDEADBEEF
+dummy_108: j die_u
+dummy_109: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_TWO
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0x22222222
+dummy_201: .word 0x20202020
+dummy_202: .word 0x02020202
+dummy_203: .word 0x22220000
+dummy_204: .word 0xDEADBEEF
+dummy_205: .word 0xDEADBEEF
+dummy_206: .word 0xDEADBEEF
+dummy_207: .word 0xDEADBEEF
+dummy_208: j die_u
+dummy_209: jump_and_check_return_dst(persistant_return_reg)
+
+
+# PAGE_THREE
+# dummy data on separate page to test everything
+.align 12
+dummy_300: .word 0x33333333
+dummy_301: .word 0x30303030
+dummy_302: .word 0x03030303
+dummy_303: .word 0x33330000
+dummy_304: .word 0xDEADBEEF
+dummy_305: .word 0xDEADBEEF
+dummy_306: .word 0xDEADBEEF
+dummy_307: .word 0xDEADBEEF
+dummy_308: j die_u
+dummy_309: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_THREE
+# dummy data on separate page to test everything
+.align 12
+RVTEST_DATA_END
diff --git a/isa/rv64uz/elrange_access_performance.S b/isa/rv64uz/elrange_access_performance.S
new file mode 100644
index 0000000..95a6c50
--- /dev/null
+++ b/isa/rv64uz/elrange_access_performance.S
@@ -0,0 +1,510 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# double_mapping.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+#define EXPECTED_EXCEPTIONS 50
+#define USER_TEST_START 11
+
+#define ACTIVATE_ENCLAVE_ECALL_CODE 2
+
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+setup_m_to_u_vm(a1)
+
+// Set SID for write
+write_value_to_csr(         0x8000000000000001, CSR_E_MSID_0 , a1)
+write_value_to_csr((PAGE_SIXTEEN  - PAGE_ZERO  )      , CSR_E_MRANGE_VBASE, a1)
+write_value_to_csr((PAGE_TWENTY - PAGE_SIXTEEN - 1)   , CSR_E_MRANGE_VSIZE, a1)
+
+clear_bit_in_csr(Z_DBG_VSIZE_XRANGE_IS_SIZE_NOT_MASK_NO_ALIGNMENT, CSR_E_DBG_CONTROL, a1)
+clear_bit_in_csr(E_STATUS_ENCLAVE_MODE_EN, CSR_E_STATUS, a1)
+
+
+li TESTNUM, 1
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+set_and_enable_store_tweak(a1, E_TWEAK_ENCLAVE)
+// Encrypt Page Two
+copy_vma_to_enclave_vma(PAGE_SIXTEEN     , PAGE_SEVENTEEN  , PAGE_SIXTEEN  , PAGE_ZERO, a1, a2, a3, a4)
+li TESTNUM, 2
+copy_vma_to_enclave_vma(PAGE_SEVENTEEN   , PAGE_EIGHTEEN   , PAGE_SEVENTEEN , PAGE_ZERO, a1, a2, a3, a4)
+li TESTNUM, 3
+copy_vma_to_enclave_vma(PAGE_NINETEEN    , PAGE_TWENTY     , PAGE_NINETEEN , PAGE_ZERO, a1, a2, a3, a4)
+li TESTNUM, 4
+copy_vma_to_enclave_vma(PAGE_SIX         , PAGE_SEVEN      , PAGE_SIX      , PAGE_ZERO, a1, a2, a3, a4)
+
+disable_lstweaks(a1)
+quick_disable_m_vm_access(a1)
+
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+# prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+li TESTNUM, 5
+mret
+
+adapted_ecall:
+  la a0, exception_save_slot
+  lwu t0, 0(a0)
+  li a0, ACTIVATE_ENCLAVE_ECALL_CODE
+  beq a7, a0, enable_enclave_mode
+  bnez a7, die
+  li a0, EXPECTED_EXCEPTIONS
+  beq t0, a0, end
+  j die
+
+enable_enclave_mode:
+  set_bit_in_csr(E_STATUS_ENCLAVE_MODE_EN, CSR_E_STATUS, a0)
+  set_user_pc_to_userlabel(a0, t0, enclave_superstart, PAGE_ZERO)
+  j return_to_userland
+
+return_to_user_jump_table_init:
+  addi a0, TESTNUM, (-(USER_TEST_START)); // align to counter start in usermode
+  slli a0, a0, 2;
+  la t0, return_to_user_jump_table;
+  add t0, a0, t0;
+  jalr x0, 0(t0);
+
+return_to_user_jump_table:
+  j die;                   // load, store execute unprotected page outside elrange (pass, pass, pass)
+  j die;                   // load, store execute unprotected page outside elrange (pass, pass, pass)
+  j die;                   // load, store execute unprotected page outside elrange (pass, pass, pass)
+  j skip_instruction;      // load, store execute unprotected page in elrange (fail, fail, fail)
+  j skip_instruction;      // load, store execute unprotected page in elrange (fail, fail, fail)
+  j return_to_caller;      // load, store execute unprotected page in elrange (fail, fail, fail)
+  j skip_instruction;      // load, store execute protected page outside elrange (pass, pass, pass)
+  j skip_instruction;      // load, store execute protected page outside elrange (pass, pass, pass)
+  j return_to_caller;      // load, store execute protected page outside elrange (pass, pass, pass)
+  j die;                   // load, store execute protected page in elrange (pass, pass, pass)
+  j die;                   // load, store execute protected page in elrange (pass, pass, pass)
+  j die;                   // load, store execute protected page in elrange (pass, pass, pass)
+  j die;                   // load store execute protected page in elrange (pass, pass, pass)
+  j die;                   // load store execute protected page in elrange (pass, pass, pass)
+  j die;                   // load store execute protected page in elrange (pass, pass, pass)
+  j skip_instruction;      // load store execute unprotected page in elrange (fail, fail, fail)
+  j skip_instruction;      // load store execute unprotected page in elrange (fail, fail, fail)
+  j return_to_caller;      // load store execute unprotected page in elrange (fail, fail, fail)
+  j skip_instruction;      // load store execute protected page outside elrange (pass, pass, fail)
+  j skip_instruction;      // load store execute protected page outside elrange (pass, pass, fail)
+  j return_to_caller;      // load store execute protected page outside elrange (pass, pass, fail)
+  j die;                   // load store execute unprotected page outside elrange (pass, pass, fail)
+  j die;                   // load store execute unprotected page outside elrange (pass, pass, fail)
+  j return_to_caller;      // load store execute unprotected page outside elrange (pass, pass, fail)
+  j die;                   // load store execute protected page in elrange (pass, pass, pass)
+  j die;                   // load store execute protected page in elrange (pass, pass, pass)
+  j die;                   // load store execute protected page in elrange (pass, pass, pass)
+  j skip_instruction;      // load store execute unprotected page in elrange (fail, fail, fail)
+  j skip_instruction;      // load store execute unprotected page in elrange (fail, fail, fail)
+  j return_to_caller;      // load store execute unprotected page in elrange (fail, fail, fail)
+  j skip_instruction;      // load store execute protected page outside elrange (pass, pass, pass)
+  j skip_instruction;      // load store execute protected page outside elrange (pass, pass, pass)
+  j return_to_caller;      // load store execute protected page outside elrange (pass, pass, pass)
+  j die;                   // load store execute unprotected page outside elrange (pass, pass, pass)
+  j die;                   // load store execute unprotected page outside elrange (pass, pass, pass)
+  j die;                   // load store execute unprotected page outside elrange (pass, pass, pass)
+  j die;                   // upper bound check
+  j die;                   // upper bound check
+  nop
+  nop
+  nop
+  j die
+
+skip_instruction:
+  csrr t0, mepc;
+  ADDI t0, t0, 4;
+  csrw mepc, t0;
+  j return_to_userland
+
+return_to_caller:
+  set_user_pc_to_return_reg(PERSISTANT_REG_FOR_RET)
+  j return_to_userland
+
+# set_user_pc_to_userlabel(a0, t0, fetch_test_08, PAGE_ZERO)
+return_to_userland:
+  mret
+
+.align 8
+// Last parameter must hold the number of expected exceptions!
+usermode_exit_handler_success_on_fetch_decryption_error(a0, t0, t3, a7, EXPECTED_EXCEPTIONS, adapted_ecall, decryption_exception)
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword  E_PT_UNPROTECTED << PTE_MSB_SHIFT  | (( PAGE_ZERO      / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_2: .dword  E_PT_UNPROTECTED << PTE_MSB_SHIFT  | (( PAGE_ONE       / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_3: .dword  E_PT_UNPROTECTED << PTE_MSB_SHIFT  | (( PAGE_TWO       / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_4: .dword  E_PT_UNPROTECTED << PTE_MSB_SHIFT  | (( PAGE_THREE     / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_5: .dword  E_PT_UNPROTECTED << PTE_MSB_SHIFT  | (( PAGE_FOUR      / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_6: .dword  E_PT_UNPROTECTED << PTE_MSB_SHIFT  | (( PAGE_FIVE      / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_7: .dword  E_PT_REGULAR     << PTE_MSB_SHIFT  | (( PAGE_SIX       / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_8: .dword  E_PT_UNPROTECTED << PTE_MSB_SHIFT  | (( PAGE_SEVEN     / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_9: .dword  E_PT_UNPROTECTED << PTE_MSB_SHIFT  | (( PAGE_EIGHT     / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_A: .dword  E_PT_UNPROTECTED << PTE_MSB_SHIFT  | (( PAGE_NINE      / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_B: .dword  E_PT_UNPROTECTED << PTE_MSB_SHIFT  | (( PAGE_TEN       / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_C: .dword  E_PT_UNPROTECTED << PTE_MSB_SHIFT  | (( PAGE_ELEVEN    / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_D: .dword  E_PT_UNPROTECTED << PTE_MSB_SHIFT  | (( PAGE_TWELVE    / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_E: .dword  E_PT_UNPROTECTED << PTE_MSB_SHIFT  | (( PAGE_THIRTEEN  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_F: .dword  E_PT_UNPROTECTED << PTE_MSB_SHIFT  | (( PAGE_FOURTEEN  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_10: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_FIFTEEN   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+// start of elrange
+page_table_leaf_entry_11: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_SIXTEEN   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_12: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_SEVENTEEN / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_13: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_EIGHTEEN  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_14: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_NINETEEN  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+// end of elrange
+page_table_leaf_entry_15: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_TWENTY    / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0x00123456
+dummy_001: .word 0xCAFEBABE
+dummy_002: .word 0xFFFFFFFF
+dummy_003: .word 0xABCDEF12
+dummy_004: jump_and_check_return_dst(persistant_return_reg)
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x0
+
+  // test strategy:
+  // exceptions for fails: 2, 4, 2
+  // non enclave mode
+  // load, store execute unprotected page outside elrange (pass, pass, pass)
+  // load, store execute unprotected page in elrange (fail, fail, fail)
+  // load, store execute protected page outside elrange (fail, fail, fail)
+  // load, store execute protected page in elrange (pass, pass, pass)
+  // switch to enclavemode (DON'T allow unprotected instr fetches)
+  // load store execute protected page in elrange (pass, pass, pass)
+  // load store execute unprotected page in elrange (fail, fail, fail)
+  // load store execute protected page outside elrange (fail, fail, fail)
+  // load store execute unprotected page outside elrange (pass, pass, fail)
+  // switch to enclavemode (allow unprotected instr fetches)
+  // load store execute protected page in elrange (pass, pass, pass)
+  // load store execute unprotected page in elrange (fail, fail, fail)
+  // load store execute protected page outside elrange (fail, fail, fail)
+  // load store execute unprotected page outside elrange (pass, pass, pass)
+  
+  li TESTNUM, 10
+  // 11 - 13
+  // load, store execute unprotected page outside elrange
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_500, dummy_502, dummy_509, 0x55555555, 0xAFFED00F, die_u)
+  nop
+  // 14 - 16
+  // load, store execute unprotected page in elrange
+  load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_D00, dummy_D02, dummy_D08, 0xDDDDDDDD, 0xAFFED00F, die_u)
+  nop
+  // 17 - 19
+  // load, store execute protected page outside elrange if all registers are loaded
+  load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_600, dummy_602, dummy_608, 0x66666666, 0xAFFED00F, die_u)
+  nop
+  // 20 - 22
+  // load, store execute protected page in elrange
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_E00, dummy_E02, dummy_E09, 0xEEEEEEEE, 0xAFFED00F, die_u)
+  
+  nop
+  nop
+  nop
+  li a7, ACTIVATE_ENCLAVE_ECALL_CODE
+  ecall
+  
+die_u:
+  li a7, 0x1
+  ecall
+
+RVTEST_CODE_END
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+.align 12
+dummy_100: .word 0xDEADBEEF
+
+
+# PAGE_TWO
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0x22222222
+dummy_201: .word 0x20202020
+dummy_202: .word 0x02020202
+dummy_203: .word 0x22220000
+dummy_204: .word 0x00002222
+dummy_205: .word 0x22002200
+dummy_206: .word 0xDEADBEEF
+dummy_207: .word 0xDEADBEEF
+dummy_208: j die_u
+dummy_209: jump_and_check_return_dst(persistant_return_reg)
+
+
+# PAGE_THREE
+# dummy data on separate page to test everything
+.align 12
+dummy_300: .word 0x33333333
+dummy_301: .word 0x30303030
+dummy_302: .word 0x03030303
+dummy_303: .word 0x33330000
+dummy_304: .word 0x00003333
+dummy_305: .word 0x33003300
+dummy_306: .word 0xDEADBEEF
+dummy_307: .word 0xDEADBEEF
+dummy_308: j die_u
+dummy_309: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_FOUR
+# dummy data on separate page to test everything
+.align 12
+dummy_400: .word 0x44444444
+dummy_401: .word 0x40404040
+dummy_402: .word 0x04040404
+dummy_403: .word 0x44440000
+dummy_404: .word 0x00004444
+dummy_405: .word 0x44004400
+dummy_406: .word 0xDEADBEEF
+dummy_407: .word 0xDEADBEEF
+dummy_408: j die_u
+dummy_409: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_FIVE
+# dummy data on separate page to test everything
+.align 12
+dummy_500: .word 0x55555555
+dummy_501: .word 0x50505050
+dummy_502: .word 0x05050505
+dummy_503: .word 0x55550000
+dummy_504: .word 0x00005555
+dummy_505: .word 0x55005500
+dummy_506: .word 0xDEADBEEF
+dummy_507: .word 0xDEADBEEF
+dummy_508: j die_u
+dummy_509: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_SIX
+# dummy data on separate page to test everything
+.align 12
+dummy_600: .word 0x66666666
+dummy_601: .word 0x60606060
+dummy_602: .word 0x06060606
+dummy_603: .word 0x66660000
+dummy_604: .word 0x00006666
+dummy_605: .word 0x66006600
+dummy_606: .word 0xDEADBEEF
+dummy_607: .word 0xDEADBEEF
+dummy_608: j die_u
+dummy_609: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_SEVEN
+.align 12
+dummy_700: .word 0xDEADBEEF
+
+# PAGE_EIGHT
+.align 12
+dummy_800: .word 0xDEADBEEF
+
+# PAGE_NINE
+.align 12
+dummy_900: .word 0xDEADBEEF
+
+# PAGE_TEN
+.align 12
+dummy_A00: .word 0xDEADBEEF
+
+# PAGE_ELEVEN
+.align 12
+dummy_B00: .word 0xDEADBEEF
+
+# PAGE_TWELVE
+.align 12
+dummy_1C00: .word 0xDEADBEEF
+
+# PAGE_THIRTEEN
+.align 12
+dummy_1D00: .word 0xDEADBEEF
+
+# PAGE_FOURTEEN
+.align 12
+dummy_1E00: .word 0xDEADBEEF
+
+# PAGE_FIFTEEN
+.align 12
+dummy_F00: .word 0xDEADBEEF
+
+# PAGE_SIXTEEN
+.align 12
+  ////////////////////////////////////////////////////////
+  // switch to enclavemode (no debugmode)
+enclave_superstart:
+  li a7, 0
+  clear_bit_in_csr(Z_DBG_ALLOW_UNSAFE_CODE_FETCHES, CSR_E_DBG_CONTROL, a1)
+  // 23 - 25
+  // load store execute protected page in elrange
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_C01, dummy_C03, dummy_C09, 0xC0C0C0C0, 0xAFFED00F, die_u)
+  nop
+  // 26 - 28
+  // load store execute unprotected page in elrange
+  load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_D01, dummy_D03, dummy_D08, 0xD0D0D0D0, 0xAFFED00F, die_u)
+  nop
+  // 29 - 31
+  // load store execute protected page outside elrange
+  load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_601, dummy_603, dummy_608, 0x60606060, 0xAFFED00F, die_u)
+  nop
+  // 32 - 34
+  // load store execute unprotected page outside elrange
+  load_store_execute_cycle_rwo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_501, dummy_503, dummy_508, 0x50505050, 0xAFFED00F, die_u)
+  
+  nop
+  nop
+  nop
+code_page_sixteen_part_2:
+  ////////////////////////////////////////////////////////
+  // switch to enclavemode (allow unprotected instr fetches)
+  set_bit_in_csr(Z_DBG_ALLOW_UNSAFE_CODE_FETCHES, CSR_E_DBG_CONTROL, a1)
+  // 35 - 37
+  // load store execute protected page in elrange
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_C04, dummy_C05, dummy_C09, 0x0000CCCC, 0xAFFED00F, die_u)
+  nop
+  // 38 - 40
+  // load store execute unprotected page in elrange
+  load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_D04, dummy_D05, dummy_D08, 0x0000DDDD, 0xAFFED00F, die_u)
+  nop
+  nop
+  // 41 - 43
+  // load store execute protected page outside elrange
+  load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_604, dummy_605, dummy_608, 0x00006666, 0xAFFED00F, die_u)
+  nop
+  // 44 - 46
+  // load store execute unprotected page outside elrange
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_504, dummy_505, dummy_509, 0x00005555, 0xAFFED00F, die_u)
+  
+
+  // 47
+  // accessing last memory location in the elrange, to check the upper bound
+  addi TESTNUM, TESTNUM, 1
+  read_value_test(a1, t2, last_word_in_range, 0xAFFEAFFE, die_u);
+  read_value_test(a1, t2, last_word_in_range, 0xAFFEAFFE, die_u);
+
+  // 48
+  // accessing first memory location after the elrange, to check the upper bound
+  // the word is unprotected, if the word would be interpreted as in-the-elrange this would lead to an exception
+  addi TESTNUM, TESTNUM, 1
+  read_value_test(a1, t2, first_word_outside_range, 0xCAFECAFE, die_u);
+  read_value_test(a1, t2, first_word_outside_range, 0xCAFECAFE, die_u);
+  nop
+  nop
+  nop
+
+  li TESTNUM, 100
+  ecall
+  
+  RVTEST_CODE_END
+
+die_uE:
+  li a7, 0x1
+  ecall
+
+# PAGE_SEVENTEEN
+.align 12
+dummy_C00: .word 0xCCCCCCCC
+dummy_C01: .word 0xC0C0C0C0
+dummy_C02: .word 0x0C0C0C0C
+dummy_C03: .word 0xCCCC0000
+dummy_C04: .word 0x0000CCCC
+dummy_C05: .word 0xCC00CC00
+dummy_C06: .word 0xDEADBEEF
+dummy_C07: .word 0xDEADBEEF
+dummy_C08: j die_u
+dummy_C09: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_EIGHTEEN (unprotected
+.align 12
+dummy_D00: .word 0xDDDDDDDD
+dummy_D01: .word 0xD0D0D0D0
+dummy_D02: .word 0x0D0D0D0D
+dummy_D03: .word 0xDDDD0000
+dummy_D04: .word 0x0000DDDD
+dummy_D05: .word 0xDD00DD00
+dummy_D06: .word 0xDEADBEEF
+dummy_D07: .word 0xDEADBEEF
+dummy_D08: j die_u
+dummy_D09: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_NINETEEN
+.align 12
+dummy_E00: .word 0xEEEEEEEE
+dummy_E01: .word 0xE0E0E0E0
+dummy_E02: .word 0x0E0E0E0E
+dummy_E03: .word 0xEEEE0000
+dummy_E04: .word 0x0000EEEE
+dummy_E05: .word 0xEE00EE00
+dummy_E06: .word 0xDEADBEEF
+dummy_E07: .word 
+dummy_E08: j die_u
+dummy_E09: jump_and_check_return_dst(persistant_return_reg)
+.align 11
+.word 0x00000000
+.align 10
+.word 0x00000000
+.align 9
+.word 0x00000000
+.align 8
+.word 0x00000000
+.align 7
+.word 0x00000000
+.align 6
+.word 0x00000000
+.align 5
+.word 0x00000000
+.align 4
+.word 0x00000000
+.align 3
+.word 0xDEADDEAD
+last_word_in_range: .word 0xAFFEAFFE
+
+# PAGE_TWENTY
+.align 12
+first_word_outside_range: .word 0xCAFECAFE
+
+# PAGE_TWENTYONE
+# dummy data on separate page to test everything
+.align 12
+RVTEST_DATA_END
diff --git a/isa/rv64uz/elrange_access_precise_bounds.S b/isa/rv64uz/elrange_access_precise_bounds.S
new file mode 100644
index 0000000..11912c5
--- /dev/null
+++ b/isa/rv64uz/elrange_access_precise_bounds.S
@@ -0,0 +1,412 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# double_mapping.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+#define EXPECTED_EXCEPTIONS 50
+#define USER_TEST_START 11
+
+#define ACTIVATE_ENCLAVE_ECALL_CODE 2
+
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+setup_m_to_u_vm(a1)
+
+// Set SID for write
+write_value_to_csr(         0x8000000000000001, CSR_E_MSID_0 , a1)
+write_value_to_csr((PAGE_ONE  - PAGE_ZERO)     , CSR_E_MRANGE_VBASE, a1)
+write_value_to_csr((PAGE_FIVE - PAGE_ONE)      , CSR_E_MRANGE_VSIZE, a1)
+
+// set the unaligned elrangetest
+set_bit_in_csr(Z_DBG_VSIZE_XRANGE_IS_SIZE_NOT_MASK_NO_ALIGNMENT, CSR_E_DBG_CONTROL, a1)
+clear_bit_in_csr(E_STATUS_ENCLAVE_MODE_EN, CSR_E_STATUS, a1)
+
+
+li TESTNUM, 1
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+li TESTNUM, 2
+set_and_enable_store_tweak(a1, E_TWEAK_ENCLAVE)
+li TESTNUM, 3
+// Encrypt Page Two
+copy_vma_to_enclave_vma(PAGE_ONE , PAGE_TWO  , PAGE_ONE , PAGE_ZERO, a1, a2, a3, a4)
+copy_vma_to_enclave_vma(PAGE_TWO , PAGE_THREE, PAGE_TWO , PAGE_ZERO, a1, a2, a3, a4)
+copy_vma_to_enclave_vma(PAGE_FOUR, PAGE_FIVE , PAGE_FOUR, PAGE_ZERO, a1, a2, a3, a4)
+li TESTNUM, 4
+copy_vma_to_enclave_vma(PAGE_SIX , PAGE_SEVEN, PAGE_SIX , PAGE_ZERO, a1, a2, a3, a4)
+
+disable_lstweaks(a1)
+quick_disable_m_vm_access(a1)
+
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+# prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+li TESTNUM, 5
+mret
+
+adapted_ecall:
+  la a0, exception_save_slot
+  lwu t0, 0(a0)
+  li a0, ACTIVATE_ENCLAVE_ECALL_CODE
+  beq a7, a0, enable_enclave_mode
+  bnez a7, die
+  li a0, EXPECTED_EXCEPTIONS
+  beq t0, a0, end
+  j die
+
+enable_enclave_mode:
+  set_bit_in_csr(E_STATUS_ENCLAVE_MODE_EN, CSR_E_STATUS, a0)
+  set_user_pc_to_userlabel(a0, t0, enclave_superstart, PAGE_ZERO)
+  j return_to_userland
+
+return_to_user_jump_table_init:
+  addi a0, TESTNUM, (-(USER_TEST_START)); // align to counter start in usermode
+  slli a0, a0, 2;
+  la t0, return_to_user_jump_table;
+  add t0, a0, t0;
+  jalr x0, 0(t0);
+
+return_to_user_jump_table:
+  j die;                   // load, store execute unprotected page outside elrange (pass, pass, pass)
+  j die;                   // load, store execute unprotected page outside elrange (pass, pass, pass)
+  j die;                   // load, store execute unprotected page outside elrange (pass, pass, pass)
+  j skip_instruction;      // load, store execute unprotected page in elrange (fail, fail, fail)
+  j skip_instruction;      // load, store execute unprotected page in elrange (fail, fail, fail)
+  j return_to_caller;      // load, store execute unprotected page in elrange (fail, fail, fail)
+  j skip_instruction;                   // load, store execute protected page outside elrange (fail, fail, fail)
+  j skip_instruction;                   // load, store execute protected page outside elrange (fail, fail, fail)
+  j return_to_caller;                   // load, store execute protected page outside elrange (fail, fail, fail)
+  j die;                   // load, store execute protected page in elrange (pass, pass, pass)
+  j die;                   // load, store execute protected page in elrange (pass, pass, pass)
+  j die;                   // load, store execute protected page in elrange (pass, pass, pass)
+  j die;                   // load store execute protected page in elrange (pass, pass, pass)
+  j die;                   // load store execute protected page in elrange (pass, pass, pass)
+  j die;                   // load store execute protected page in elrange (pass, pass, pass)
+  j skip_instruction;      // load store execute unprotected page in elrange (fail, fail, fail)
+  j skip_instruction;      // load store execute unprotected page in elrange (fail, fail, fail)
+  j return_to_caller;      // load store execute unprotected page in elrange (fail, fail, fail)
+  j skip_instruction;                   // load store execute protected page outside elrange (fail, fail, fail)
+  j skip_instruction;                   // load store execute protected page outside elrange (fail, fail, fail)
+  j return_to_caller;      // load store execute protected page outside elrange (fail, fail, fail)
+  j die;                   // load store execute unprotected page outside elrange (pass, pass, fail)
+  j die;                   // load store execute unprotected page outside elrange (pass, pass, fail)
+  j return_to_caller;      // load store execute unprotected page outside elrange (pass, pass, fail)
+  j die;                   // load store execute protected page in elrange (pass, pass, pass)
+  j die;                   // load store execute protected page in elrange (pass, pass, pass)
+  j die;                   // load store execute protected page in elrange (pass, pass, pass)
+  j skip_instruction;      // load store execute unprotected page in elrange (fail, fail, fail)
+  j skip_instruction;      // load store execute unprotected page in elrange (fail, fail, fail)
+  j return_to_caller;      // load store execute unprotected page in elrange (fail, fail, fail)
+  j skip_instruction;                   // load store execute protected page outside elrange (fail, fail, fail)
+  j skip_instruction;                   // load store execute protected page outside elrange (fail, fail, fail)
+  j return_to_caller;                   // load store execute protected page outside elrange (fail, fail, fail)
+  j die;                   // load store execute unprotected page outside elrange (pass, pass, pass)
+  j die;                   // load store execute unprotected page outside elrange (pass, pass, pass)
+  j die;                   // load store execute unprotected page outside elrange (pass, pass, pass)
+  j die;                   // upper bound check
+  j die;                   // upper bound check
+  nop
+  nop
+  nop
+  j die
+
+skip_instruction:
+  csrr t0, mepc;
+  ADDI t0, t0, 4;
+  csrw mepc, t0;
+  j return_to_userland
+
+return_to_caller:
+  set_user_pc_to_return_reg(PERSISTANT_REG_FOR_RET)
+  j return_to_userland
+
+return_to_userland:
+  mret
+
+.align 8
+// Last parameter must hold the number of expected exceptions!
+usermode_exit_handler_success_on_fetch_decryption_error(a0, t0, t3, a7, EXPECTED_EXCEPTIONS, adapted_ecall, decryption_exception)
+
+RVTEST_CODE_END
+
+.data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ZERO / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+// start of elrange
+page_table_leaf_entry_2: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_ONE  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_3: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_TWO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_4: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_THREE/ RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_5: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_FOUR / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+// end of elrange
+page_table_leaf_entry_6: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_FIVE / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_7: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_SIX  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0x00123456
+dummy_001: .word 0xCAFEBABE
+dummy_002: .word 0xFFFFFFFF
+dummy_003: .word 0xABCDEF12
+dummy_004: jump_and_check_return_dst(persistant_return_reg)
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x0
+
+  // test strategy:
+  // exceptions for fails: 2, 4, 2
+  // non enclave mode
+  // load, store, execute unprotected page outside elrange (pass, pass, pass)
+  // load, store, execute unprotected page in elrange (fail, fail, fail)
+  // load, store, execute protected page outside elrange (fail, fail, fail)
+  // load, store, execute protected page in elrange (pass, pass, pass)
+  // switch to enclavemode (DON'T allow unprotected instr fetches)
+  // load, store, execute protected page in elrange (pass, pass, pass)
+  // load, store, execute unprotected page in elrange (fail, fail, fail)
+  // load, store, execute protected page outside elrange (fail, fail, fail)
+  // load, store, execute unprotected page outside elrange (pass, pass, fail)
+  // switch to enclavemode (allow unprotected instr fetches)
+  // load, store, execute protected page in elrange (pass, pass, pass)
+  // load, store, execute unprotected page in elrange (fail, fail, fail)
+  // load, store, execute protected page outside elrange (fail, fail, fail)
+  // load, store, execute unprotected page outside elrange (pass, pass, pass)
+  
+  li TESTNUM, 10
+  // 11 - 13
+  // load, store execute unprotected page outside elrange
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_500, dummy_502, dummy_509, 0x55555555, 0xAFFED00F, die_u)
+  nop
+  // 14 - 16
+  // load, store execute unprotected page in elrange
+  load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_300, dummy_302, dummy_308, 0x33333333, 0xAFFED00F, die_u)
+  nop
+  // 17 - 19
+  // load, store execute protected page outside elrange if all registers are loaded
+  load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_600, dummy_602, dummy_608, 0x66666666, 0xAFFED00F, die_u)
+  nop
+  // 20 - 22
+  // load, store execute protected page in elrange
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_200, dummy_202, dummy_209, 0x22222222, 0xAFFED00F, die_u)
+  
+  nop
+  nop
+  nop
+  li a7, ACTIVATE_ENCLAVE_ECALL_CODE
+  ecall
+  
+die_u:
+  li a7, 0x1
+  ecall
+
+RVTEST_CODE_END
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+.align 12
+  ////////////////////////////////////////////////////////
+  // switch to enclavemode (no debugmode)
+enclave_superstart:
+  li a7, 0
+  clear_bit_in_csr(Z_DBG_ALLOW_UNSAFE_CODE_FETCHES, CSR_E_DBG_CONTROL, a1)
+  // 23 - 25
+  // load store execute protected page in elrange
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_201, dummy_203, dummy_209, 0x20202020, 0xAFFED00F, die_u)
+  nop
+  // 26 - 28
+  // load store execute unprotected page in elrange
+  load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_301, dummy_303, dummy_308, 0x30303030, 0xAFFED00F, die_u)
+  nop
+  // 29 - 31
+  // load store execute protected page outside elrange
+  load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_601, dummy_603, dummy_608, 0x60606060, 0xAFFED00F, die_u)
+  nop
+  // 32 - 34
+  // load store execute unprotected page outside elrange
+  load_store_execute_cycle_rwo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_500, dummy_503, dummy_508, 0x55555555, 0xAFFED00F, die_u)
+  
+  nop
+  nop
+  nop
+
+  ////////////////////////////////////////////////////////
+  // switch to enclavemode (allow unprotected instr fetches)
+  set_bit_in_csr(Z_DBG_ALLOW_UNSAFE_CODE_FETCHES, CSR_E_DBG_CONTROL, a1)
+  // 35 - 37
+  // load store execute protected page in elrange
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_204, dummy_205, dummy_209, 0x00002222, 0xAFFED00F, die_u)
+  nop
+  // 38 - 40
+  // load store execute unprotected page in elrange
+  load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_304, dummy_305, dummy_308, 0x00003333, 0xAFFED00F, die_u)
+  nop
+  nop
+  // 41 - 43
+  // load store execute protected page outside elrange
+  load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_604, dummy_605, dummy_608, 0x00006666, 0xAFFED00F, die_u)
+  nop
+  // 44 - 46
+  // load store execute unprotected page outside elrange
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_500, dummy_505, dummy_509, 0x55555555, 0xAFFED00F, die_u)
+
+  // 47
+  // accessing last memory location in the elrange, to check the upper bound
+  addi TESTNUM, TESTNUM, 1
+  read_value_test(a1, t2, last_word_in_range, 0xAFFEAFFE, die_u);
+  read_value_test(a1, t2, last_word_in_range, 0xAFFEAFFE, die_u);
+
+  nop
+  nop
+  nop
+
+  li TESTNUM, 100
+  ecall
+  
+  RVTEST_CODE_END
+
+die_uE:
+  li a7, 0x1
+  ecall
+
+
+# PAGE_TWO
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0x22222222
+dummy_201: .word 0x20202020
+dummy_202: .word 0x02020202
+dummy_203: .word 0x22220000
+dummy_204: .word 0x00002222
+dummy_205: .word 0x22002200
+dummy_206: .word 0xDEADBEEF
+dummy_207: .word 0xDEADBEEF
+dummy_208: j die_u
+dummy_209: jump_and_check_return_dst(persistant_return_reg)
+
+
+# PAGE_THREE
+# dummy data on separate page to test everything
+.align 12
+dummy_300: .word 0x33333333
+dummy_301: .word 0x30303030
+dummy_302: .word 0x03030303
+dummy_303: .word 0x33330000
+dummy_304: .word 0x00003333
+dummy_305: .word 0x33003300
+dummy_306: .word 0xDEADBEEF
+dummy_307: .word 0xDEADBEEF
+dummy_308: j die_u
+dummy_309: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_FOUR
+# dummy data on separate page to test everything
+.align 12
+dummy_400: .word 0x44444444
+dummy_401: .word 0x40404040
+dummy_402: .word 0x04040404
+dummy_403: .word 0x44440000
+dummy_404: .word 0x00004444
+dummy_405: .word 0x44004400
+dummy_406: .word 0xDEADBEEF
+dummy_407: .word 0xDEADBEEF
+dummy_408: j die_u
+dummy_409: jump_and_check_return_dst(persistant_return_reg)
+.align 11
+.word 0x00000000
+.align 10
+.word 0x00000000
+.align 9
+.word 0x00000000
+.align 8
+.word 0x00000000
+.align 7
+.word 0x00000000
+.align 6
+.word 0x00000000
+.align 5
+.word 0x00000000
+.align 4
+.word 0x00000000
+.align 3
+.word 0xDEADDEAD
+last_word_in_range: .word 0xAFFEAFFE
+
+# PAGE_FIVE
+# dummy data on separate page to test everything
+.align 12
+dummy_500: .word 0x55555555
+dummy_501: .word 0x50505050
+dummy_502: .word 0x05050505
+dummy_503: .word 0x55550000
+dummy_504: .word 0x00005555
+dummy_505: .word 0x55005500
+dummy_506: .word 0xDEADBEEF
+dummy_507: .word 0xDEADBEEF
+dummy_508: j die_u
+dummy_509: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_SIX
+# dummy data on separate page to test everything
+.align 12
+dummy_600: .word 0x66666666
+dummy_601: .word 0x60606060
+dummy_602: .word 0x06060606
+dummy_603: .word 0x66660000
+dummy_604: .word 0x00006666
+dummy_605: .word 0x66006600
+dummy_606: .word 0xDEADBEEF
+dummy_607: .word 0xDEADBEEF
+dummy_608: j die_u
+dummy_609: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_SEVEN
+.align 12
+first_word_outside_range: .word 0xCAFECAFE
+
+# PAGE_EIGHT
+# dummy data on separate page to test everything
+.align 12
+RVTEST_DATA_END
diff --git a/isa/rv64uz/enclave_execute_encrypted_memory_simple.S b/isa/rv64uz/enclave_execute_encrypted_memory_simple.S
new file mode 100644
index 0000000..7cb3e6a
--- /dev/null
+++ b/isa/rv64uz/enclave_execute_encrypted_memory_simple.S
@@ -0,0 +1,210 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# enclave_execute_encrypted_memory_simple.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+setup_m_to_u_vm(a1)
+
+// Set SID for write
+write_value_to_csr(         0x8000000000000001, CSR_E_MSID_0 , a1)
+write_value_to_csr((PAGE_ZERO  - PAGE_ZERO  )      , CSR_E_MRANGE_VBASE, a1)
+write_value_to_csr((PAGE_FOUR - PAGE_ZERO - 1)   , CSR_E_MRANGE_VSIZE, a1)
+
+// set unaligned range test
+set_bit_in_csr(Z_DBG_VSIZE_XRANGE_IS_SIZE_NOT_MASK_NO_ALIGNMENT, CSR_E_DBG_CONTROL, a1)
+
+li TESTNUM, 1
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+li TESTNUM, 2
+set_and_enable_store_tweak(a1, E_TWEAK_ENCLAVE)
+li TESTNUM, 3
+copy_vma_to_enclave_vma(PAGE_ZERO, PAGE_FOUR, PAGE_ZERO, PAGE_ZERO, a1, a2, a3, a4)
+
+disable_lstweaks(a1)
+
+li TESTNUM, 4
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+# prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+li TESTNUM, 5
+li a7, 0x3
+mret
+
+.align 8
+usermode_exit_handler(a0, t0, t3, a7)
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_ZERO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_2: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_ONE   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_3: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_TWO   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_4: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_THREE / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_5: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_FOUR  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0x00123456
+dummy_001: .word 0xCAFEBABE
+dummy_002: .word 0xFFFFFFFF
+dummy_003: .word 0xABCDEF12
+dummy_004: .word 0xDEADBEEF
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x2
+1:
+  # // if key has to be incremented too often there's an error
+  li TESTNUM, 10
+  # load dummy address on same page (fetches slow path)
+  read_value_test(t0, t1, dummy_000, 0x00123456, die_u)
+
+  li TESTNUM, 11
+  # load dummy address on same page (fetches should access cache)
+  read_value_test(t0, t1, dummy_000, 0x00123456, die_u)
+ 
+  li TESTNUM, 12
+  # write dummy unprotected address on same page (should work)
+  write_and_readback_value_test(t0, t1, dummy_002, 0xCAFEBABE, die_u)
+
+  li TESTNUM, 13
+  # write dummy unprotected address on same page (should access cache)
+  write_and_readback_value_test(t0, t1, dummy_002, 0xCAFEBABE, die_u)
+
+  nop
+  nop
+  nop
+  nop
+
+  li TESTNUM, 20
+  # load dummy protected address on other page (should work)
+  read_value_test(t0, t1, dummy_101, 0xCEAEBEBA, die_u)
+
+  li TESTNUM, 21
+  # load dummy protected address on other page (fetches should access cache)
+  read_value_test(t0, t1, dummy_101, 0xCEAEBEBA, die_u)
+
+  li TESTNUM, 22
+  # write dummy protected address on other page (should work)
+  write_and_readback_value_test(t0, t1, dummy_101, 0xF00BAA, die_u)
+
+  li TESTNUM, 23
+  # write dummy protected address on other page (should access cache)
+  write_and_readback_value_test(t0, t1, dummy_101, 0xF00BAA, die_u)
+  
+  nop
+  nop
+  nop
+  nop
+
+  li TESTNUM,30
+  # load dummy protected address on other page (should work)
+  read_value_test(t0, t1, dummy_203, 0xBADEAFFE, die_u)
+
+  li TESTNUM,31
+  # load dummy protected address on other page (fetches should access cache)
+  read_value_test(t0, t1, dummy_203, 0xBADEAFFE, die_u)
+
+  li TESTNUM,32
+  # write dummy protected address on other page (should work)
+  write_and_readback_value_test(t0, t1, dummy_300, 0xCAFECAFE, die_u)
+
+  li TESTNUM,33
+  # write dummy protected address on other page (should access cache))
+  write_and_readback_value_test(t0, t1, dummy_300, 0xCAFECAFE, die_u)
+
+  nop
+  nop
+  nop
+  nop
+  nop
+  nop
+  li a7, 0x0
+  ecall
+  
+  RVTEST_CODE_END
+
+die_u:
+  li a7, 0x1
+  ecall
+  
+
+RVTEST_CODE_END
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+.align 12
+dummy_100: .word 0x00000123
+dummy_101: .word 0xCEAEBEBA
+dummy_102: .word 0x88888888
+dummy_103: .word 0x12345ABC
+dummy_104: .word 0x11111111
+
+# PAGE_TWO
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0xBEEFDEAD
+dummy_201: .word 0xBEEFDEAD
+dummy_202: .word 0xBEEFDEAD
+dummy_203: .word 0xBADEAFFE
+dummy_204: .word 0xBEEFDEAD
+
+# PAGE_THREE
+# dummy data on separate page to test everything
+.align 12
+dummy_300: .word 0xBEEFBEEF
+dummy_301: .word 0xBEEFDEAD
+dummy_302: .word 0xBEEFDEAD
+dummy_303: .word 0xBEEFDEAD
+dummy_304: .word 0xBEEFDEAD
+
+
+# PAGE_FOUR
+.align 12
+RVTEST_DATA_END
diff --git a/isa/rv64uz/enclave_load_encrypted_memory.S b/isa/rv64uz/enclave_load_encrypted_memory.S
new file mode 100644
index 0000000..61d66bf
--- /dev/null
+++ b/isa/rv64uz/enclave_load_encrypted_memory.S
@@ -0,0 +1,211 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# enclave_load_encrypted_memory.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+setup_m_to_u_vm(a1)
+
+// set unaligned range test
+set_bit_in_csr(Z_DBG_VSIZE_XRANGE_IS_SIZE_NOT_MASK_NO_ALIGNMENT, CSR_E_DBG_CONTROL, a1)
+// Set SID for write
+write_value_to_csr(         0x8000000000000001, CSR_E_MSID_0 , a1)
+write_value_to_csr((PAGE_TWO  - PAGE_ZERO  )   , CSR_E_MRANGE_VBASE, a1)
+write_value_to_csr((PAGE_FOUR - PAGE_ZERO - 1) , CSR_E_MRANGE_VSIZE, a1)
+
+li TESTNUM, 1
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+li TESTNUM, 2
+set_and_enable_store_tweak(a1, E_TWEAK_ENCLAVE)
+li TESTNUM, 3
+copy_vma_to_enclave_vma(PAGE_TWO, PAGE_THREE, PAGE_TWO, PAGE_ZERO, a1, a2, a3, a4)
+li TESTNUM, 4
+copy_vma_to_enclave_vma(PAGE_THREE, PAGE_FOUR, PAGE_THREE, PAGE_ZERO, a1, a2, a3, a4)
+
+disable_lstweaks(a1)
+
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+# prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+li TESTNUM, 5
+mret
+
+.align 8
+usermode_exit_handler(a0, t0, t3, a7)
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ZERO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_2: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ONE   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_3: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_TWO   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_4: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_THREE / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_5: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_FOUR  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0x00123456
+dummy_001: .word 0xCAFEBABE
+dummy_002: .word 0xFFFFFFFF
+dummy_003: .word 0xABCDEF12
+dummy_004: .word 0xDEADBEEF
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x2
+1:
+  # // if key has to be incremented too often there's an error
+  # addi TESTNUM, TESTNUM, 1
+  li TESTNUM, 10
+  # load dummy address on same page (fetches slow path)
+  read_value_test(t0, t1, dummy_000, 0x00123456, die_u)
+
+  li TESTNUM, 11
+  # load dummy address on same page (fetches should access cache)
+  read_value_test(t0, t1, dummy_000, 0x00123456, die_u)
+ 
+  li TESTNUM, 12
+  # write dummy unprotected address on same page (should work)
+  write_and_readback_value_test(t0, t1, dummy_002, 0xCAFEBABE, die_u)
+
+  li TESTNUM, 13
+  # write dummy unprotected address on same page (should access cache)
+  write_and_readback_value_test(t0, t1, dummy_002, 0xCAFEBABE, die_u)
+
+  nop
+  nop
+  nop
+  nop
+
+  li TESTNUM, 20
+  # load dummy unprotected address on other page (should work)
+  read_value_test(t0, t1, dummy_101, 0xCEAEBEBA, die_u)
+
+  li TESTNUM, 21
+  # load dummy unprotected address on other page (fetches should access cache)
+  read_value_test(t0, t1, dummy_101, 0xCEAEBEBA, die_u)
+
+  li TESTNUM, 22
+  # write dummy unprotected address on other page (should work)
+  write_and_readback_value_test(t0, t1, dummy_101, 0xF00BAA, die_u)
+  
+  li TESTNUM, 23
+  # write dummy unprotected address on other page (should access cache)
+  write_and_readback_value_test(t0, t1, dummy_101, 0xF00BAA, die_u)
+  
+  nop
+  nop
+  nop
+  nop
+
+  li TESTNUM,30
+  # load dummy protected address on other page (should work)
+  read_value_test(t0, t1, dummy_203, 0xBADEAFFE, die_u)
+
+  li TESTNUM,31
+  # load dummy protected address on other page (fetches should access cache)
+  read_value_test(t0, t1, dummy_203, 0xBADEAFFE, die_u)
+
+  li TESTNUM,32
+  # write dummy protected address on other page (should work)
+  write_and_readback_value_test(t0, t1, dummy_300, 0xCAFECAFE, die_u)
+
+  li TESTNUM,33
+  # write dummy protected address on other page (should access cache))
+  write_and_readback_value_test(t0, t1, dummy_300, 0xCAFECAFE, die_u)
+
+  nop
+  nop
+  nop
+  nop
+  nop
+  nop
+  li a7, 0x0
+  ecall
+  
+  RVTEST_CODE_END
+
+die_u:
+  li a7, 0x1
+  ecall
+  
+
+RVTEST_CODE_END
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+.align 12
+dummy_100: .word 0x00000123
+dummy_101: .word 0xCEAEBEBA
+dummy_102: .word 0x88888888
+dummy_103: .word 0x12345ABC
+dummy_104: .word 0x11111111
+
+# PAGE_TWO
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0xBEEFDEAD
+dummy_201: .word 0xBEEFDEAD
+dummy_202: .word 0xBEEFDEAD
+dummy_203: .word 0xBADEAFFE
+dummy_204: .word 0xBEEFDEAD
+
+# PAGE_THREE
+# dummy data on separate page to test everything
+.align 12
+dummy_300: .word 0xBEEFBEEF
+dummy_301: .word 0xBEEFDEAD
+dummy_302: .word 0xBEEFDEAD
+dummy_303: .word 0xBEEFDEAD
+dummy_304: .word 0xBEEFDEAD
+
+
+# PAGE_FOUR
+.align 12
+
+RVTEST_DATA_END
diff --git a/isa/rv64uz/enclave_load_store_byte_sizes.S b/isa/rv64uz/enclave_load_store_byte_sizes.S
new file mode 100644
index 0000000..dc665fd
--- /dev/null
+++ b/isa/rv64uz/enclave_load_store_byte_sizes.S
@@ -0,0 +1,447 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# enclave_load_store_byte_sizes.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+#define read_value_type(read_instr, treg_0, treg_1, label, expected_value) \
+  la treg_1, label ; \
+  read_instr treg_0, 0(treg_1) ; \
+  li treg_1, expected_value ; \
+
+#define read_double_as_all_types(treg_0, treg_1, label, expected_value_double, error_label) \
+  read_value_type(ld, treg_0, treg_1, label, expected_value_double) \
+  bne treg_0, treg_1, error_label ; \
+  \
+  read_value_type(lwu, treg_0, treg_1, label + 0, (expected_value_double & 0x00000000FFFFFFFF) >>  0 ) \
+  bne treg_0, treg_1, error_label ; \
+  read_value_type(lwu, treg_0, treg_1, label + 4, (expected_value_double & 0xFFFFFFFF00000000) >> 32 ) \
+  bne treg_0, treg_1, error_label ; \
+  \
+  read_value_type(lhu, treg_0, treg_1, label + 0, (expected_value_double & 0x000000000000FFFF) >>  0 ) \
+  bne treg_0, treg_1, error_label ; \
+  read_value_type(lhu, treg_0, treg_1, label + 2, (expected_value_double & 0x00000000FFFF0000) >> 16 ) \
+  bne treg_0, treg_1, error_label ; \
+  read_value_type(lhu, treg_0, treg_1, label + 4, (expected_value_double & 0x0000FFFF00000000) >> 32 ) \
+  bne treg_0, treg_1, error_label ; \
+  read_value_type(lhu, treg_0, treg_1, label + 6, (expected_value_double & 0xFFFF000000000000) >> 48 ) \
+  bne treg_0, treg_1, error_label ; \
+  \
+  read_value_type(lbu, treg_0, treg_1, label + 0, (expected_value_double & 0x00000000000000FF) >>  0 ) \
+  bne treg_0, treg_1, error_label ; \
+  read_value_type(lbu, treg_0, treg_1, label + 1, (expected_value_double & 0x000000000000FF00) >>  8 ) \
+  bne treg_0, treg_1, error_label ; \
+  read_value_type(lbu, treg_0, treg_1, label + 2, (expected_value_double & 0x0000000000FF0000) >> 16 ) \
+  bne treg_0, treg_1, error_label ; \
+  read_value_type(lbu, treg_0, treg_1, label + 3, (expected_value_double & 0x00000000FF000000) >> 24 ) \
+  bne treg_0, treg_1, error_label ; \
+  read_value_type(lbu, treg_0, treg_1, label + 4, (expected_value_double & 0x000000FF00000000) >> 32 ) \
+  bne treg_0, treg_1, error_label ; \
+  read_value_type(lbu, treg_0, treg_1, label + 5, (expected_value_double & 0x0000FF0000000000) >> 40 ) \
+  bne treg_0, treg_1, error_label ; \
+  read_value_type(lbu, treg_0, treg_1, label + 6, (expected_value_double & 0x00FF000000000000) >> 48 ) \
+  bne treg_0, treg_1, error_label ; \
+  read_value_type(lbu, treg_0, treg_1, label + 7, (expected_value_double & 0xFF00000000000000) >> 56 ) \
+  bne treg_0, treg_1, error_label ;
+
+
+#define copy_vma_to_enclave_vma_as_bytes(start_phys, end_phys, dst_phys, vm_offset, treg_0, treg_1, treg_2, treg_3) \
+  vma_from_physical(start_phys, vm_offset, treg_0, treg_1) \
+  vma_from_physical(end_phys, vm_offset, treg_1, treg_2) \
+  vma_from_physical(dst_phys, vm_offset, treg_2, treg_3) \
+  1: \
+  lbu treg_3, 0(treg_0); \
+  sb treg_3, 0(treg_2); \
+  addi treg_0, treg_0, 1; \
+  addi treg_2, treg_2, 1; \
+  bltu treg_0, treg_1, 1b;
+
+#define copy_vma_to_enclave_vma_half_words(start_phys, end_phys, dst_phys, vm_offset, treg_0, treg_1, treg_2, treg_3) \
+  vma_from_physical(start_phys, vm_offset, treg_0, treg_1) \
+  vma_from_physical(end_phys, vm_offset, treg_1, treg_2) \
+  vma_from_physical(dst_phys, vm_offset, treg_2, treg_3) \
+  1: \
+  lhu treg_3, 0(treg_0); \
+  sh treg_3, 0(treg_2); \
+  addi treg_0, treg_0, 2; \
+  addi treg_2, treg_2, 2; \
+  bltu treg_0, treg_1, 1b;
+
+#define copy_vma_to_enclave_vma_as_words(start_phys, end_phys, dst_phys, vm_offset, treg_0, treg_1, treg_2, treg_3) \
+  vma_from_physical(start_phys, vm_offset, treg_0, treg_1) \
+  vma_from_physical(end_phys, vm_offset, treg_1, treg_2) \
+  vma_from_physical(dst_phys, vm_offset, treg_2, treg_3) \
+  1: \
+  lwu treg_3, 0(treg_0); \
+  sw treg_3, 0(treg_2); \
+  addi treg_0, treg_0, 4; \
+  addi treg_2, treg_2, 4; \
+  bltu treg_0, treg_1, 1b;
+
+#define copy_vma_to_enclave_vma_as_doubles(start_phys, end_phys, dst_phys, vm_offset, treg_0, treg_1, treg_2, treg_3) \
+  vma_from_physical(start_phys, vm_offset, treg_0, treg_1) \
+  vma_from_physical(end_phys, vm_offset, treg_1, treg_2) \
+  vma_from_physical(dst_phys, vm_offset, treg_2, treg_3) \
+  1: \
+  ld treg_3, 0(treg_0); \
+  sd treg_3, 0(treg_2); \
+  addi treg_0, treg_0, 8; \
+  addi treg_2, treg_2, 8; \
+  bltu treg_0, treg_1, 1b;
+
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+setup_m_to_u_vm(a1)
+
+// Set SID for write and configure mrange
+write_value_to_csr(         0x8000000000000001, CSR_E_MSID_0 , a1)
+write_value_to_csr((PAGE_SIX  - PAGE_ZERO  ) , CSR_E_MRANGE_VBASE, a1)
+write_value_to_csr((PAGE_TEN - PAGE_SIX - 1), CSR_E_MRANGE_VSIZE, a1)
+
+// set the unaligned elrangetest
+set_bit_in_csr(Z_DBG_VSIZE_XRANGE_IS_SIZE_NOT_MASK_NO_ALIGNMENT, CSR_E_DBG_CONTROL, a1)
+
+li TESTNUM, 1
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+li TESTNUM, 2
+set_and_enable_store_tweak(a1, E_TWEAK_ENCLAVE)
+li TESTNUM, 3
+copy_vma_to_enclave_vma_as_words  (PAGE_SIX  , PAGE_SEVEN, PAGE_SIX  , PAGE_ZERO, a1, a2, a3, a4)
+li TESTNUM, 4
+copy_vma_to_enclave_vma_half_words(PAGE_SEVEN, PAGE_EIGHT, PAGE_SEVEN, PAGE_ZERO, a1, a2, a3, a4)
+li TESTNUM, 5
+copy_vma_to_enclave_vma_as_bytes  (PAGE_EIGHT, PAGE_NINE , PAGE_EIGHT, PAGE_ZERO, a1, a2, a3, a4)
+li TESTNUM, 6
+copy_vma_to_enclave_vma_as_doubles(PAGE_NINE , PAGE_TEN  , PAGE_NINE , PAGE_ZERO, a1, a2, a3, a4)
+disable_lstweaks(a1)
+
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+# prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+li TESTNUM, 5
+mret
+
+.align 8
+usermode_exit_handler(a0, t0, t3, a7)
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ZERO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_2: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ONE   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_3: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_TWO   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_4: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_THREE / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_5: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_FOUR  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+// testpages start here
+page_table_leaf_entry_6: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_FIVE  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_7: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_SIX   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_8: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_SEVEN / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_9: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_EIGHT / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_A: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_NINE  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .dword 0x0000000000000000
+dummy_001: .dword 0x0000000000000000
+dummy_002: .dword 0x1122334455667788
+dummy_003: .dword 0x0123456789ABCDEF
+dummy_004: .dword 0xDEADDEADDEADDEAD
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x2
+1:
+  li TESTNUM, 10
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_000, 0x0000000000000000, die_u)
+  li TESTNUM, 11
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_000, 0x0000000000000000, die_u)
+  li TESTNUM, 12
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_001, 0x0000000000000000, die_u)
+  li TESTNUM, 13
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_001, 0x0000000000000000, die_u)
+  li TESTNUM, 14
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_002, 0x1122334455667788, die_u)
+  li TESTNUM, 15
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_002, 0x1122334455667788, die_u)
+  li TESTNUM, 16
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_003, 0x0123456789ABCDEF, die_u)
+  li TESTNUM, 17
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_003, 0x0123456789ABCDEF, die_u)
+ 
+  nop
+  nop
+  nop
+  nop
+  li TESTNUM, 20
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_100, 0x0000000000000000, die_u)
+  li TESTNUM, 21
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_100, 0x0000000000000000, die_u)
+  li TESTNUM, 22
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_101, 0x1111111111111111, die_u)
+  li TESTNUM, 23
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_101, 0x1111111111111111, die_u)
+  li TESTNUM, 24
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_102, 0x1122334455667788, die_u)
+  li TESTNUM, 25
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_102, 0x1122334455667788, die_u)
+  li TESTNUM, 26
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_103, 0x0123456789ABCDEF, die_u)
+  li TESTNUM, 27
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_103, 0x0123456789ABCDEF, die_u)
+  
+  nop
+  nop
+  nop
+  nop
+
+  li TESTNUM, 30
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_200, 0x0000000000000000, die_u)
+  li TESTNUM, 31
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_200, 0x0000000000000000, die_u)
+  li TESTNUM, 32
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_201, 0x2222222222222222, die_u)
+  li TESTNUM, 33
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_201, 0x2222222222222222, die_u)
+  li TESTNUM, 34
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_202, 0x1122334455667788, die_u)
+  li TESTNUM, 35
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_202, 0x1122334455667788, die_u)
+  li TESTNUM, 36
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_203, 0x0123456789ABCDEF, die_u)
+  li TESTNUM, 37
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_203, 0x0123456789ABCDEF, die_u)
+  
+  nop
+  nop
+  nop
+  nop
+
+  li TESTNUM, 40
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_300, 0x0000000000000000, die_u)
+  li TESTNUM, 41
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_300, 0x0000000000000000, die_u)
+  li TESTNUM, 42
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_301, 0x3333333333333333, die_u)
+  li TESTNUM, 43
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_301, 0x3333333333333333, die_u)
+  li TESTNUM, 44
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_302, 0x1122334455667788, die_u)
+  li TESTNUM, 45
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_302, 0x1122334455667788, die_u)
+  li TESTNUM, 46
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_303, 0x0123456789ABCDEF, die_u)
+  li TESTNUM, 47
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_303, 0x0123456789ABCDEF, die_u)
+
+  
+  nop
+  nop
+  nop
+  nop
+
+  li TESTNUM, 50
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_400, 0x0000000000000000, die_u)
+  li TESTNUM, 51
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_400, 0x0000000000000000, die_u)
+  li TESTNUM, 52
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_401, 0x4444444444444444, die_u)
+  li TESTNUM, 53
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_401, 0x4444444444444444, die_u)
+  li TESTNUM, 54
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_402, 0x1122334455667788, die_u)
+  li TESTNUM, 55
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_402, 0x1122334455667788, die_u)
+  li TESTNUM, 56
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_403, 0x0123456789ABCDEF, die_u)
+  li TESTNUM, 57
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_403, 0x0123456789ABCDEF, die_u)
+
+  
+  nop
+  nop
+  nop
+  nop
+
+  li TESTNUM, 60
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_500, 0x0000000000000000, die_u)
+  li TESTNUM, 61
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_500, 0x0000000000000000, die_u)
+  li TESTNUM, 62
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_501, 0x5555555555555555, die_u)
+  li TESTNUM, 63
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_501, 0x5555555555555555, die_u)
+  li TESTNUM, 64
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_502, 0x1122334455667788, die_u)
+  li TESTNUM, 65
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_502, 0x1122334455667788, die_u)
+  li TESTNUM, 66
+  # load dummy address on same page (fetches slow path)
+  read_double_as_all_types(t0, t1, dummy_503, 0x0123456789ABCDEF, die_u)
+  li TESTNUM, 67
+  # load dummy address on same page (fetches should access cache)
+  read_double_as_all_types(t0, t1, dummy_503, 0x0123456789ABCDEF, die_u)
+
+
+  nop
+  nop
+  nop
+  nop
+  nop
+  nop
+  li a7, 0x0
+  ecall
+  
+  RVTEST_CODE_END
+
+die_u:
+  li a7, 0x1
+  ecall
+  
+
+RVTEST_CODE_END
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+# don't rewrite
+.align 12
+dummy_100: .dword 0x0000000000000000
+dummy_101: .dword 0x1111111111111111
+dummy_102: .dword 0x1122334455667788
+dummy_103: .dword 0x0123456789ABCDEF
+dummy_104: .dword 0xDEADDEADDEADDEAD
+
+# PAGE_TWO
+# dummy data on separate page to test everything
+# write using words
+.align 12
+dummy_200: .dword 0x0000000000000000
+dummy_201: .dword 0x2222222222222222
+dummy_202: .dword 0x1122334455667788
+dummy_203: .dword 0x0123456789ABCDEF
+dummy_204: .dword 0xDEADDEADDEADDEAD
+
+# PAGE_THREE
+# dummy data on separate page to test everything
+# write using half words
+.align 12
+dummy_300: .dword 0x0000000000000000
+dummy_301: .dword 0x3333333333333333
+dummy_302: .dword 0x1122334455667788
+dummy_303: .dword 0x0123456789ABCDEF
+dummy_304: .dword 0xDEADDEADDEADDEAD
+
+# PAGE_FOUR
+# dummy data on separate page to test everything
+# write using bytes
+.align 12
+dummy_400: .dword 0x0000000000000000
+dummy_401: .dword 0x4444444444444444
+dummy_402: .dword 0x1122334455667788
+dummy_403: .dword 0x0123456789ABCDEF
+dummy_404: .dword 0xDEADDEADDEADDEAD
+
+# PAGE_FIVE
+# dummy data on separate page to test everything
+# write using doubles
+.align 12
+dummy_500: .dword 0x0000000000000000
+dummy_501: .dword 0x5555555555555555
+dummy_502: .dword 0x1122334455667788
+dummy_503: .dword 0x0123456789ABCDEF
+dummy_504: .dword 0xDEADDEADDEADDEAD
+
+
+# PAGE_FOUR
+.align 12
+
+RVTEST_DATA_END
diff --git a/isa/rv64uz/fetch_authentication_exception.S b/isa/rv64uz/fetch_authentication_exception.S
new file mode 100644
index 0000000..6856830
--- /dev/null
+++ b/isa/rv64uz/fetch_authentication_exception.S
@@ -0,0 +1,266 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# fetch_authentication_exception.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+#define EXPECTED_EXCEPTIONS 6
+
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+setup_m_to_u_vm(a1)
+
+// Set SID for write
+write_value_to_csr(0x8000000000000001, CSR_E_MSID_0, a1)
+write_value_to_csr((PAGE_TWO  - PAGE_ZERO  )      , CSR_E_MRANGE_VBASE, a1)
+write_value_to_csr((PAGE_FIVE - PAGE_TWO - 1)     , CSR_E_MRANGE_VSIZE, a1)
+// set unaligned range test
+set_bit_in_csr(Z_DBG_VSIZE_XRANGE_IS_SIZE_NOT_MASK_NO_ALIGNMENT, CSR_E_DBG_CONTROL, a1)
+
+li TESTNUM, 1
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+li TESTNUM, 2
+set_and_enable_store_tweak(a1, E_TWEAK_ENCLAVE)
+li TESTNUM, 3
+copy_vma_to_enclave_vma(PAGE_TWO, PAGE_THREE, PAGE_TWO, PAGE_ZERO, a1, a2, a3, a4)
+
+li TESTNUM, 4
+// reset SID before return
+// for page three we first encrypt it correctly and the overwrite the physical memory
+// this tests also any non-strict authentication implemention
+write_value_to_csr(0x0, CSR_E_MSID_0, a1)
+
+copy_vma_to_enclave_vma(PAGE_THREE, PAGE_FOUR, PAGE_THREE, PAGE_ZERO, a1, a2, a3, a4)
+
+disable_lstweaks(a1)
+quick_disable_m_vm_access(a1)
+
+// voerwrite physical memory with jump to die_u
+set_physical_memory_range_to_value(PAGE_THREE, PAGE_FOUR, 0x0a00006f, a1, a2, a3)
+
+
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+# prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+li TESTNUM, 5
+mret
+
+.align 8
+// Last parameter must hold the number of expected exceptions!
+usermode_exit_handler_success_on_fetch_decryption_error(a0, t0, t3, a7, EXPECTED_EXCEPTIONS, final_ecall, decryption_exception)
+
+return_to_user_jump_table_init:
+  addi a0, TESTNUM, -10; // align to counter start in usermode
+  slli a0, a0, 2;
+  la t0, return_to_user_jump_table;
+  add t0, a0, t0;
+  jalr t0, 0(t0);
+
+return_to_user_jump_table:
+  j die; // should not result in an exception
+  j die; // should not result in an exception
+  j die; // should not result in an exception
+  j die; // should not result in an exception
+  j m_fetch_test_05; // 
+  j m_fetch_test_06; // 
+  j m_fetch_test_07; // 
+  j m_fetch_test_08; // 
+  j m_fetch_test_09; // 
+  j m_fetch_test_10; // 
+
+m_fetch_test_05:
+  set_user_pc_to_userlabel(a0, t0, fetch_test_06, PAGE_ZERO)
+  j return_to_userland;
+m_fetch_test_06:
+  set_user_pc_to_userlabel(a0, t0, fetch_test_07, PAGE_ZERO)
+  j return_to_userland;
+m_fetch_test_07:
+  set_user_pc_to_userlabel(a0, t0, fetch_test_08, PAGE_ZERO)
+  j return_to_userland;
+m_fetch_test_08:
+  set_user_pc_to_userlabel(a0, t0, fetch_test_09, PAGE_ZERO)
+  j return_to_userland;
+m_fetch_test_09:
+  set_user_pc_to_userlabel(a0, t0, fetch_test_10, PAGE_ZERO)
+  j return_to_userland;
+m_fetch_test_10:
+  set_user_pc_to_userlabel(a0, t0, after_last_test, PAGE_ZERO)
+  j return_to_userland;
+
+return_to_userland:
+  mret
+
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ZERO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_2: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ONE   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_3: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_TWO   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_4: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_THREE / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_5: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_FOUR  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... befor an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0x00123456
+dummy_001: .word 0xCAFEBABE
+dummy_002: .word 0xFFFFFFFF
+dummy_003: .word 0xABCDEF12
+dummy_004: .word 0xDEADBEEF
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x0
+
+// access anything twice to check caching behaviour
+
+// unencrypted fetch
+// TODO do something to also test cache here... loop, counter, jump,...
+fetch_test_01:
+  li TESTNUM, 10
+  jump_and_check_return_test(t0, t1, PERSISTANT_REG_FOR_RET, unencrypted_page_1, die_u)
+fetch_test_02:
+  li TESTNUM, 11
+  jump_and_check_return_test(t0, t1, PERSISTANT_REG_FOR_RET, unencrypted_page_2, die_u)
+
+// weak authentication this should pass
+fetch_test_03:
+  set_bit_in_csr(Z_DBG_AUTH_WEAK, CSR_E_DBG_CONTROL, t0)
+  li TESTNUM, 12
+  jump_and_check_return_test(t0, t1, PERSISTANT_REG_FOR_RET, encrypted_page_different_SID_1, die_u)
+fetch_test_04:
+  li TESTNUM, 13
+  jump_and_check_return_test(t0, t1, PERSISTANT_REG_FOR_RET, encrypted_page_different_SID_2, die_u)
+
+// strict authentication should throw exception
+fetch_test_05:
+  clear_bit_in_csr(Z_DBG_AUTH_WEAK, CSR_E_DBG_CONTROL, t0)
+  li TESTNUM, 14
+  jump_and_check_return_test_expect_fail(t0, t1, PERSISTANT_REG_FOR_RET, encrypted_page_different_SID_3, die_u)
+fetch_test_06:
+  li TESTNUM, 15
+  jump_and_check_return_test_expect_fail(t0, t1, PERSISTANT_REG_FOR_RET, encrypted_page_different_SID_3, die_u)
+
+
+// weak authentication
+fetch_test_07:
+  set_bit_in_csr(Z_DBG_AUTH_WEAK, CSR_E_DBG_CONTROL, t0)
+  li TESTNUM, 16 
+  jump_and_check_return_test_expect_fail(t0, t1, PERSISTANT_REG_FOR_RET, encrypted_page_physically_overwritten_1, die_u)
+fetch_test_08:
+  li TESTNUM, 17
+  jump_and_check_return_test_expect_fail(t0, t1, PERSISTANT_REG_FOR_RET, encrypted_page_physically_overwritten_1, die_u)
+// strict authentication
+fetch_test_09:
+  clear_bit_in_csr(Z_DBG_AUTH_WEAK, CSR_E_DBG_CONTROL, t0)
+  li TESTNUM, 18 
+  jump_and_check_return_test_expect_fail(t0, t1, PERSISTANT_REG_FOR_RET, encrypted_page_physically_overwritten_2, die_u)
+fetch_test_10:
+  li TESTNUM, 19
+  jump_and_check_return_test_expect_fail(t0, t1, PERSISTANT_REG_FOR_RET, encrypted_page_physically_overwritten_2, die_u)
+
+after_last_test:
+  nop
+  nop
+  nop
+  nop
+  nop
+  li TESTNUM, 100
+  ecall
+  
+  RVTEST_CODE_END
+
+die_u:
+  li a7, 0x1
+  ecall
+  
+
+RVTEST_CODE_END
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+.align 12
+dummy_100: .word 0x00000123
+dummy_101: .word 0xCEAEBEBA
+unencrypted_page_1:
+  jump_and_check_return_dst(persistant_return_reg)
+unencrypted_page_2:
+  jump_and_check_return_dst(persistant_return_reg)
+dummy_104: .word 0x88888888
+dummy_105: .word 0x12345ABC
+dummy_106: .word 0x11111111
+
+# PAGE_TWO 
+# data is encrypted with different SID accesses are ought to fail in strict mode
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0xBEEFDEAD
+dummy_201: .word 0xBEEFDEAD
+encrypted_page_different_SID_1:
+  jump_and_check_return_dst(persistant_return_reg)
+encrypted_page_different_SID_2:
+  jump_and_check_return_dst(persistant_return_reg)
+encrypted_page_different_SID_3:
+  j die_u
+dummy_205: .word 0xBEEFDEAD
+dummy_206: .word 0xBEEFDEAD
+dummy_207: .word 0xBEEFDEAD
+
+# PAGE_THREE
+# data is encrypted with zero SID, accesses should succeed
+# dummy data on separate page to test everything
+.align 12
+dummy_300: .word 0xBEEFDEAD
+dummy_301: .word 0xBEEFDEAD
+dummy_302: .word 0xBEEFDEAD
+encrypted_page_physically_overwritten_1: .word 0xCAFECAFE
+encrypted_page_physically_overwritten_2: .word 0xBEEFBEEF
+dummy_305: .word 0xBEEFDEAD
+dummy_306: .word 0xBEEFDEAD
+
+.align 12
+RVTEST_DATA_END
diff --git a/isa/rv64uz/load_authentication_exception.S b/isa/rv64uz/load_authentication_exception.S
new file mode 100644
index 0000000..93ed0d5
--- /dev/null
+++ b/isa/rv64uz/load_authentication_exception.S
@@ -0,0 +1,217 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# load_authentication_exception.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+#define EXPECTED_EXCEPTIONS 6
+
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+setup_m_to_u_vm(a1)
+
+// Set SID for write
+write_value_to_csr(0x8000000000000001, CSR_E_MSID_0, a1)
+write_value_to_csr((PAGE_ONE   - PAGE_ZERO   )      , CSR_E_MRANGE_VBASE, a1)
+write_value_to_csr((PAGE_THREE - PAGE_ONE - 1)     , CSR_E_MRANGE_VSIZE, a1)
+// set unaligned range test
+set_bit_in_csr(Z_DBG_VSIZE_XRANGE_IS_SIZE_NOT_MASK_NO_ALIGNMENT, CSR_E_DBG_CONTROL, a1)
+
+li TESTNUM, 1
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+li TESTNUM, 2
+set_and_enable_store_tweak(a1, E_TWEAK_ENCLAVE)
+li TESTNUM, 3
+copy_vma_to_enclave_vma(PAGE_ONE, PAGE_TWO, PAGE_ONE, PAGE_ZERO, a1, a2, a3, a4)
+
+li TESTNUM, 4
+// reset SID before return
+// for page TWO we first encrypt it correctly and then overwrite the physical memory
+// this tests also any non-strict authentication implemention
+write_value_to_csr(0x0, CSR_E_MSID_0, a1)
+
+copy_vma_to_enclave_vma(PAGE_TWO, PAGE_THREE, PAGE_TWO, PAGE_ZERO, a1, a2, a3, a4)
+
+disable_lstweaks(a1)
+quick_disable_m_vm_access(a1)
+
+set_physical_memory_range_to_value(PAGE_TWO, PAGE_THREE, 0xffffffff, a1, a2, a3)
+
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+# prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+li TESTNUM, 5
+mret
+
+.align 8
+// Last parameter must hold the number of expected exceptions!
+usermode_exit_handler_success_on_decryption_error(a0, t0, t3, a7, EXPECTED_EXCEPTIONS, final_ecall, decryption_exception)
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ZERO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_2: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_ONE   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_3: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_TWO   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_4: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_THREE / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0x00123456
+dummy_001: .word 0xCAFEBABE
+dummy_002: .word 0xFFFFFFFF
+dummy_003: .word 0xABCDEF12
+dummy_004: .word 0xDEADBEEF
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x0
+
+  # this passes fine!
+  # // if key has to be incremented too often there's an error
+  li TESTNUM, 10
+  # load dummy address on same page (fetches slow path)
+  read_value_test(t0, t1, dummy_000, 0x00123456, die_u)
+
+  li TESTNUM, 11
+  # load dummy address on same page (fetches should access cache)
+  read_value_test(t0, t1, dummy_000, 0x00123456, die_u)
+
+  nop
+  nop
+  nop
+  nop
+
+  set_bit_in_csr(Z_DBG_AUTH_WEAK, CSR_E_DBG_CONTROL, t0)
+  // This should not trigger the exception for authentication == weak:
+  li TESTNUM, 20
+  # load dummy address on other page (should work)
+  read_value_test(t0, t1, dummy_101, 0xCEAEBEBA, die_u)
+2:
+  li TESTNUM, 21
+  # load dummy address on other page (should work)
+  read_value_test(t0, t1, dummy_101, 0xCEAEBEBA, die_u)
+2:
+
+  clear_bit_in_csr(Z_DBG_AUTH_WEAK, CSR_E_DBG_CONTROL, t0)
+  // This should trigger the exception for authentication = stric:
+  li TESTNUM, 22
+  # load dummy address on other page (throws exception)
+  read_value_test(t0, t1, dummy_101, 0xCEAEBEBA, 2f)
+2:
+  li TESTNUM, 23
+  # load dummy address on other page (throws exception)
+  read_value_test(t0, t1, dummy_101, 0xCEAEBEBA, 2f)
+2:
+  nop
+  nop
+  nop
+  nop
+
+  set_bit_in_csr(Z_DBG_AUTH_WEAK, CSR_E_DBG_CONTROL, t0)
+  // This should trigger the exception in weak and strict authentication:
+  li TESTNUM, 30
+  # load dummy address on other page (throws exception)
+  read_value_test(t0, t1, dummy_201, 0x0, 2f)
+2:
+  li TESTNUM, 31
+  # load dummy address on other page (throws exception)
+  read_value_test(t0, t1, dummy_201, 0x0, 2f)
+2:
+
+  clear_bit_in_csr(Z_DBG_AUTH_WEAK, CSR_E_DBG_CONTROL, t0)
+  // This should trigger the exception in weak and strict authentication:
+  li TESTNUM, 30
+  # load dummy address on other page (throws exception)
+  read_value_test(t0, t1, dummy_204, 0x0, 2f)
+2:
+  li TESTNUM, 31
+  # load dummy address on other page (throws exception)
+  read_value_test(t0, t1, dummy_204, 0x0, 2f)
+2:
+
+
+
+  nop
+  nop
+  nop
+  nop
+  nop
+  li TESTNUM, 100
+  ecall
+  
+  RVTEST_CODE_END
+
+die_u:
+  li a7, 0x1
+  ecall
+  
+
+RVTEST_CODE_END
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+.align 12
+dummy_100: .word 0x00000123
+dummy_101: .word 0xCEAEBEBA
+dummy_102: .word 0x88888888
+dummy_103: .word 0x12345ABC
+dummy_104: .word 0x11111111
+
+# PAGE_TWO
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0xBEEFDEAD
+dummy_201: .word 0xBEEFDEAD
+dummy_202: .word 0xBEEFDEAD
+dummy_203: .word 0xBADEAFFE
+dummy_204: .word 0xBEEFDEAD
+
+# Three
+# dummy data on separate page to test everything
+.align 12
+RVTEST_DATA_END
diff --git a/isa/rv64uz/load_store_operation_for_hw.S b/isa/rv64uz/load_store_operation_for_hw.S
new file mode 100644
index 0000000..6404360
--- /dev/null
+++ b/isa/rv64uz/load_store_operation_for_hw.S
@@ -0,0 +1,385 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# store_authentication_exception.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+#define EXPECTED_EXCEPTIONS 12
+
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+setup_m_to_u_vm(a1)
+
+
+
+// Set SID for write
+write_value_to_csr(0x8C0FFEE0000BEEF1, CSR_E_MSID_0, a1)
+
+set_bit_in_csr(Z_DBG_VSIZE_XRANGE_IS_SIZE_NOT_MASK_NO_ALIGNMENT, CSR_E_DBG_CONTROL, a1)
+
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+# # prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+
+li TESTNUM, 6
+mret
+
+.align 8
+usermode_exit_handler(a0, t0, t3, a7)
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_UNPROTECTED   << PTE_MSB_SHIFT | (( PAGE_ZERO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_2: .dword E_PT_UNPROTECTED   << PTE_MSB_SHIFT | (( PAGE_ONE   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_3: .dword E_PT_UNPROTECTED   << PTE_MSB_SHIFT | (( PAGE_TWO   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_4: .dword E_PT_UNPROTECTED   << PTE_MSB_SHIFT | (( PAGE_THREE / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0x00123456
+dummy_001: .word 0xCAFEBABE
+dummy_002: .word 0xFFFFFFFF
+dummy_003: .word 0xABCDEF12
+dummy_004: .word 0xDEADBEEF
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x0
+1:
+  # // if key has to be incremented too often there's an error
+  # addi TESTNUM, TESTNUM, 1
+  li TESTNUM, 10
+  read_value_test(a1, a2, dummy_100, 0x00000000, die_u);
+  read_value_test(a1, a2, dummy_101, 0x11111111, die_u);
+  read_value_test(a1, a2, dummy_102, 0x22222222, die_u);
+  read_value_test(a1, a2, dummy_103, 0x33333333, die_u);
+  read_value_test(a1, a2, dummy_104, 0x44444444, die_u);
+  read_value_test(a1, a2, dummy_105, 0x55555555, die_u);
+  read_value_test(a1, a2, dummy_106, 0x66666666, die_u);
+  read_value_test(a1, a2, dummy_107, 0x77777777, die_u);
+  read_value_test(a1, a2, dummy_108, 0x88888888, die_u);
+  read_value_test(a1, a2, dummy_109, 0x99999999, die_u);
+  read_value_test(a1, a2, dummy_10A, 0xAAAAAAAA, die_u);
+  read_value_test(a1, a2, dummy_10B, 0xBBBBBBBB, die_u);
+  read_value_test(a1, a2, dummy_10C, 0xCCCCCCCC, die_u);
+  read_value_test(a1, a2, dummy_10D, 0xDDDDDDDD, die_u);
+  read_value_test(a1, a2, dummy_10E, 0xEEEEEEEE, die_u);
+  read_value_test(a1, a2, dummy_10F, 0xFFFFFFFF, die_u);
+  read_value_test(a1, a2, dummy_110, 0x00000000, die_u);
+  read_value_test(a1, a2, dummy_111, 0x11111111, die_u);
+  read_value_test(a1, a2, dummy_112, 0x22222222, die_u);
+  read_value_test(a1, a2, dummy_113, 0x33333333, die_u);
+  read_value_test(a1, a2, dummy_114, 0x44444444, die_u);
+  read_value_test(a1, a2, dummy_115, 0x55555555, die_u);
+  read_value_test(a1, a2, dummy_116, 0x66666666, die_u);
+  read_value_test(a1, a2, dummy_117, 0x77777777, die_u);
+  read_value_test(a1, a2, dummy_118, 0x88888888, die_u);
+  read_value_test(a1, a2, dummy_119, 0x99999999, die_u);
+  read_value_test(a1, a2, dummy_11A, 0xAAAAAAAA, die_u);
+  read_value_test(a1, a2, dummy_11B, 0xBBBBBBBB, die_u);
+  read_value_test(a1, a2, dummy_11C, 0xCCCCCCCC, die_u);
+  read_value_test(a1, a2, dummy_11D, 0xDDDDDDDD, die_u);
+  read_value_test(a1, a2, dummy_11E, 0xEEEEEEEE, die_u);
+  read_value_test(a1, a2, dummy_11F, 0xFFFFFFFF, die_u);
+
+  // mid cacheline test
+  read_value_test(a1, a2, dummy_129, 0x99999999, die_u);
+  read_value_test(a1, a2, dummy_12A, 0xAAAAAAAA, die_u);
+  read_value_test(a1, a2, dummy_12B, 0xBBBBBBBB, die_u);
+  read_value_test(a1, a2, dummy_12C, 0xCCCCCCCC, die_u);
+  read_value_test(a1, a2, dummy_12D, 0xDDDDDDDD, die_u);
+  read_value_test(a1, a2, dummy_12E, 0xEEEEEEEE, die_u);
+  read_value_test(a1, a2, dummy_12F, 0xFFFFFFFF, die_u);
+  
+  li TESTNUM, 0xB
+  write_and_readback_value_test(a1, a2, dummy_200, 0x00000000, die_u);
+  write_and_readback_value_test(a1, a2, dummy_200, 0x00000000, die_u);
+  write_and_readback_value_test(a1, a2, dummy_201, 0x11111111, die_u);
+  write_and_readback_value_test(a1, a2, dummy_201, 0x11111111, die_u);
+  write_and_readback_value_test(a1, a2, dummy_202, 0x22222222, die_u);
+  write_and_readback_value_test(a1, a2, dummy_202, 0x22222222, die_u);
+  write_and_readback_value_test(a1, a2, dummy_203, 0x33333333, die_u);
+  write_and_readback_value_test(a1, a2, dummy_203, 0x33333333, die_u);
+  write_and_readback_value_test(a1, a2, dummy_204, 0x44444444, die_u);
+  write_and_readback_value_test(a1, a2, dummy_204, 0x44444444, die_u);
+  write_and_readback_value_test(a1, a2, dummy_205, 0x55555555, die_u);
+  write_and_readback_value_test(a1, a2, dummy_205, 0x55555555, die_u);
+  write_and_readback_value_test(a1, a2, dummy_206, 0x66666666, die_u);
+  write_and_readback_value_test(a1, a2, dummy_206, 0x66666666, die_u);
+  write_and_readback_value_test(a1, a2, dummy_207, 0x77777777, die_u);
+  write_and_readback_value_test(a1, a2, dummy_207, 0x77777777, die_u);
+  write_and_readback_value_test(a1, a2, dummy_208, 0x88888888, die_u);
+  write_and_readback_value_test(a1, a2, dummy_208, 0x88888888, die_u);
+  write_and_readback_value_test(a1, a2, dummy_209, 0x99999999, die_u);
+  write_and_readback_value_test(a1, a2, dummy_209, 0x99999999, die_u);
+  write_and_readback_value_test(a1, a2, dummy_20A, 0xAAAAAAAA, die_u);
+  write_and_readback_value_test(a1, a2, dummy_20A, 0xAAAAAAAA, die_u);
+  write_and_readback_value_test(a1, a2, dummy_20B, 0xBBBBBBBB, die_u);
+  write_and_readback_value_test(a1, a2, dummy_20B, 0xBBBBBBBB, die_u);
+  write_and_readback_value_test(a1, a2, dummy_20C, 0xCCCCCCCC, die_u);
+  write_and_readback_value_test(a1, a2, dummy_20C, 0xCCCCCCCC, die_u);
+  write_and_readback_value_test(a1, a2, dummy_20D, 0xDDDDDDDD, die_u);
+  write_and_readback_value_test(a1, a2, dummy_20D, 0xDDDDDDDD, die_u);
+  write_and_readback_value_test(a1, a2, dummy_20E, 0xEEEEEEEE, die_u);
+  write_and_readback_value_test(a1, a2, dummy_20E, 0xEEEEEEEE, die_u);
+  write_and_readback_value_test(a1, a2, dummy_20F, 0xFFFFFFFF, die_u);
+  write_and_readback_value_test(a1, a2, dummy_20F, 0xFFFFFFFF, die_u);
+  li TESTNUM, 0xC
+  // Change rtid every four writes (given a cacheline size of 128bits):
+  write_value_to_csr(0x1111111111111111, CSR_E_DBG_MSID_0, a1)
+  write_and_readback_value_test(a1, a2, dummy_210, 0x00000000, die_u);
+  write_and_readback_value_test(a1, a2, dummy_210, 0x00000000, die_u);
+  write_and_readback_value_test(a1, a2, dummy_211, 0x11111111, die_u);
+  write_and_readback_value_test(a1, a2, dummy_211, 0x11111111, die_u);
+  write_and_readback_value_test(a1, a2, dummy_212, 0x22222222, die_u);
+  write_and_readback_value_test(a1, a2, dummy_212, 0x22222222, die_u);
+  write_and_readback_value_test(a1, a2, dummy_213, 0x33333333, die_u);
+  write_and_readback_value_test(a1, a2, dummy_213, 0x33333333, die_u);
+  li TESTNUM, 0xD
+  write_value_to_csr(0x2222222222222222, CSR_E_DBG_MSID_0, a1)
+  write_and_readback_value_test(a1, a2, dummy_214, 0x44444444, die_u);
+  write_and_readback_value_test(a1, a2, dummy_214, 0x44444444, die_u);
+  write_and_readback_value_test(a1, a2, dummy_215, 0x55555555, die_u);
+  write_and_readback_value_test(a1, a2, dummy_215, 0x55555555, die_u);
+  write_and_readback_value_test(a1, a2, dummy_216, 0x66666666, die_u);
+  write_and_readback_value_test(a1, a2, dummy_216, 0x66666666, die_u);
+  write_and_readback_value_test(a1, a2, dummy_217, 0x77777777, die_u);
+  write_and_readback_value_test(a1, a2, dummy_217, 0x77777777, die_u);
+  li TESTNUM, 0xE
+  write_value_to_csr(0x3333333333333333, CSR_E_DBG_MSID_0, a1)
+  write_and_readback_value_test(a1, a2, dummy_218, 0x88888888, die_u);
+  write_and_readback_value_test(a1, a2, dummy_218, 0x88888888, die_u);
+  write_and_readback_value_test(a1, a2, dummy_219, 0x99999999, die_u);
+  write_and_readback_value_test(a1, a2, dummy_219, 0x99999999, die_u);
+  write_and_readback_value_test(a1, a2, dummy_21A, 0xAAAAAAAA, die_u);
+  write_and_readback_value_test(a1, a2, dummy_21A, 0xAAAAAAAA, die_u);
+  write_and_readback_value_test(a1, a2, dummy_21B, 0xBBBBBBBB, die_u);
+  write_and_readback_value_test(a1, a2, dummy_21B, 0xBBBBBBBB, die_u);
+  li TESTNUM, 0xF
+  write_value_to_csr(0x4444444444444444, CSR_E_DBG_MSID_0, a1)
+  write_and_readback_value_test(a1, a2, dummy_21C, 0xCCCCCCCC, die_u);
+  write_and_readback_value_test(a1, a2, dummy_21C, 0xCCCCCCCC, die_u);
+  write_and_readback_value_test(a1, a2, dummy_21D, 0xDDDDDDDD, die_u);
+  write_and_readback_value_test(a1, a2, dummy_21D, 0xDDDDDDDD, die_u);
+  write_and_readback_value_test(a1, a2, dummy_21E, 0xEEEEEEEE, die_u);
+  write_and_readback_value_test(a1, a2, dummy_21E, 0xEEEEEEEE, die_u);
+  write_and_readback_value_test(a1, a2, dummy_21F, 0xFFFFFFFF, die_u);
+  write_and_readback_value_test(a1, a2, dummy_21F, 0xFFFFFFFF, die_u);
+
+  // do the same thing in the mid of the cacheline
+  li TESTNUM, 0x10
+  write_value_to_csr(0x5555555555555555, CSR_E_DBG_MSID_0, a1)
+  write_and_readback_value_test(a1, a2, dummy_229, 0x99999999, die_u);
+  write_and_readback_value_test(a1, a2, dummy_22A, 0xAAAAAAAA, die_u);
+  write_and_readback_value_test(a1, a2, dummy_22B, 0xBBBBBBBB, die_u);
+  write_and_readback_value_test(a1, a2, dummy_22C, 0xCCCCCCCC, die_u);
+  write_and_readback_value_test(a1, a2, dummy_22D, 0xDDDDDDDD, die_u);
+  write_and_readback_value_test(a1, a2, dummy_22E, 0xEEEEEEEE, die_u);
+  write_and_readback_value_test(a1, a2, dummy_22F, 0xFFFFFFFF, die_u);
+
+  // Write to start of cacheline, read with different rtid
+  li TESTNUM, 0x11
+  write_value_to_csr(0x6666666666666666, CSR_E_DBG_MSID_0, a1)
+  write_and_readback_value_test(a1, a2, dummy_230, 0x00000000, die_u);
+  write_value_to_csr(0x7777777777777777, CSR_E_DBG_MSID_0, a1)
+  read_value_test(a1, a2, dummy_230, 0x00000000, die_u);
+  // Write to center of cacheline, read with different rtid
+  li TESTNUM, 0x12
+  write_value_to_csr(0x6666666666666666, CSR_E_DBG_MSID_0, a1)
+  write_and_readback_value_test(a1, a2, dummy_235, 0x55555555, die_u);
+  write_value_to_csr(0x7777777777777777, CSR_E_DBG_MSID_0, a1)
+  read_value_test(a1, a2, dummy_235, 0x55555555, die_u);
+
+  // write to same cacheline twice with different rtid
+  li TESTNUM, 0x13
+  write_value_to_csr(0x6666666666666666, CSR_E_DBG_MSID_0, a1)
+  write_and_readback_value_test(a1, a2, dummy_238, 0x33333333, die_u);
+  write_value_to_csr(0x8888888888888888, CSR_E_DBG_MSID_0, a1)
+  write_and_readback_value_test(a1, a2, dummy_238, 0x44444444, die_u);
+  // write to middle same cacheline twice with different rtid
+  li TESTNUM, 0x14
+  write_value_to_csr(0x9999999999999999, CSR_E_DBG_MSID_0, a1)
+  write_and_readback_value_test(a1, a2, dummy_23D, 0x55555555, die_u);
+  write_value_to_csr(0xAAAAAAAAAAAAAAAA, CSR_E_DBG_MSID_0, a1)
+  write_and_readback_value_test(a1, a2, dummy_23D, 0x66666666, die_u);
+  // 
+
+
+  li TESTNUM, 0x0
+  ecall
+  
+  RVTEST_CODE_END
+
+die_u:
+  li a7, 0x1
+  ecall
+  
+
+RVTEST_CODE_END
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+.align 12
+dummy_100: .word 0x00000000
+dummy_101: .word 0x11111111
+dummy_102: .word 0x22222222
+dummy_103: .word 0x33333333
+dummy_104: .word 0x44444444
+dummy_105: .word 0x55555555
+dummy_106: .word 0x66666666
+dummy_107: .word 0x77777777
+dummy_108: .word 0x88888888
+dummy_109: .word 0x99999999
+dummy_10A: .word 0xAAAAAAAA
+dummy_10B: .word 0xBBBBBBBB
+dummy_10C: .word 0xCCCCCCCC
+dummy_10D: .word 0xDDDDDDDD
+dummy_10E: .word 0xEEEEEEEE
+dummy_10F: .word 0xFFFFFFFF
+dummy_110: .word 0x00000000
+dummy_111: .word 0x11111111
+dummy_112: .word 0x22222222
+dummy_113: .word 0x33333333
+dummy_114: .word 0x44444444
+dummy_115: .word 0x55555555
+dummy_116: .word 0x66666666
+dummy_117: .word 0x77777777
+dummy_118: .word 0x88888888
+dummy_119: .word 0x99999999
+dummy_11A: .word 0xAAAAAAAA
+dummy_11B: .word 0xBBBBBBBB
+dummy_11C: .word 0xCCCCCCCC
+dummy_11D: .word 0xDDDDDDDD
+dummy_11E: .word 0xEEEEEEEE
+dummy_11F: .word 0xFFFFFFFF
+dummy_120: .word 0x00000000
+dummy_121: .word 0x11111111
+dummy_122: .word 0x22222222
+dummy_123: .word 0x33333333
+dummy_124: .word 0x44444444
+dummy_125: .word 0x55555555
+dummy_126: .word 0x66666666
+dummy_127: .word 0x77777777
+dummy_128: .word 0x88888888
+dummy_129: .word 0x99999999
+dummy_12A: .word 0xAAAAAAAA
+dummy_12B: .word 0xBBBBBBBB
+dummy_12C: .word 0xCCCCCCCC
+dummy_12D: .word 0xDDDDDDDD
+dummy_12E: .word 0xEEEEEEEE
+dummy_12F: .word 0xFFFFFFFF
+
+# PAGE_TWO
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0xBEEFDEAD
+dummy_201: .word 0xBEEFDEAD
+dummy_202: .word 0xBEEFDEAD
+dummy_203: .word 0xBEEFDEAD
+dummy_204: .word 0xBEEFDEAD
+dummy_205: .word 0xBEEFDEAD
+dummy_206: .word 0xBEEFDEAD
+dummy_207: .word 0xBEEFDEAD
+dummy_208: .word 0xBEEFDEAD
+dummy_209: .word 0xBEEFDEAD
+dummy_20A: .word 0xBEEFDEAD
+dummy_20B: .word 0xBEEFDEAD
+dummy_20C: .word 0xBEEFDEAD
+dummy_20D: .word 0xBEEFDEAD
+dummy_20E: .word 0xBEEFDEAD
+dummy_20F: .word 0xBEEFDEAD
+dummy_210: .word 0xBEEFDEAD
+dummy_211: .word 0xBEEFDEAD
+dummy_212: .word 0xBEEFDEAD
+dummy_213: .word 0xBEEFDEAD
+dummy_214: .word 0xBEEFDEAD
+dummy_215: .word 0xBEEFDEAD
+dummy_216: .word 0xBEEFDEAD
+dummy_217: .word 0xBEEFDEAD
+dummy_218: .word 0xBEEFDEAD
+dummy_219: .word 0xBEEFDEAD
+dummy_21A: .word 0xBEEFDEAD
+dummy_21B: .word 0xBEEFDEAD
+dummy_21C: .word 0xBEEFDEAD
+dummy_21D: .word 0xBEEFDEAD
+dummy_21E: .word 0xBEEFDEAD
+dummy_21F: .word 0xBEEFDEAD
+dummy_220: .word 0xBEEFDEAD
+dummy_221: .word 0xBEEFDEAD
+dummy_222: .word 0xBEEFDEAD
+dummy_223: .word 0xBEEFDEAD
+dummy_224: .word 0xBEEFDEAD
+dummy_225: .word 0xBEEFDEAD
+dummy_226: .word 0xBEEFDEAD
+dummy_227: .word 0xBEEFDEAD
+dummy_228: .word 0xBEEFDEAD
+dummy_229: .word 0xBEEFDEAD
+dummy_22A: .word 0xBEEFDEAD
+dummy_22B: .word 0xBEEFDEAD
+dummy_22C: .word 0xBEEFDEAD
+dummy_22D: .word 0xBEEFDEAD
+dummy_22E: .word 0xBEEFDEAD
+dummy_22F: .word 0xBEEFDEAD
+dummy_230: .word 0xBEEFDEAD
+dummy_231: .word 0xBEEFDEAD
+dummy_232: .word 0xBEEFDEAD
+dummy_233: .word 0xBEEFDEAD
+dummy_234: .word 0xBEEFDEAD
+dummy_235: .word 0xBEEFDEAD
+dummy_236: .word 0xBEEFDEAD
+dummy_237: .word 0xBEEFDEAD
+dummy_238: .word 0xBEEFDEAD
+dummy_239: .word 0xBEEFDEAD
+dummy_23A: .word 0xBEEFDEAD
+dummy_23B: .word 0xBEEFDEAD
+dummy_23C: .word 0xBEEFDEAD
+dummy_23D: .word 0xBEEFDEAD
+dummy_23E: .word 0xBEEFDEAD
+dummy_23F: .word 0xBEEFDEAD
+
+RVTEST_DATA_END
diff --git a/isa/rv64uz/machine_store_tweak_user_load.S b/isa/rv64uz/machine_store_tweak_user_load.S
new file mode 100644
index 0000000..2897903
--- /dev/null
+++ b/isa/rv64uz/machine_store_tweak_user_load.S
@@ -0,0 +1,204 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# machine_store_tweak_user_load.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+setup_m_to_u_vm(a1)
+
+// Set SID for write and configure mrange
+write_value_to_csr(         0x8000000000000001, CSR_E_MSID_0 , a1)
+write_value_to_csr((PAGE_TWO  - PAGE_ZERO  ) , CSR_E_MRANGE_VBASE, a1)
+write_value_to_csr((PAGE_FOUR - PAGE_TWO - 1), CSR_E_MRANGE_VSIZE, a1)
+
+// set the unaligned elrangetest
+set_bit_in_csr(Z_DBG_VSIZE_XRANGE_IS_SIZE_NOT_MASK_NO_ALIGNMENT, CSR_E_DBG_CONTROL, a1)
+
+nop;
+nop;
+nop;
+nop;
+
+quick_disable_m_vm_access(a0)
+li TESTNUM, 0x1
+check_physical_memory(dummy_001, a0, a1, 0xBEEFBEEF, die)
+li TESTNUM, 0x2
+check_physical_memory(dummy_102, a0, a1, 0xBEEFBEEF, die)
+li TESTNUM, 0x3
+check_physical_memory(dummy_203, a0, a1, 0xBEEFBEEF, die)
+li TESTNUM, 0x4
+check_physical_memory(dummy_300, a0, a1, 0xBEEFBEEF, die)
+
+
+nop;
+nop;
+nop;
+
+
+li a1, PAGE_ZERO
+li TESTNUM, 0x10
+set_and_enable_load_tweak(a0, E_TWEAK_UNPROTECTED_U)
+write_with_stweak_no_verify(E_TWEAK_UNPROTECTED_U, a0, t0, t1, dummy_001, a1, 0xBADEAFFE, die)
+li TESTNUM, 0x11
+write_with_stweak_no_verify(E_TWEAK_UNPROTECTED_U, a0, t0, t1, dummy_102, a1, 0xBADEAFFE, die)
+li TESTNUM, 0x12
+set_and_enable_load_tweak(a0, E_TWEAK_ENCLAVE)
+write_with_stweak_no_verify(E_TWEAK_ENCLAVE, a0, t0, t1, dummy_203, a1, 0xBADEAFFE, die)
+li TESTNUM, 0x13
+write_with_stweak_no_verify(E_TWEAK_ENCLAVE, a0, t0, t1, dummy_300, a1, 0xBADEAFFE, die)
+disable_lstweaks(a0)
+nop;
+nop;
+nop;
+
+
+quick_disable_m_vm_access(a0)
+// unencrypted page
+li TESTNUM, 0x20
+check_physical_memory(dummy_001, a0, a1, 0xBADEAFFE, die)
+li TESTNUM, 0x21
+check_physical_memory(dummy_102, a0, a1, 0xBADEAFFE, die)
+// these registers should have neither their original value nor the new value due to the encryption
+li TESTNUM, 0x22
+check_physical_memory_change(dummy_203, a0, a1, 0xBADEAFFE, die)
+li TESTNUM, 0x23
+check_physical_memory_change(dummy_203, a0, a1, 0xBEEFBEEF, die)
+li TESTNUM, 0x24
+check_physical_memory_change(dummy_300, a0, a1, 0xBADEAFFE, die)
+li TESTNUM, 0x25
+check_physical_memory_change(dummy_300, a0, a1, 0xBEEFBEEF, die)
+
+nop;
+nop;
+
+# ONLY for spike and the NOT "encryption", as we know the outcome of the encryption
+li TESTNUM, 0x31
+check_physical_memory(dummy_203, a0, a1, inv_32bit(0xBADEAFFE), die)
+li TESTNUM, 0x32
+check_physical_memory(dummy_300, a0, a1, inv_32bit(0xBADEAFFE), die)
+
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+# prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+mret
+
+.align 8
+usermode_exit_handler(a0, t0, t3, a7)
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ZERO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_2: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ONE   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_3: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_TWO   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_4: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_THREE / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_5: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_FOUR  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0xDEADBEEF
+dummy_001: .word 0xBEEFBEEF
+dummy_002: .word 0xDEADBEEF
+dummy_003: .word 0xDEADBEEF
+dummy_004: .word 0xDEADBEEF
+
+superstart:
+// access to unencrypted page
+li TESTNUM, 0x40
+check_physical_memory(dummy_001, a0, a1, 0xBADEAFFE, die_u)
+// access to other unencrypted page
+li TESTNUM, 0x41
+check_physical_memory(dummy_102, a0, a1, 0xBADEAFFE, die_u)
+// access to encrypted page
+li TESTNUM, 0x42
+check_physical_memory(dummy_203, a0, a1, 0xBADEAFFE, die_u)
+// access to other encrypted page
+li TESTNUM, 0x43
+check_physical_memory(dummy_300, a0, a1, 0xBADEAFFE, die_u)
+
+userspace_end_labels(end_u, die_u, a7)
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+.align 12
+dummy_100: .word 0xDEADBEEF
+dummy_101: .word 0xDEADBEEF
+dummy_102: .word 0xBEEFBEEF
+dummy_103: .word 0xDEADBEEF
+dummy_104: .word 0xDEADBEEF
+
+# PAGE_TWO
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0xDEADBEEF
+dummy_201: .word 0xDEADBEEF
+dummy_202: .word 0xDEADBEEF
+dummy_203: .word 0xBEEFBEEF
+dummy_204: .word 0xDEADBEEF
+
+# PAGE_THREE
+# dummy data on separate page to test everything
+.align 12
+dummy_300: .word 0xBEEFBEEF
+dummy_301: .word 0xDEADBEEF
+dummy_302: .word 0xDEADBEEF
+dummy_303: .word 0xDEADBEEF
+dummy_304: .word 0xDEADBEEF
+
+
+# PAGE_FOUR
+# dummy data on separate page to test everything
+.align 12
+dummy_400: .word 0xDEADBEEF
+dummy_401: .word 0xDEADBEEF
+dummy_402: .word 0xDEADBEEF
+dummy_403: .word 0xDEADBEEF
+dummy_404: .word 0xDEADBEEF
+
+RVTEST_DATA_END
diff --git a/isa/rv64uz/modify_pte.S b/isa/rv64uz/modify_pte.S
new file mode 100644
index 0000000..5a40e26
--- /dev/null
+++ b/isa/rv64uz/modify_pte.S
@@ -0,0 +1,137 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# modify_pte.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+setup_m_to_u_vm(a1)
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+set_and_enable_store_tweak(a1, E_TWEAK_ENCLAVE)
+disable_lstweaks(a1)
+
+
+li TESTNUM, 1
+  save_PTE_disable_VM(a1, page_to_leaf_address(PAGE_ZERO, PAGE_ZERO))
+li TESTNUM, 2
+
+  make_pte_in_reg_rwx(a1, a2)
+li TESTNUM, 3
+  restore_PTE_disable_VM(a2, a3, page_to_leaf_address(PAGE_ZERO, PAGE_ZERO))
+
+disable_lstweaks(a1)
+
+li TESTNUM, 4
+
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+# prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+li TESTNUM, 5
+li a7, 0x3
+mret
+
+.align 8
+usermode_exit_handler(a0, t0, t3, a7)
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ZERO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U                         | PTE_A | PTE_D
+page_table_leaf_entry_2: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ONE   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U                         | PTE_A | PTE_D
+page_table_leaf_entry_3: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_TWO   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U                         | PTE_A | PTE_D
+page_table_leaf_entry_4: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ONE   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_5: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_FOUR  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0x00123456
+dummy_001: .word 0xCAFEBABE
+dummy_002: .word 0xFFFFFFFF
+dummy_003: .word 0xABCDEF12
+dummy_004: .word 0xDEADBEEF
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x2
+
+  li a7, 0x0
+  ecall
+  
+
+RVTEST_CODE_END
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+.align 12
+dummy_100: .word 0x00000123
+dummy_101: .word 0xCEAEBEBA
+dummy_102: .word 0x88888888
+dummy_103: .word 0x12345ABC
+dummy_104: .word 0x11111111
+
+# PAGE_TWO
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0xBEEFDEAD
+dummy_201: .word 0xBEEFDEAD
+dummy_202: .word 0xBEEFDEAD
+dummy_203: .word 0xBADEAFFE
+dummy_204: .word 0xBEEFDEAD
+
+# PAGE_THREE
+# dummy data on separate page to test everything
+.align 12
+dummy_300: .word 0xBEEFBEEF
+dummy_301: .word 0xBEEFDEAD
+dummy_302: .word 0xBEEFDEAD
+dummy_303: .word 0xBEEFDEAD
+dummy_304: .word 0xBEEFDEAD
+
+
+# PAGE_FOUR
+.align 12
+RVTEST_DATA_END
diff --git a/isa/rv64uz/page_type_data_access.S b/isa/rv64uz/page_type_data_access.S
new file mode 100644
index 0000000..19bc178
--- /dev/null
+++ b/isa/rv64uz/page_type_data_access.S
@@ -0,0 +1,455 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# page_type_data_access.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+#define EXPECTED_EXCEPTIONS 6
+#define USER_TEST_START 11
+
+#define INVALID_ENCLAVE_CSR 0xFFFFFFFFFFFFFFFF
+
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+setup_m_to_u_vm(a1)
+
+#define VALID_CSR_E_MSID_0           0x0123456789ABCDEF
+#define VALID_CSR_E_MSID_1           0xFEDCBA9876543210
+#define VALID_CSR_E_TCS              0x8080808080808080
+#define VALID_CSR_E_SECS             0xBADEAFFEBADEAFFE
+#define VALID_CSR_E_USID_0           0xABABABABABABABAB
+#define VALID_CSR_E_USID_1           0xABAB
+
+#define VALID_E_MRANGE_VBASE         (PAGE_FIVE  - PAGE_ZERO  )
+#define VALID_E_MRANGE_VSIZE         (PAGE_TEN - PAGE_FIVE - 1)
+#define VALID_E_URANGE_VBASE         (PAGE_THREE  - PAGE_ZERO  )
+#define VALID_E_URANGE_VSIZE         (PAGE_FIVE - PAGE_THREE - 1)
+
+// Set SID for write and configure mrange
+write_value_to_csr(         0x8000000000000001, CSR_E_MSID_0 , a1)
+write_value_to_csr(VALID_E_MRANGE_VBASE , CSR_E_MRANGE_VBASE, a1)
+write_value_to_csr(VALID_E_MRANGE_VSIZE , CSR_E_MRANGE_VSIZE, a1)
+
+// set the unaligned elrangetest
+set_bit_in_csr(Z_DBG_VSIZE_XRANGE_IS_SIZE_NOT_MASK_NO_ALIGNMENT, CSR_E_DBG_CONTROL, a1)
+// allow insecure code fetches to preven
+
+// Set SID for write
+write_value_to_csr(INVALID_ENCLAVE_CSR, CSR_E_URANGE_VBASE, a1)
+write_value_to_csr(INVALID_ENCLAVE_CSR, CSR_E_URANGE_VSIZE, a1)
+write_value_to_csr(INVALID_ENCLAVE_CSR, CSR_E_MSID_0, a1) // RTID
+write_value_to_csr(INVALID_ENCLAVE_CSR, CSR_E_MSID_1, a1) // SHCODE
+write_value_to_csr(INVALID_ENCLAVE_CSR, CSR_E_TCS, a1)
+write_value_to_csr(INVALID_ENCLAVE_CSR, CSR_E_USID_0, a1)
+write_value_to_csr(INVALID_ENCLAVE_CSR, CSR_E_USID_1, a1)
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+li TESTNUM, 1
+set_and_enable_store_tweak(a1, E_TWEAK_UNPROTECTED_U)
+copy_vma_to_enclave_vma(PAGE_ONE, PAGE_TWO, PAGE_ONE, PAGE_ZERO, a1, a2, a3, a4)
+
+li TESTNUM, 2
+write_value_to_csr(VALID_CSR_E_MSID_0, CSR_E_MSID_0, a1)
+set_and_enable_store_tweak(a1, E_TWEAK_ENCLAVE)
+copy_vma_to_enclave_vma(PAGE_NINE, PAGE_TEN, PAGE_NINE, PAGE_ZERO, a1, a2, a3, a4)
+write_value_to_csr(INVALID_ENCLAVE_CSR, CSR_E_MSID_0, a1)
+
+li TESTNUM, 3
+write_value_to_csr(VALID_CSR_E_USID_0, CSR_E_USID_0, a1)
+write_value_to_csr(VALID_CSR_E_USID_1, CSR_E_USID_1, a1)
+write_value_to_csr(VALID_E_URANGE_VBASE        , CSR_E_URANGE_VBASE , a1)
+write_value_to_csr(VALID_E_URANGE_VSIZE        , CSR_E_URANGE_VSIZE , a1)
+set_and_enable_store_tweak(a1, E_TWEAK_SHDATA)
+copy_vma_to_enclave_vma_preserve_pte(PAGE_THREE, PAGE_FOUR, PAGE_THREE, PAGE_ZERO, a1, a2, a3, a4, t0)
+copy_vma_to_enclave_vma(PAGE_FOUR, PAGE_FIVE, PAGE_FOUR, PAGE_ZERO, a1, a2, a3, a4)
+write_value_to_csr(INVALID_ENCLAVE_CSR, CSR_E_URANGE_VBASE , a1)
+write_value_to_csr(INVALID_ENCLAVE_CSR, CSR_E_URANGE_VSIZE , a1)
+write_value_to_csr(INVALID_ENCLAVE_CSR, CSR_E_USID_0, a1)
+write_value_to_csr(INVALID_ENCLAVE_CSR, CSR_E_USID_1, a1)
+
+li TESTNUM, 4
+write_value_to_csr(VALID_CSR_E_MSID_1, CSR_E_MSID_1, a1)
+set_and_enable_store_tweak(a1, E_TWEAK_SHCODE)
+copy_vma_to_enclave_vma_preserve_pte(PAGE_FIVE, PAGE_SIX, PAGE_FIVE, PAGE_ZERO, a1, a2, a3, a4, t0)
+copy_vma_to_enclave_vma(PAGE_SIX, PAGE_SEVEN, PAGE_SIX, PAGE_ZERO, a1, a2, a3, a4)
+write_value_to_csr(INVALID_ENCLAVE_CSR, CSR_E_MSID_1, a1)
+
+// delete this, was previous thread meta
+li TESTNUM, 5
+write_value_to_csr(VALID_CSR_E_TCS, CSR_E_TCS, a1)
+set_and_enable_store_tweak(a1, E_TWEAK_MONITOR)
+copy_vma_to_enclave_vma(PAGE_TEN, PAGE_ELEVEN, PAGE_TEN, PAGE_ZERO, a1, a2, a3, a4)
+write_value_to_csr(INVALID_ENCLAVE_CSR, CSR_E_TCS, a1)
+
+li TESTNUM, 6
+write_value_to_csr(VALID_CSR_E_SECS, CSR_E_SECS, a1)
+set_and_enable_store_tweak(a1, E_TWEAK_MONITOR)
+copy_vma_to_enclave_vma(PAGE_ELEVEN, PAGE_TWELVE, PAGE_ELEVEN, PAGE_ZERO, a1, a2, a3, a4)
+write_value_to_csr(INVALID_ENCLAVE_CSR, CSR_E_SECS, a1)
+
+li TESTNUM, 7
+disable_lstweaks(a1)
+quick_disable_m_vm_access(a1)
+
+// set all CSR to correct value:
+write_value_to_csr(VALID_E_URANGE_VBASE        , CSR_E_URANGE_VBASE , a1)
+write_value_to_csr(VALID_E_URANGE_VSIZE        , CSR_E_URANGE_VSIZE , a1)
+write_value_to_csr(VALID_CSR_E_MSID_0          , CSR_E_MSID_0       , a1)
+write_value_to_csr(VALID_CSR_E_MSID_1          , CSR_E_MSID_1       , a1)
+write_value_to_csr(VALID_CSR_E_TCS             , CSR_E_TCS          , a1)
+write_value_to_csr(VALID_CSR_E_SECS            , CSR_E_SECS         , a1)
+write_value_to_csr(VALID_CSR_E_USID_0          , CSR_E_USID_0       , a1)
+write_value_to_csr(VALID_CSR_E_USID_1          , CSR_E_USID_1       , a1)
+
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+# prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+li TESTNUM, 10
+mret
+
+adapted_ecall:
+  li a0, 0x70661E
+  beq a0, a7, shdev_mode_loader
+  bnez a7, die
+  la a0, exception_save_slot
+  lwu t0, 0(a0)
+  li a0, EXPECTED_EXCEPTIONS
+  beq t0, a0, end
+  j die
+
+shdev_mode_loader:
+  j skip_instruction
+
+return_to_user_jump_table_init:
+  addi a0, TESTNUM, (-(USER_TEST_START)); // align to counter start in usermode
+  slli a0, a0, 2;
+  la t0, return_to_user_jump_table;
+  add t0, a0, t0;
+  jalr x0, 0(t0);
+
+return_to_user_jump_table:
+  j die;                   // E_TWEAK_UNPROTECTED_U        load         0-ex
+  j die;                   // E_TWEAK_UNPROTECTED_U        store        0-ex
+  j die;                   // E_TWEAK_UNPROTECTED_U        exec         0-ex
+  j die;                   // E_TWEAK_ENCLAVE load       load         0-ex
+  j die;                   // E_TWEAK_ENCLAVE store      store        0-ex
+  j die;                   // E_TWEAK_ENCLAVE exec       exec         0-ex
+  j die;                   // E_TWEAK_SHDATA                load         1-ex
+  j die;                   // E_TWEAK_SHDATA                store        1-ex
+  j return_to_caller;      // E_TWEAK_SHDATA                exec         1-ex
+  j die;                   // E_TWEAK_SHCODE            load         1-ex
+  j skip_instruction;      // E_TWEAK_SHCODE            store        1-ex
+  j die;                   // E_TWEAK_SHCODE            exec         1-ex
+  j skip_instruction;      // E_TWEAK_MONITOR         load         3-ex +1 for read-back  previous threadmeta 
+  j skip_instruction;      // E_TWEAK_MONITOR         store        3-ex +1 for read-back  previous threadmeta
+  j return_to_caller;      // E_TWEAK_MONITOR         exec         3-ex +1 for read-back  previous threadmeta
+  j skip_instruction;      // E_TWEAK_MONITOR               load         3-ex +1 for read-back
+  j skip_instruction;      // E_TWEAK_MONITOR               store        3-ex +1 for read-back
+  j return_to_caller;      // E_TWEAK_MONITOR               exec         3-ex +1 for read-back
+  nop
+  nop
+  nop
+  j die
+
+skip_instruction:
+  csrr t0, mepc;
+  ADDI t0, t0, 4;
+  csrw mepc, t0;
+  j return_to_userland
+
+return_to_caller:
+  set_user_pc_to_return_reg(PERSISTANT_REG_FOR_RET)
+  j return_to_userland
+
+return_to_userland:
+  mret
+
+.align 8
+// Last parameter must hold the number of expected exceptions!
+usermode_exit_handler_success_on_fetch_decryption_error(a0, t0, t3, a7, EXPECTED_EXCEPTIONS, adapted_ecall, decryption_exception)
+
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_01: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ZERO       / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_02: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ONE        / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_03: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_TWO        / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_04: .dword E_PT_SHDATA      << PTE_MSB_SHIFT | (( PAGE_THREE      / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W         | PTE_A | PTE_D
+page_table_leaf_entry_05: .dword E_PT_SHDATA      << PTE_MSB_SHIFT | (( PAGE_FOUR       / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_06: .dword E_PT_SHCODE      << PTE_MSB_SHIFT | (( PAGE_FIVE       / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R         | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_07: .dword E_PT_SHCODE      << PTE_MSB_SHIFT | (( PAGE_SIX        / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_08: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_SEVEN      / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R                 | PTE_A | PTE_D
+page_table_leaf_entry_09: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_EIGHT      / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_10: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_NINE       / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_13: .dword E_PT_MONITOR     << PTE_MSB_SHIFT | (( PAGE_TEN        / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_14: .dword E_PT_MONITOR     << PTE_MSB_SHIFT | (( PAGE_ELEVEN     / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0x00123456
+dummy_001: .word 0xCAFEBABE
+dummy_002: .word 0xFFFFFFFF
+dummy_003: .word 0xABCDEF12
+dummy_004: .word 0xDEADBEEF
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x0
+/*
+ for each page-type perforem
+ read check
+ write check
+ execute check
+ perform a good-will check, i.e. test that the correct values result in the correct behaviour
+ exception check performed separately
+
+ variables affecting the result:
+ SID_SIGNER
+ SID_ENCLAVE
+ SID_THREAD
+ RTID
+ SHM_shared_secret
+ 
+
+E_PT_UNPROTECTED     rwx   0-ex
+E_PT_REGULAR         rwx   0-ex
+E_PT_SHDATA          rw    1-ex
+E_PT_SHCODE          rx    1-ex +1 for read-back
+E_PT_THREADMETA      -     3-ex +1 for read-back
+E_PT_MONITOR         -     3-ex +1 for read-back
+--------------------------------------------
+                           8-ex +3 for readback (* 2 because we also test cache) = 22 total
+
+
+ */
+
+// access anything twice to check caching behaviour
+// unencrypted fetch
+  clear_bit_in_csr(Z_DBG_AUTH_WEAK, CSR_E_DBG_CONTROL, t2)
+  li TESTNUM, 10
+TEST_E_PT_UNPROTECTED:
+  // 11 - 13
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_101, dummy_103, dummy_105, 0x11111111, 0xAFFED00F, die_u)
+  nop
+TEST_E_PT_REGULAR:
+  // 14 - 16
+  load_store_execute_cycle_rwx(a1, t2, PERSISTANT_REG_FOR_RET, dummy_901, dummy_903, dummy_905, 0x99999999, 0xAFFED00F, die_u)
+  nop
+TEST_E_PT_SHDATA:
+  // 17 - 19
+  load_store_execute_cycle_rwo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_301, dummy_303, dummy_405, 0x33333333, 0xAFFED00F, die_u)
+  nop
+TEST_E_PT_SHCODE:
+  // 20 - 22
+  load_store_execute_cycle_rox(a1, t2, PERSISTANT_REG_FOR_RET, dummy_501, dummy_603, dummy_505, 0x55555555, 0xAFFED00F, die_u)
+  nop
+# TEST_E_PT_THREADMETA:
+#   // 23 - 25
+#   load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_A01, dummy_A03, dummy_A05, 0xAAAAAAAA, 0xAFFED00F, die_u)
+#   nop
+# TEST_E_PT_MONITOR:
+#   // 26 - 28
+#   load_store_execute_cycle_ooo(a1, t2, PERSISTANT_REG_FOR_RET, dummy_B01, dummy_B03, dummy_B05, 0xBBBBBBBB, 0xAFFED00F, die_u)
+  nop
+  nop
+  nop
+  nop
+after_last_test:
+  nop
+  nop
+  nop
+  nop
+  nop
+  li TESTNUM, 100
+  li a7, 0
+  ecall
+  
+  RVTEST_CODE_END
+
+die_u:
+  li a7, 0x1
+  ecall
+  
+
+RVTEST_CODE_END
+
+# PAGE_ONE E_PT_UNPROTECTED (RWX)
+# dummy data on separate page to test everything
+.align 12
+dummy_100: .word 0xBEEFDEAD
+dummy_101: .word 0x11111111
+dummy_102: .word 0xBEEFDEAD
+dummy_103: .word 0x12345ABC
+dummy_104: .word 0xBEEFDEAD
+dummy_105: jump_and_check_return_dst(persistant_return_reg)
+
+
+# PAGE_TWO E_PT_REGULAR (RWX)
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0xBEEFDEAD
+dummy_201: .word 0x22222222
+dummy_202: .word 0xBEEFDEAD
+dummy_203: .word 0x12345ABC
+dummy_204: .word 0xBEEFDEAD
+dummy_205: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_THREE E_PT_SHDATA (RW)
+# dummy data on separate page to test everything
+.align 12
+dummy_300: .word 0xBEEFDEAD
+dummy_301: .word 0x33333333
+dummy_302: .word 0xBEEFDEAD
+dummy_303: .word 0x12345ABC
+dummy_304: .word 0xBEEFDEAD
+dummy_305: jump_and_check_return_dst(persistant_return_reg)
+
+
+# PAGE_FOUR E_PT_SHDATA (RWX)
+# dummy data on separate page to test everything
+.align 12
+dummy_400: .word 0xBEEFDEAD
+dummy_401: .word 0x44444444
+dummy_402: .word 0xBEEFDEAD
+dummy_403: .word 0x12345ABC
+dummy_404: .word 0xBEEFDEAD
+dummy_405: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_FIVE E_PT_SHCODE (RX)
+# dummy data on separate page to test everything
+.align 12
+dummy_500: .word 0xBEEFDEAD
+dummy_501: .word 0x55555555
+dummy_502: .word 0xBEEFDEAD
+dummy_503: .word 0x12345ABC
+dummy_504: .word 0xBEEFDEAD
+dummy_505: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_SIX E_PT_SHCODE (RWX)
+# dummy data on separate page to test everything
+.align 12
+dummy_600: .word 0xBEEFDEAD
+dummy_601: .word 0x66666666
+dummy_602: .word 0xBEEFDEAD
+dummy_603: .word 0x12345ABC
+dummy_604: .word 0xBEEFDEAD
+dummy_605: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_SEVEN E_PT_SHMGLOB (R)
+# dummy data on separate page to test everything
+.align 12
+dummy_700: .word 0xBEEFDEAD
+dummy_701: .word 0x77777777
+dummy_702: .word 0xBEEFDEAD
+dummy_703: .word 0x12345ABC
+dummy_704: .word 0xBEEFDEAD
+dummy_705: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_EIGHT E_PT_SHMGLOB (RWX)
+# dummy data on separate page to test everything
+.align 12
+dummy_800: .word 0xBEEFDEAD
+dummy_801: .word 0x88888888
+dummy_802: .word 0xBEEFDEAD
+dummy_803: .word 0x12345ABC
+dummy_804: .word 0xBEEFDEAD
+dummy_805: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_NINE E_PT_THREADLOCAL (RWX)
+# dummy data on separate page to test everything
+.align 12
+dummy_900: .word 0xBEEFDEAD
+dummy_901: .word 0x99999999
+dummy_902: .word 0xBEEFDEAD
+dummy_903: .word 0x12345ABC
+dummy_904: .word 0xBEEFDEAD
+dummy_905: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_TEN E_PT_THREADMETA (RWX)
+# dummy data on separate page to test everything
+.align 12
+dummy_A00: .word 0xBEEFDEAD
+dummy_A01: .word 0xAAAAAAAA
+dummy_A02: .word 0xBEEFDEAD
+dummy_A03: .word 0x12345ABC
+dummy_A04: .word 0xBEEFDEAD
+dummy_A05: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_ELEVEN E_PT_MONITOR (RWX)
+# dummy data on separate page to test everything
+.align 12
+dummy_B00: .word 0xBEEFDEAD
+dummy_B01: .word 0xBBBBBBBB
+dummy_B02: .word 0xBEEFDEAD
+dummy_B03: .word 0x12345ABC
+dummy_B04: .word 0xBEEFDEAD
+dummy_B05: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_TWELVE
+# dummy data on separate page to test everything
+.align 12
+dummy_C00: .word 0xBEEFDEAD
+dummy_C01: .word 0xCCCCCCCC
+dummy_C02: .word 0xBEEFDEAD
+dummy_C03: .word 0x12345ABC
+dummy_C04: .word 0xBEEFDEAD
+dummy_C05: jump_and_check_return_dst(persistant_return_reg)
+
+# PAGE_THIRTEEN
+# dummy data on separate page to test everything
+.align 12
+dummy_D00: .word 0xBEEFDEAD
+dummy_D01: .word 0xDDDDDDDD
+dummy_D02: .word 0xBEEFDEAD
+dummy_D03: .word 0x12345ABC
+dummy_D04: .word 0xBEEFDEAD
+dummy_D05: jump_and_check_return_dst(persistant_return_reg)
+
+
+.align 12
+RVTEST_DATA_END
diff --git a/isa/rv64uz/store_authentication_exception.S b/isa/rv64uz/store_authentication_exception.S
new file mode 100644
index 0000000..b0b71ef
--- /dev/null
+++ b/isa/rv64uz/store_authentication_exception.S
@@ -0,0 +1,213 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# store_authentication_exception.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+#define EXPECTED_EXCEPTIONS 12
+
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+setup_m_to_u_vm(a1)
+
+
+
+// Set SID for write
+write_value_to_csr(0x8000000000000001, CSR_E_MSID_0, a1)
+write_value_to_csr((PAGE_ZERO  - PAGE_ZERO  )      , CSR_E_MRANGE_VBASE, a1)
+write_value_to_csr((PAGE_THREE - PAGE_ZERO - 1)   , CSR_E_MRANGE_VSIZE, a1)
+
+set_bit_in_csr(Z_DBG_VSIZE_XRANGE_IS_SIZE_NOT_MASK_NO_ALIGNMENT, CSR_E_DBG_CONTROL, a1)
+
+
+li TESTNUM, 1
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+li TESTNUM, 2
+set_and_enable_store_tweak(a1, E_TWEAK_ENCLAVE)
+li TESTNUM, 3
+copy_vma_to_enclave_vma(PAGE_ZERO, PAGE_ONE, PAGE_ZERO, PAGE_ZERO, a1, a2, a3, a4)
+
+write_value_to_csr(0x0, CSR_E_MSID_0, a1)
+
+li TESTNUM, 4
+copy_vma_to_enclave_vma(PAGE_ONE, PAGE_TWO, PAGE_ONE, PAGE_ZERO, a1, a2, a3, a4)
+
+li TESTNUM, 5
+// reset SID before return
+// for page three we first encrypt it correctly and the overwrite the physical memory
+// this tests also any non-strict authentication implemention
+write_value_to_csr(0x8000000000000001, CSR_E_MSID_0, a1)
+
+copy_vma_to_enclave_vma(PAGE_TWO, PAGE_THREE, PAGE_TWO, PAGE_ZERO, a1, a2, a3, a4)
+
+disable_lstweaks(a1)
+quick_disable_m_vm_access(a1)
+
+set_physical_memory_range_to_value(PAGE_TWO, PAGE_THREE, 0xffffffff, a1, a2, a3)
+
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+# prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+set_bit_in_csr(E_STATUS_ENCLAVE_MODE_EN, CSR_E_STATUS, a1)
+
+li TESTNUM, 6
+mret
+
+.align 8
+usermode_exit_handler_success_on_decryption_error(a0, t0, t3, a7, EXPECTED_EXCEPTIONS, final_ecall, decryption_exception)
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_ZERO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_2: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_ONE   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_3: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_TWO   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0x00123456
+dummy_001: .word 0xCAFEBABE
+dummy_002: .word 0xFFFFFFFF
+dummy_003: .word 0xABCDEF12
+dummy_004: .word 0xDEADBEEF
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x0
+1:
+  # // if key has to be incremented too often there's an error
+  li TESTNUM, 10
+  # write dummy address on same page (fetches slow path)
+  write_and_readback_value_test(t0, t1, dummy_000, 0x00123456, die_u)
+
+  li TESTNUM, 11
+  # write dummy address on same page (fetches should/could access cache)
+  write_and_readback_value_test(t0, t1, dummy_000, 0x00123456, die_u)
+
+  nop
+  nop
+  nop
+  nop
+
+  clear_bit_in_csr(Z_DBG_AUTH_WEAK, CSR_E_DBG_CONTROL, t0)
+  // This should trigger the exception in strict mode:
+  li TESTNUM, 20
+  # write dummy protected address on other page (should cause an exception)
+  write_and_readback_value_test(t0, t1, dummy_101, 0xCEAEBEBA, 2f)
+2:
+  li TESTNUM, 21
+  # write dummy protected address on other page (should cause an exception)
+  write_and_readback_value_test(t0, t1, dummy_101, 0xCEAEBEBA, 2f)
+2:
+
+  set_bit_in_csr(Z_DBG_AUTH_WEAK, CSR_E_DBG_CONTROL, t0)
+  // This should pass in weak mode:
+  li TESTNUM, 22
+  # write dummy protected address on other page (will not cause an exception)
+  write_and_readback_value_test(t0, t1, dummy_102, 0xCEAEBEBA, die_u)
+2:
+  li TESTNUM, 23
+  # write dummy protected address on other page (will not cause an exception)
+  write_and_readback_value_test(t0, t1, dummy_102, 0xCEAEBEBA, die_u)
+2:
+
+  nop
+  nop
+  nop
+  nop
+  set_bit_in_csr(Z_DBG_AUTH_WEAK, CSR_E_DBG_CONTROL, t0)
+  // This should trigger the exception in weak mode:
+  li TESTNUM, 30
+  # write dummy protected address on other page (should cause an exception)
+  write_and_readback_value_test(t0, t1, dummy_201, 0xCEAEBEBA, 2f)
+2:
+  li TESTNUM, 31
+  # write dummy protected address on other page (should cause an exception)
+  write_and_readback_value_test(t0, t1, dummy_201, 0xCEAEBEBA, 2f)
+2:  
+  clear_bit_in_csr(Z_DBG_AUTH_WEAK, CSR_E_DBG_CONTROL, t0)
+  // This should trigger the exception in stric mode:
+  li TESTNUM, 32
+  # write dummy protected address on other page (should cause an exception)
+  write_and_readback_value_test(t0, t1, dummy_203, 0xCEAEBEBA, 2f)
+2:
+  li TESTNUM, 33
+  # write dummy protected address on other page (should cause an exception)
+  write_and_readback_value_test(t0, t1, dummy_203, 0xCEAEBEBA, 2f)
+2:
+
+  li TESTNUM, 100
+  ecall
+  
+  RVTEST_CODE_END
+
+die_u:
+  li a7, 0x1
+  ecall
+  
+
+RVTEST_CODE_END
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+.align 12
+dummy_100: .word 0x00000123
+dummy_101: .word 0xCEAEBEBA
+dummy_102: .word 0x88888888
+dummy_103: .word 0x12345ABC
+dummy_104: .word 0x11111111
+
+# PAGE_TWO
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0xBEEFDEAD
+dummy_201: .word 0xBEEFDEAD
+dummy_202: .word 0xBEEFDEAD
+dummy_203: .word 0xBADEAFFE
+dummy_204: .word 0xBEEFDEAD
+
+RVTEST_DATA_END
diff --git a/isa/rv64uz/tweaked_vaddr_paddr_access.S b/isa/rv64uz/tweaked_vaddr_paddr_access.S
new file mode 100644
index 0000000..839752e
--- /dev/null
+++ b/isa/rv64uz/tweaked_vaddr_paddr_access.S
@@ -0,0 +1,197 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# tweaked_vaddr_paddr_access.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+#define EXPECTED_EXCEPTIONS 0
+
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+#define read_vdata_from_m(treg_0, treg_1, p_label, PAGE_OFFSET, expected_value, error_label) \
+  quick_enable_m_vm_access(treg_0) \
+  la treg_0, p_label; \
+  li treg_1, PAGE_OFFSET; \
+  sub treg_0, treg_0, treg_1; \
+  lwu treg_0, 0(treg_0); \
+  quick_disable_m_vm_access(treg_1) \
+  li treg_1, expected_value; \
+  bne treg_0, treg_1, error_label;
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+
+# save original mtvec
+csrr t3, CSR_MTVEC
+
+write_value_to_csr(0x8000000000000001, CSR_E_MSID_0, a1)
+write_value_to_csr((PAGE_ONE  - PAGE_ZERO  )      , CSR_E_MRANGE_VBASE, a1)
+write_value_to_csr((PAGE_TWO - PAGE_ONE - 1)   , CSR_E_MRANGE_VSIZE, a1)
+// set unaligned range test
+set_bit_in_csr(Z_DBG_VSIZE_XRANGE_IS_SIZE_NOT_MASK_NO_ALIGNMENT, CSR_E_DBG_CONTROL, a1)
+
+// #####################
+li TESTNUM, 1
+// access as physical address
+check_physical_memory(dummy_101, a1, a2, 0xCEAEBEBA, die)
+
+li TESTNUM, 2
+// access a physical address that is also a PTE
+check_physical_memory(page_table_leaf_entry_1, a1, a2, 0x200014df, die)
+
+// #####################
+
+// Setup Virtual Memory
+setup_m_to_u_vm(a1)
+// value from location
+li TESTNUM, 3
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+read_vdata_from_m(a1, a2, dummy_001, PAGE_ZERO, 0xCAFEBABE, die)
+disable_lstweaks(a1)
+
+li TESTNUM, 4
+// access physical memory of page without rwx
+check_physical_memory(dummy_203, a1, a2, 0xBADEAFFE, die)
+
+li TESTNUM, 5
+// make page RWX by accessing the physical memory
+save_PTE_disable_VM(a1, page_to_leaf_address(PAGE_TWO, PAGE_ZERO))
+make_pte_in_reg_rwx(a1, a2)
+restore_PTE_disable_VM(a2, a3, page_to_leaf_address(PAGE_TWO, PAGE_ZERO))
+
+// access the VM on page two and read value
+li TESTNUM, 6
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+read_vdata_from_m(a1, a2, dummy_201, PAGE_ZERO, 0xAFFEAFFE, die)
+disable_lstweaks(a1)
+
+
+// #####################
+// activate lstweak and access virtual, physical, PTE, virtual again
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+
+li TESTNUM, 7
+read_vdata_from_m(a1, a2, dummy_003, PAGE_ZERO, 0xABCDEF12, die)
+disable_lstweaks(a1)
+
+li TESTNUM, 8
+check_physical_memory(dummy_104, a1, a2, 0x11111111, die)
+
+li TESTNUM, 9
+// make page RWX by accessing the physical memory
+save_PTE_disable_VM(a1, page_to_leaf_address(PAGE_THREE, PAGE_ZERO))
+make_pte_in_reg_rwx(a1, a2)
+restore_PTE_disable_VM(a2, a3, page_to_leaf_address(PAGE_THREE, PAGE_ZERO))
+
+li TESTNUM, 0xA
+// access the VM on page two and read value
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+read_vdata_from_m(a1, a2, dummy_301, PAGE_ZERO, 0xAFFEAFFE, die)
+disable_lstweaks(a1)
+
+// #####################
+
+j end
+
+.align 8
+// Last parameter must hold the number of expected exceptions!
+usermode_exit_handler_success_on_decryption_error(a0, t0, t3, a7, EXPECTED_EXCEPTIONS, final_ecall, decryption_exception)
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+# Set the dirty page bit because: privileged 1.10 p. 61
+# The A and D bits are never cleared by the implementation. If the supervisor software does
+# not rely on accessed and/or dirty bits, e.g. if it does not swap memory pages to secondary storage
+# or if the pages are being used to map I/O space, it should always set them to 1 in the PTE to
+# improve performance.
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ZERO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U |    PTE_R | PTE_W | PTE_X |    PTE_A | PTE_D
+page_table_leaf_entry_2: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_ONE   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | /* PTE_R | PTE_W | PTE_X | */ PTE_A | PTE_D
+page_table_leaf_entry_3: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_TWO   / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | /* PTE_R | PTE_W | PTE_X | */ PTE_A | PTE_D
+page_table_leaf_entry_4: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_THREE / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U |    PTE_R | PTE_W | PTE_X |    PTE_A | PTE_D
+page_table_leaf_entry_5: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_FOUR  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U |    PTE_R | PTE_W | PTE_X |    PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0x00123456
+dummy_001: .word 0xCAFEBABE
+dummy_002: .word 0xFFFFFFFF
+dummy_003: .word 0xABCDEF12
+dummy_004: .word 0xDEADBEEF
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x0
+
+  li TESTNUM, 100
+  ecall 
+
+RVTEST_CODE_END
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+.align 12
+dummy_100: .word 0x00000123
+dummy_101: .word 0xCEAEBEBA
+dummy_102: .word 0x88888888
+dummy_103: .word 0x12345ABC
+dummy_104: .word 0x11111111
+
+# PAGE_TWO
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0xBEEFDEAD
+dummy_201: .word 0xAFFEAFFE
+dummy_202: .word 0xBEEFDEAD
+dummy_203: .word 0xBADEAFFE
+dummy_204: .word 0xBEEFDEAD
+
+# PAGE_THREE
+# dummy data on separate page to test everything
+.align 12
+dummy_300: .word 0xBEEFDEAD
+dummy_301: .word 0xAFFEAFFE
+dummy_302: .word 0xBEEFDEAD
+dummy_303: .word 0xBADEAFFE
+dummy_304: .word 0xBEEFDEAD
+
+# PAGE_FOUR
+# dummy data on separate page to test everything
+.align 12
+RVTEST_DATA_END
diff --git a/isa/rv64uz/vectored_exceptions_base.S b/isa/rv64uz/vectored_exceptions_base.S
new file mode 100644
index 0000000..bb7dc2c
--- /dev/null
+++ b/isa/rv64uz/vectored_exceptions_base.S
@@ -0,0 +1,169 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# vectored_exceptions.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+# define EXPECTED_ACCUMULATED_CAUSES (CAUSE_USER_ECALL + CAUSE_LOAD_PAGE_FAULT)
+
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+# prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+li TESTNUM, 5
+mret
+
+
+exception_save_slot: .word 0x0;
+accumulate_exceptions:
+  la a0, exception_save_slot;
+  lwu t0, 0(a0)
+  csrr a0, CSR_MCAUSE
+  add t0, t0, a0
+  la a0, exception_save_slot;
+  sw t0, 0(a0)
+  li a0, EXPECTED_ACCUMULATED_CAUSES
+  // terminate if it's an error ecall
+  bnez a7, die
+  // end test if all causes happened
+  beq t0, a0, end
+  // if not yet all causes happened, skip the instruction
+  blt t0, a0, skip_instruction
+  // if too many causes happened, terminate
+  bgt t0, a0, die
+
+  
+.align 8
+my_mtvec_handler:
+  j die                                   ;     /* Instruction address misaligned */
+  j die                                   ;     /* Instruction access fault */
+  j die                                   ;     /* Illegal instruction */
+  j die                                   ;     /* Breakpoint */
+  j die                                   ;     /* Load address misaligned */
+  j die                                   ;     /* Load access fault */
+  j die                                   ;     /* Store/AMO address misaligned */
+  j die                                   ;     /* Store/AMO access fault */
+  j accumulate_exceptions                 ;     /* Environment call from U-mode */
+  j die                                   ;     /* Environment call from S-mode */
+  j die                                   ;     /* reserved */
+  j die                                   ;     /* Environment call from M-mode */
+  j die                                   ;     /* Instruction page fault */
+  j accumulate_exceptions                 ;     /* Load page fault */
+  j die                                   ;     /* Donky Exception */
+  j die                                   ;     /* Store/AMO page fault */
+  j die                                   ;     /* Decryption/ Authentication exception */
+
+
+skip_instruction:
+  csrr t0, mepc;
+  ADDI t0, t0, 4;
+  csrw mepc, t0;
+  j return_to_userland
+
+return_to_userland:
+  mret
+
+m_exception_handler_end_and_die(a0, t0, t3);
+
+.align 8
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ZERO / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_2: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ONE  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_3: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_TWO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_4: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_THREE/ RISCV_PGSIZE ) << PTE_PPN_SHIFT) |         PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0x00123456
+dummy_001: .word 0xCAFEBABE
+dummy_002: .word 0xFFFFFFFF
+dummy_003: .word 0xABCDEF12
+dummy_004: jump_and_check_return_dst(persistant_return_reg)
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x0
+
+  // trigger a pagefault
+  li TESTNUM, 10
+  read_value_test_expect_fail(a1, a2, dummy_300, 0x33333333, die_u);
+  
+  // ecall to leave testsuite
+  li TESTNUM, 11
+  ecall
+  j die_u
+  
+  RVTEST_CODE_END
+
+die_u:
+  li a7, 0x1
+  ecall
+  
+
+RVTEST_CODE_END
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+.align 12
+dummy_100: .word 0x11111111
+
+# PAGE_TWO
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0x22222222
+
+
+# PAGE_THREE
+# dummy data on separate page to test everything
+.align 12
+dummy_300: .word 0x33333333
+
+
+# PAGE_THREE
+# dummy data on separate page to test everything
+.align 12
+RVTEST_DATA_END
diff --git a/isa/rv64uz/vectored_exceptions_extended.S b/isa/rv64uz/vectored_exceptions_extended.S
new file mode 100644
index 0000000..d62217d
--- /dev/null
+++ b/isa/rv64uz/vectored_exceptions_extended.S
@@ -0,0 +1,193 @@
+# See LICENSE for license details.
+
+#*****************************************************************************
+# vectored_exceptions.S
+#-----------------------------------------------------------------------------
+#
+# 
+# 
+# 
+#
+
+#include "riscv_test.h"
+#include "test_macros.h"
+#include "z_routines.h"
+
+# define EXPECTED_ACCUMULATED_CAUSES (CAUSE_USER_ECALL + CAUSE_LOAD_PAGE_FAULT + CAUSE_DECRYPTION_INTEGRITY_FAILURE)
+
+
+RVTEST_RV64M
+RVTEST_CODE_BEGIN
+
+
+# activate VM
+li a0, (SATP_MODE & ~(SATP_MODE<<1)) * SATP_MODE_SV39
+la a1, page_table_root_node
+srl a1, a1, RISCV_PGSHIFT
+or a1, a1, a0
+csrw sptbr, a1
+sfence.vma
+
+setup_m_to_u_vm(a1)
+
+// Set SID for write
+write_value_to_csr(         0x8000000000000001, CSR_E_MSID_0 , a1)
+write_value_to_csr((PAGE_TWO - PAGE_ZERO)     , CSR_E_MRANGE_VBASE, a1)
+write_value_to_csr((PAGE_THREE - PAGE_TWO - 1 )     , CSR_E_MRANGE_VSIZE, a1)
+li TESTNUM, 1
+set_and_enable_load_tweak(a1, E_TWEAK_UNPROTECTED_U)
+li TESTNUM, 2
+set_and_enable_store_tweak(a1, E_TWEAK_ENCLAVE)
+li TESTNUM, 3
+// Encrypt Page Two
+copy_vma_to_enclave_vma(PAGE_TWO, PAGE_THREE, PAGE_TWO, PAGE_ZERO, a1, a2, a3, a4)
+
+disable_lstweaks(a1)
+quick_disable_m_vm_access(a1)
+write_value_to_csr(                        0x0, CSR_E_MSID_0 , a1)
+
+save_and_deploy_mtvec(a3, t3, my_mtvec_handler)
+
+# prepare return to userspace by loading the virtual address into the mepc
+set_user_pc_to_userlabel(a0, t0, superstart, PAGE_ZERO)
+li TESTNUM, 5
+mret
+
+
+exception_save_slot: .word 0x0;
+accumulate_exceptions:
+  la a0, exception_save_slot;
+  lwu t0, 0(a0)
+  csrr a0, CSR_MCAUSE
+  add t0, t0, a0
+  la a0, exception_save_slot;
+  sw t0, 0(a0)
+  li a0, EXPECTED_ACCUMULATED_CAUSES
+  // terminate if it's an error ecall
+  bnez a7, die
+  // end test if all causes happened
+  beq t0, a0, end
+  // if not yet all causes happened, skip the instruction
+  blt t0, a0, skip_instruction
+  // if too many causes happened, terminate
+  bgt t0, a0, die
+
+  
+.align 8
+my_mtvec_handler:
+  j die                                   ;     /* Instruction address misaligned */
+  j die                                   ;     /* Instruction access fault */
+  j die                                   ;     /* Illegal instruction */
+  j die                                   ;     /* Breakpoint */
+  j die                                   ;     /* Load address misaligned */
+  j die                                   ;     /* Load access fault */
+  j die                                   ;     /* Store/AMO address misaligned */
+  j die                                   ;     /* Store/AMO access fault */
+  j accumulate_exceptions                 ;     /* Environment call from U-mode */
+  j die                                   ;     /* Environment call from S-mode */
+  j die                                   ;     /* reserved */
+  j die                                   ;     /* Environment call from M-mode */
+  j die                                   ;     /* Instruction page fault */
+  j accumulate_exceptions                 ;     /* Load page fault */
+  j die                                   ;     /* Donky Exception */
+  j die                                   ;     /* Store/AMO page fault */
+  j accumulate_exceptions                 ;     /* Decryption/ Authentication exception */
+
+
+skip_instruction:
+  csrr t0, mepc;
+  ADDI t0, t0, 4;
+  csrw mepc, t0;
+  j return_to_userland
+
+return_to_userland:
+  mret
+
+m_exception_handler_end_and_die(a0, t0, t3);
+
+.align 8
+// Last parameter must hold the number of expected exceptions!
+//usermode_exit_handler_success_on_fetch_decryption_error(a0, t0, t3, a7, EXPECTED_EXCEPTIONS, adapted_ecall, decryption_exception)
+
+RVTEST_CODE_END
+
+  .data
+RVTEST_DATA_BEGIN
+
+TEST_DATA
+
+
+
+# root page table
+.align 12
+page_table_root_node: .dword (( INTERMEDIATE_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# intermediate paget table
+.align 12
+page_table_entry_1: .dword (( LEAF_PTES / RISCV_PGSIZE) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_A
+
+# leaf page table with and without mpkeys
+.align 12
+page_table_leaf_entry_1: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ZERO / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_2: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_ONE  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_3: .dword E_PT_REGULAR     << PTE_MSB_SHIFT | (( PAGE_TWO  / RISCV_PGSIZE ) << PTE_PPN_SHIFT) | PTE_V | PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+page_table_leaf_entry_4: .dword E_PT_UNPROTECTED << PTE_MSB_SHIFT | (( PAGE_THREE/ RISCV_PGSIZE ) << PTE_PPN_SHIFT) |         PTE_U | PTE_R | PTE_W | PTE_X | PTE_A | PTE_D
+
+# t3 ... old mtvec DON'T overwrite!!
+# a7 != ... before an ecall means DIE
+# PAGE_ZERO
+.align 12
+dummy_000: .word 0x00123456
+dummy_001: .word 0xCAFEBABE
+dummy_002: .word 0xFFFFFFFF
+dummy_003: .word 0xABCDEF12
+dummy_004: jump_and_check_return_dst(persistant_return_reg)
+superstart:
+  li t1, 0xA5A5A5A5
+  li a1, 0x2
+  li a2, 0x0
+  li a7, 0x0
+
+  // trigger a pagefault
+  li TESTNUM, 10
+  read_value_test_expect_fail(a1, a2, dummy_300, 0x33333333, die_u);
+
+  // trigger an decryption error
+  li TESTNUM, 11
+  read_value_test_expect_fail(a1, a2, dummy_200, 0x22222222, die_u);
+  
+  // ecall to leave testsuite
+  li TESTNUM, 12
+  ecall
+  j die_u
+  
+  RVTEST_CODE_END
+
+die_u:
+  li a7, 0x1
+  ecall
+  
+
+RVTEST_CODE_END
+
+# PAGE_ONE
+# dummy data on separate page to test everything
+.align 12
+dummy_100: .word 0x11111111
+
+# PAGE_TWO
+# dummy data on separate page to test everything
+.align 12
+dummy_200: .word 0x22222222
+
+
+# PAGE_THREE
+# dummy data on separate page to test everything
+.align 12
+dummy_300: .word 0x33333333
+
+
+# PAGE_THREE
+# dummy data on separate page to test everything
+.align 12
+RVTEST_DATA_END
-- 
2.25.1

